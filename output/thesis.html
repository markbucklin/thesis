<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta http-equiv="Content-Style-Type" content="text/css" />
        <meta name="generator" content="pandoc" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
                        <title>All Figures</title>
        <style type="text/css">code{white-space: pre;}</style>
                                                    
            .caption {
              color: #777;
              margin-top: 10px;
            }
            p code {
              white-space: inherit;
            }
            pre {
              word-break: normal;
              word-wrap: normal;
            }
            pre code {
              white-space: inherit;
            }
            p.flushright {
              text-align: right;
            }
            blockquote > p:last-child {
              text-align: right;
            }
            blockquote > p:first-child {
              text-align: inherit;
            }
            .header-section-number {
              padding-right: .2em;
              font-weight: 500;
            }
            .level1 .header-section-number {
              display: inline-block;
              border-bottom: 3px solid;
            }
            .level1 h1 {
              border-bottom: 1px solid;
            }
            h1, h2, h3, h4, h5, h6 {
              font-weight: normal;
            }
            h1.title {
              font-weight: 700;
            }
            .smallcaps {
              font-variant: small-caps;
            }
            .book .book-body .page-wrapper .page-inner section.normal strong {
              font-weight: 600;
            }
            
            .figure img{
              display: block;
              margin-left: auto;
              margin-right: auto;
            }
            
            blockquote{
              font-size: 1.2em;
              width:70%;
              margin:50px auto;
              font-style:italic;
              padding:1.2em 30px 1.2em 75px;
              line-height:1.6;
              position: relative;
              margin-bottom: 10em;
            }
            
            blockquote::before{
              font-family:Arial;
              content: "\201C";
              font-size:4em;
              position: absolute;
              left: 10px;
              top:-10px;
            }
            
            blockquote::after{
              content: '';
            }
            
            .book .book-body .page-wrapper .page-inner section.normal blockquote {
              margin: 0 0 0 30%;
              padding: 0 0.5em 0 3em;
              opacity: 0.75;
              border-left: 0px solid #dcdcdc
            }
                <script src="js/jquery.js"></script>
        <script src="js/diff.js"></script>
        <script src="js/main.js"></script>
    </head>
    <body>
                <!--
                    <div id="header">
                <h1 class="title">All Figures</h1>
                                                            </div>
                -->
        <div id="title-page">
            <h1>This is the title of the thesis</h1>
            <h2>Firstname Surname</h2>
        </div>
                    <div id="TOC">
                <ul>
                <li><a href="#introduction-background-and-literature-review"><span class="toc-section-number">1</span> Introduction: Background and Literature Review</a><ul>
                <li><a href="#primary-goals"><span class="toc-section-number">1.1</span> Primary Goals</a></li>
                <li><a href="#optical-imaging-of-neural-activity"><span class="toc-section-number">1.2</span> Optical Imaging of Neural Activity</a></li>
                <li><a href="#procedures-for-calcium-imaging"><span class="toc-section-number">1.3</span> Procedures for Calcium Imaging</a></li>
                <li><a href="#computer-software-environments-for-image-processing"><span class="toc-section-number">1.4</span> Computer Software Environments for Image Processing</a></li>
                <li><a href="#computational-resources-for-processing-large-data-sets"><span class="toc-section-number">1.5</span> Computational Resources for Processing Large Data Sets</a></li>
                </ul></li>
                <li><a href="#neural-interfaces-fabrication-programming-and-assembly"><span class="toc-section-number">2</span> Neural Interfaces: Fabrication, programming, and assembly</a><ul>
                <li><a href="#animal-tracking"><span class="toc-section-number">2.1</span> Animal Tracking</a><ul>
                <li><a href="#behavior-box"><span class="toc-section-number">2.1.1</span> Behavior Box</a></li>
                <li><a href="#mouse-in-a-bowl"><span class="toc-section-number">2.1.2</span> Mouse in a Bowl</a></li>
                <li><a href="#spherical-treadmill"><span class="toc-section-number">2.1.3</span> Spherical Treadmill</a></li>
                <li><a href="#headplate-holder"><span class="toc-section-number">2.1.4</span> Headplate Holder</a></li>
                <li><a href="#motion-sensors"><span class="toc-section-number">2.1.5</span> Motion Sensors</a></li>
                </ul></li>
                <li><a href="#microscopes"><span class="toc-section-number">2.2</span> Microscopes</a><ul>
                <li><a href="#microscope-construction"><span class="toc-section-number">2.2.1</span> Microscope Construction</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#neural-signals-computational-considerations-interpretation-and-usage"><span class="toc-section-number">3</span> Neural Signals: Computational considerations, interpretation and usage</a><ul>
                <li><a href="#image-processing"><span class="toc-section-number">3.1</span> Image Processing</a><ul>
                <li><a href="#image-processing-tonemapping-and-filtering"><span class="toc-section-number">3.1.1</span> Image Processing: Tonemapping and Filtering</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#discussion-broader-considerations-for-clinicians-engineers-and-research-scientists-preparing-to-make-use-of-an-increasingly-hyper-connected-future"><span class="toc-section-number">4</span> Discussion: Broader considerations for clinicians, engineers, and research scientists preparing to make use of an increasingly hyper-connected future</a></li>
                <li><a href="#appendix">Appendix</a></li>
                <li><a href="#list-of-figures">List of Figures</a></li>
                <li><a href="#introduction-background-and-literature-review-1"><span class="toc-section-number">5</span> Introduction: Background and Literature Review</a></li>
                <li><a href="#neural-interface-construction-fabrication-programming-and-assembly-of-of-an-automated-system-to-open-the-channel-to-your-mouses-mind."><span class="toc-section-number">6</span> Neural Interface Construction: Fabrication, programming, and assembly of of an automated system to open the channel to your mouses mind.</a><ul>
                <li><a href="#animal-tracking-1"><span class="toc-section-number">6.1</span> Animal Tracking</a><ul>
                <li><a href="#behavior-box-1"><span class="toc-section-number">6.1.1</span> Behavior Box</a></li>
                <li><a href="#mouse-in-a-bowl-1"><span class="toc-section-number">6.1.2</span> Mouse in a Bowl</a></li>
                <li><a href="#spherical-treadmill-1"><span class="toc-section-number">6.1.3</span> Spherical Treadmill</a></li>
                <li><a href="#headplate-holder-1"><span class="toc-section-number">6.1.4</span> Headplate Holder</a></li>
                <li><a href="#motion-sensors-1"><span class="toc-section-number">6.1.5</span> Motion Sensors</a></li>
                </ul></li>
                <li><a href="#microscopes-1"><span class="toc-section-number">6.2</span> Microscopes</a><ul>
                <li><a href="#microscope-construction-1"><span class="toc-section-number">6.2.1</span> Microscope Construction</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#neural-analytics-computational-considerations-and-approaches-to-manage-the-continuous-flow-of-neural-imaging-data-and-if-not-making-sense-of-it-perhaps-making-use-of-it."><span class="toc-section-number">7</span> Neural Analytics: Computational considerations and approaches to manage the continuous flow of neural imaging data… and if not making sense of it, perhaps making use of it.</a><ul>
                <li><a href="#image-processing-1"><span class="toc-section-number">7.1</span> Image Processing</a><ul>
                <li><a href="#image-processing-tonemapping-and-filtering-1"><span class="toc-section-number">7.1.1</span> Image Processing: Tonemapping and Filtering</a></li>
                </ul></li>
                </ul></li>
                <li><a href="#discussion-broader-considerations-for-clinicians-engineers-and-research-scientists-preparing-to-make-use-of-an-increasingly-hyper-connected-future-1"><span class="toc-section-number">8</span> Discussion: Broader considerations for clinicians, engineers, and research scientists preparing to make use of an increasingly hyper-connected future</a></li>
                <li><a href="#appendix-1">Appendix</a></li>
                </ul>
            </div>
                                <h1 id="introduction-background-and-literature-review"><span class="header-section-number">1</span> Introduction: Background and Literature Review</h1>
<h2 id="primary-goals"><span class="header-section-number">1.1</span> Primary Goals</h2>
<p>The function of the brain is to translate/encode sensory input into neural output, actuating an effect that promotes organism survival or propagates to promote the survival of offspring (response generation). It achieves this by communicating input through interconnected neurons via converging and diverging connections that comprise the neural network .Testing and observing the properties of individual neurons and the response to changing conditions at the direct connections they form with others is The function of the brain is to translate/encode sensory input into neural output that actuates an effect that promotes survival of the organism or propagates to promote the survival of offspring (generation of a response). It does this by communicating input through interconnected neurons via converging and diverging connections which comprise the neural network. One way to study the brain is by testing and observing the properties of individual neurons and the response to changing conditions at the direct connections they form with others. Another approach is to observe a collection of neurons and then measure their response to variable conditions in their external environment either by recording or stimulating variations in sensory input or measuring an organism’s physical/behavioral response.</p>
<p>One might presume that the expansion of information provided by measuring activity from a larger number of cells in a network would simplify analysis in stimulus-response type experiments and afford insight about underlying functional mechanisms. Unfortunately, the correlation and information theoretic procedures traditionally used to make these associations suffer from a systematic bias that exponentially grows with the number responses considered for each stimulus (i.e., the number of included cells). The trial number necessary to overcome this bias becomes exponentially large although methods such as shuffling/resampling tests exist for bias correction.</p>
<p>A systems neuroscience experiment benefits from online feedback in one or both of two ways:</p>
<pre><code>1. It informs the user regarding the current number of trials, i.e., repeated presentations of the stimulus will be sufficient to overcome limited sampling bias in an experiment attempting to learn the neural response/pattern associated with a specific stimulus. This could be done by testing pattern hypotheses online against subsets of collected data and then assessing their stability.

2. Online pattern recognition feedback maximizes the information in the response to a stimulus either by directing modification of the stimulus, or directing modification of the field-of-view either by directing modification of the stimulus, or directing modification of the field-of-view.</code></pre>
<p>Streaming processing addresses the issues of processing and storing for sufficient learning from large networks. Additionally, I propose a strategy in the methods section by which incorporating this online processing stream into stimulus-response-type experiments could help correct limited sampling bias, enabling neural coding analysis in large populations of neurons <span class="citation" data-cites="ince_presence_2009">(Ince et al. 2009)</span>. This approach works when the experimental intention is to study neural coding in general, for which it’s sufficient to have an arbitrary stimulus.</p>
<p>An additional goal of this project focuses on the ability to use the expanded information made available by the first two project components to train an encoder that predicts intended motor states from one healthy mouse and uses the predictions to direct neuromodulatory control of a second mouse. This setup simulates pathologic disconnection in a brain, tests the ability to distinguish intention to start or stop running, and also applies this is a manner that easily measures performance.</p>
<h2 id="optical-imaging-of-neural-activity"><span class="header-section-number">1.2</span> Optical Imaging of Neural Activity</h2>
<p>Optical techniques for observing neural activity have recently advanced owing to both an evolution of digital imaging technology, and the development of engineered proteins that act as fluorescent indicators of neural activity. Image sensors, like those found in scientific-CMOS (sCMOS) cameras are larger, faster, and more sensitive than prior scientific grade cameras. Meanwhile, the latest generation of Genetically Encoded Calcium Indicators (GECIs), collectively called GCaMP6, report fluctuations in neural activation with extremely high fidelity. This combination of developments enables neuroscientists to open a wider channel to the brain than previously possible using conventional epifluorescence microscopy techniques that enable simultaneous recording from hundreds to thousands of neurons. Expanding the fraction of the observable neurons in an interconnected network could improve understanding of neural coding and provide insight into mechanistic properties of neural disease. Additionally, feeding a large set of neural response information to a machine learning algorithm in a neuroprosthetic application may provide improved predictive performance even when the exact mechanism of prediction is difficult to discern. However, several major challenges currently antagonize the potential benefits of these new technologies:</p>
<pre><code>1. The increased size of raw data from a single imaging session can easily overwhelm the computational resources typically used to process similar but smaller sets of data.

2. The accumulation of raw data on disk over multiple imaging sessions quickly exceeds the data-storage capacity of most lab-scale servers, forcing researchers to halt data collection to process and delete, potentially creating a “nightmare scenario”.

3. The experimental design and data analysis procedures familiar to neuroscientists for network activity data for 5 to 10 cells produce highly biased spurious results in the absence of numerous stimulus-response repetitions, i.e., trials. The number of repeated trials sufficient to produce an accurate description of the neural response to any stimulus is on the order of 2N, where N is the number of neurons being measured.</code></pre>
<!-- todo: remove -->
<p>The objective of this project is to establish procedures that address these specific challenges and then use these procedures to evaluate the effect that expanding available neural response input on the performance of a closed-loop encoder. For example, sensors on a moving ball can be trained such that a closed-loop encoder can predict changes in motor state of a mouse running on the ball. It will then use the predicted motor state to modulate motor state in another mouse using opsins. This process can be thought of as a model neuroprosthetic designed to overcome dysfunction caused by pathologically disconnected brain areas that occur in Parkinson’s disease (PD). A primary goal is to increase the synchronization of mice beyond chance, such that they tend to both run together and rest together.</p>
<p>In the chapters that follow I provide background on the general procedure for offline video processing. I also discuss some of the issues that limit execution of these procedures on a large dataset, and the variety of approaches that I and others have attempted to address this issue. I then introduce the streaming approach that is capable of directly processing video during acquisition and extracting signals, thereby saving relevant signals only while also discarding or compressing the raw video. This approach relies on GPU programming and therefore I also provide background on the application of graphics cards for computationally demanding tasks. Using a graphics card for programming in the MATLAB environment is also discussed.</p>
<p>Capturing wide-field fluorescence images at high spatial and temporal resolution enables us to measure functional dynamic changes in multiple cells within a large interconnected network. Extracting a measure for each cell in a way that preserves spatial and temporal continuity with uniform/unbiased sampling of the observed signal is achievable but several factors complicate procedures intended to accomplish this task. One class of computer-vision procedure commonly applied to this task is image-segmentation (cell-segmentation in histology applications), a procedure that attempts to represent distinct objects in an image by association of each image pixel with one of any number of abstract objects or with the background. A variety of algorithms exist for efficiently performing this operation on single images. Most methods can be extended to operate in a 3rd dimension, applied to stacks of image frames to enable tracking cells at multiple depths, or equivalently over time.</p>
<p>However, motion induced by physiologic changes and animal movement necessitates the correct alignment of all frames in the sequence. Moreover, the massive fluctuations in signal intensity from individual and spatially overlapping cells often breeds unstable solutions for alignment that radically complicate cell identification routines by disrupting temporal continuity. Implementing a reliable procedure for identifying and tracking the same cells in each frame throughout the sequence thus becomes non-trivial.</p>
<h2 id="procedures-for-calcium-imaging"><span class="header-section-number">1.3</span> Procedures for Calcium Imaging</h2>
<p>The general goal of processing image data from functional fluorescence imaging experiments is to restructure raw image data in a way that maps pixels in each image frame to distinct individual cells or subcellular components, called ‘Regions-Of-Interest’ (ROI). Pixel-intensity values from mapped pixels are often reduced by combination to single dimensional ‘trace’ time-series. These traces indicate the fluorescence intensity of an individual neuron over time, and the collection approximates the distinct activity of all individual neurons in the microscope’s field of view. However, this task is made difficult by motion of the brain throughout the experiment and by the apparent overlap of cells in the single image plane due to limitations of the camera’s 2-dimensional perspective. These issues can be partially mitigated with a few image pre-processing steps. Most importantly is the alignment of images to correct for motion. These options are described in the Methods &amp; Approaches section below. Most software packages specifically geared toward functional imaging implement either of two basic classes of pixel-&gt;cell mapping algorithms. One approach is to use image-segmentation routines for computer vision that seeks to combine adjacent pixels into distinct spatially segregated regions representing objects in the image.</p>
<p>The other common approach is to perform an eigenvalue decomposition on the covariance matrix from a stack of image frames (also called spectral decomposition, or Principal Component Analysis, PCA), resulting in an assembly of basis vectors that define the weighting coefficients for each pixel. Multiplying the basis-vectors (i.e., “components”) with all frames produces a one-dimensional trace for each component. The linear combination is similar to the weighted image-segmentation method in that it assigns fractional coefficients to pixels. However, the procedure for computing the covariance matrix employed by PCA operates on as many pixels as exist in the image, multiplying each with every other pixel that creates a problem with np2 complexity, where p is the number of pixels in the image. I mention these issues inherent to PCA not because this project addresses them but because this project was initiated following substantial difficulty attempting to use PCA-based cell sorting methods with large datasets.</p>
<h2 id="computer-software-environments-for-image-processing"><span class="header-section-number">1.4</span> Computer Software Environments for Image Processing</h2>
<p>The widespread usage of MATLAB in neuroscience communities lends potential for greater usability and easier adaptation to software developed in this environment. While software development environments focused on “ease-of-use” traditionally presume crippling sacrifices to computational performance, this assumption is now less accurate.</p>
<p>Standard programs include ImageJ, the built-in routines in MATLAB’s Image Processing Toolbox, Mosaic from Inscopix that are merely a compiled version of MATLAB routines employing the MATLAB engine, Sci-Kits Image for Python, and a remarkable diversity of miscellaneous applications. MATLAB is a commercial software development platform that is geared toward fast production and the prototyping of data processing routines in a high-level programming language. It implements several core libraries (LINPACK, BLAS, etc.) that make multi-threaded operations on matrix type data highly efficient. While MATLAB has traditionally been considered the standard across neuroscience research labs, it is well recognized that its performance was lackluster for “vectorized” routines as compared to applications developed using lower-level languages like FORTRAN, C, and C++. Nevertheless, it remained in common use, and recent releases have added features that can drastically mitigate its poor performance issues, particularly through the development of a “Just-In-Time” compiler that automatically optimizes the deployment of computation accelerator resources for standard MATLAB functions. This feature enables code that performs repeated operations using for-loops or while-loops nearly as fast as equivalent code written in C. Additionally, code can be compiled into executable format using the Matlab Compiler toolbox, or used to generate equivalent C or C++ code using Matlab Coder.</p>
<h2 id="computational-resources-for-processing-large-data-sets"><span class="header-section-number">1.5</span> Computational Resources for Processing Large Data Sets</h2>
<p>Routines for extracting the activity in each cell from a collection of raw imaging data rely on simultaneous access to many pixels separated over space and time (and consequently, are separated on a disk). For long recording sessions however, the size of the collection of stored image data dramatically grows. This substantial increase in data size easily exceeds the capacity of system memory in the typical workstation computer available to most researchers. Thus, performing the necessary processing performance enhancing routines using standard programs is often unfeasible.</p>
<p>Another popular approach to this challenge is the migration of processing routines to a cluster-based system. In this way, image data can be distributed across many interconnected computer nodes capable of performing all locally restricted image processing procedures in parallel and then passing data to other nodes in the cluster for tasks that rely on comparisons made across time. Access to clusters capable of performing in this way has been historically restricted to researchers in universities or other large organization, and the diversity of cluster types is sizeable, with clusters often having very particular configuration requirements for efficiently implementing data processing jobs. These issues pose difficulty to the use and shared development of software libraries for image processing routines, although the growth of “cloud computing” services such as Amazon’s EC2 and the Google Compute Engine, as well as collaborative computing facilities such as the Massachusetts Green High-Performance Computing Center minimize several of these processing issues. Additionally, efforts to produce a standardized interface for accessing and distributing data and for managing computing resources across diverse computing environments have seen appreciable success. Apache’s release of the open-source cluster computing framework, Hadoop, and a companion data-processing engine called Spark, has encouraged a massive growth in collaborative development projects, a consequently increased the availability of robust shared libraries for data processing in a variety of applications. The Spark API can be accessed using the open-source programming Python or other languages including Java, Scala, or R. The Thunder library, a Spark package released by the Freeman lab and developed in collaboration with a number of other groups at Janelia Farm and elsewhere is specifically geared for image processing of neural imaging data.</p>
<p>Many applications will find that the recent improvements in accessibility and standardization make cluster computing an attractive and worthwhile option for processing large sets of reusable data. However, this strategy imposes harsh limitations for a neuroscientist engaged in a project that is continuously generating new data, as the time required to transfer entire imaging data sets across the internet may be prohibitive. Unfortunately, storage on the cloud is not so unlimited that it can manage an accumulated collection of imaging data generated that approximates the rate that at which sCMOS cameras operate. This rate imbalance is a central motivating issue in this project and is discussed in detail below.</p>
<p>The current generation of sCMOS cameras capture full-frame resolution video at either 30 fps or 100 fps depending on the data interface between camera and computer (USB3.0 or CameraLink). At 16-bits per pixel and 2048x2048 pixels, the maximum data rate for the USB3.0 camera is 240 MB/s. Imaging sessions typically last 30-minutes or less. However, pixels are typically binned down 2x2, and frame rate is often reduced as motivated by the constraints of processing speed and storage. However, the effect of doubling resolution on processing time when using the graphics card is virtually negligible. Identifying ROIs online and extracting the traces of neural activity allows us to discard acquired images and instead, only store the traces or feed them into an encoder for online analysis.</p>
<p>Graphics Processing Units were traditionally developed for the consumer gaming market. They are optimized for the process that involves translating a continuous stream of information into a two-dimensional image format for transfer to a computer monitor. In the context of gaming, the stream of information received by a GPU describes the state of objects in a dynamic virtual environment and is typically produced by a video game engine. These processors are highly optimized for this task. However, they are equally efficient at performing the same procedure type in reverse, reducing a stream of images to structured streams of information about dynamic objects in the image. These features render them popular for video processing and computer vision applications.</p>
<p>All GPU architecture consists of a hierarchy of parallel processing elements. NVIDIA’s CUDA architecture refers to the lowest level processing element as “CUDA Cores” and the highest level as “Symmetric Multiprocessors.” Typically, data is distributed across cores and multiprocessors by specifying a layout in C-code using different terminology, “threads” and “blocks.” Blocks are then termed to be organized in a “grid.” Adapting traditional image processing or computer vision algorithms to quickly run on a GPU involves efficiently distributing threads and ideally minimizes communication between blocks.</p>
<p>MATLAB makes processing data using the GPU seemingly trivial by overloading a large number of built in functions. Performance varies however. Writing a kernel-type subfunction is often the fastest way to implement a routine written as if it operates on single (scalar) element only that can be called on all pixels at once or employs all pixel-subscripts used by the function to retrieve the pixel value at a given subscript. The kernel-type function is compiled into a CUDA kernel the first time it’s called, then repeated calls directly contact the kernel with minimal overhead. Calls typically use the arrayfun() function.</p>
<p>Data transfers between system memory and graphics memory is often a major “bottle-neck”. Therefore, this operation is best performed only once. However, once data is available to the GPU, many complex operations can be performed to extract information from the image without exceeding the processing-time limit imposed by the frame-rate of the camera sending the images.</p>
<p>In total, this project employs advances in both software and hardware that facilitate rapid accurate image analysis of living organisms with the ultimate goals of simplifying neuronal behavior in both normal and pathologic states.</p>
<h1 id="neural-interfaces-fabrication-programming-and-assembly"><span class="header-section-number">2</span> Neural Interfaces: Fabrication, programming, and assembly</h1>
<h2 id="animal-tracking"><span class="header-section-number">2.1</span> Animal Tracking</h2>
<h3 id="behavior-box"><span class="header-section-number">2.1.1</span> Behavior Box</h3>
<figure>
<img src="img/behavior-box/task-schematic.jpg" alt="behaviorbox schematic" style="width:50.0%" /><figcaption>behaviorbox schematic</figcaption>
</figure>
<h3 id="mouse-in-a-bowl"><span class="header-section-number">2.1.2</span> Mouse in a Bowl</h3>
<div id="fig:mouse-in-a-bowl">
<p><img src="img/animal-tracking/01raw.jpg" alt="Raw frame of video being tacked" style="width:30.0%" /> <img src="img/animal-tracking/02black-and-white.jpg" alt="Area of detected mouse" style="width:30.0%" /> <img src="img/animal-tracking/03twoframes.jpg" alt="Overlay of 3 consecutive frames showing movement of mouse between each" style="width:30.0%" /></p>
<p><img src="img/animal-tracking/07mousedata1close.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/06mousedata1.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/08mousedata2.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/09mousedata1fiberon1.jpg" alt="video overlay showing tracked points" style="width:20.0%" /></p>
<p>Automated animal Tracking for “Mouse in a bowl” type experiments</p>
</div>
<h3 id="spherical-treadmill"><span class="header-section-number">2.1.3</span> Spherical Treadmill</h3>
<div id="fig:spherical-tradmill">
<p><img src="img/spherical-treadmill-VR/01-treadmill-mouse-running.jpg" alt="01-treadmill-mouse-running" style="width:30.0%" /> <img src="img/spherical-treadmill-water-delivery/01-water-port.jpg" alt="01-water-port" style="width:30.0%" /> <img src="img/spherical-treadmill-water-delivery/03-water-delivery-zoom.jpg" alt="03-water-delivery-zoom" style="width:30.0%" /></p>
<p>Spherical treadmill</p>
</div>
<h3 id="headplate-holder"><span class="header-section-number">2.1.4</span> Headplate Holder</h3>
<div id="fig:headplate-holder">
<p><img src="img/headplate-holder/photo-front.jpg" alt="front" style="width:30.0%" /> <img src="img/headplate-holder/photo-top.jpg" alt="top" style="width:30.0%" /> <img src="img/headplate-holder/photo-bottom.jpg" alt="bottom" style="width:30.0%" /></p>
<p>Headplate holder</p>
</div>
<h3 id="motion-sensors"><span class="header-section-number">2.1.5</span> Motion Sensors</h3>
<div id="fig:motion-sensors">
<p><img src="img/spherical-treadmill-motion-sensors/01-motion-sensors-installed.jpg" alt="01-motion-sensors-installed" style="width:30.0%" /> <img src="img/spherical-treadmill-motion-sensors/02-motion-sensors.jpg" alt="02-motion-sensors" style="width:30.0%" /> <img src="img/spherical-treadmill-motion-sensors/Striatum_Figure2.png" alt="Striatum_Figure2" style="width:30.0%" /></p>
<p>Motion sensors</p>
</div>
<h2 id="microscopes"><span class="header-section-number">2.2</span> Microscopes</h2>
<h3 id="microscope-construction"><span class="header-section-number">2.2.1</span> Microscope Construction</h3>
<div id="fig:widefield-microscope">
<figure>
<img src="img/microscope/widefield_microscope_diagram.png" alt="schamatic showing relation of microscope and mouse on spherical treadmill" style="width:50.0%" /><figcaption>schamatic showing relation of microscope and mouse on spherical treadmill</figcaption>
</figure>
<p><img src="img/microscope/setup1.jpg" alt="setup1" style="width:40.0%" /> <img src="img/microscope/setup2.jpg" alt="setup2" style="width:40.0%" /></p>
<p><img src="img/microscope/setup3-front.jpg" alt="setup3-front" style="width:30.0%" /> <img src="img/microscope/setup3-closeup.jpg" alt="setup3-closeup" style="width:30.0%" /> <img src="img/microscope/setup3-side.jpg" alt="setup3-side" style="width:30.0%" /></p>
<p><img src="img/microscope/setup4-front.jpg" alt="setup4-front" style="width:30.0%" /> <img src="img/microscope/setup4-closeup.jpg" alt="setup4-closeup" style="width:30.0%" /> <img src="img/microscope/setup4-side.jpg" alt="setup4-side" style="width:30.0%" /></p>
<p>Widefield fluorescence microscope</p>
</div>
<h1 id="neural-signals-computational-considerations-interpretation-and-usage"><span class="header-section-number">3</span> Neural Signals: Computational considerations, interpretation and usage</h1>
<h2 id="image-processing"><span class="header-section-number">3.1</span> Image Processing</h2>
<h3 id="image-processing-tonemapping-and-filtering"><span class="header-section-number">3.1.1</span> Image Processing: Tonemapping and Filtering</h3>
<figure>
<img src="img/sw-gui-interactive-parameter-selection-homomorphic-filter/Screenshot_20150608180058.png" alt="Screenshot_20150608180058" style="width:50.0%" /><figcaption>Screenshot_20150608180058</figcaption>
</figure>
<figure>
<img src="img/sw-fluopro/motion_correction_sample.png" alt="motion Correction" style="width:50.0%" /><figcaption>motion Correction</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-feature-generation.png" alt="feature generation" style="width:50.0%" /><figcaption>feature generation</figcaption>
</figure>
<figure>
<img src="img/2.png" alt="Pixel features useful for segmentation" style="width:50.0%" /><figcaption>Pixel features useful for segmentation</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-feature-pointwise-mutual-information.png" alt="sw-video-processing-feature-pointwise-mutual-information" style="width:50.0%" /><figcaption>sw-video-processing-feature-pointwise-mutual-information</figcaption>
</figure>
<figure>
<img src="img/sw-sequence-bw.png" alt="pixel statistics" style="width:50.0%" /><figcaption>pixel statistics</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-spatially-vs-temporally-adaptive-filter.png" alt="sw-video-processing-spatially-vs-temporally-adaptive-filter" style="width:50.0%" /><figcaption>sw-video-processing-spatially-vs-temporally-adaptive-filter</figcaption>
</figure>
<figure>
<img src="vid/trgb-013.gif" alt="trgb" style="width:50.0%" /><figcaption>trgb</figcaption>
</figure>
<figure>
<img src="img/sw-video-statistics/statistics_of_128_frames_contrast_enhanced.jpg" alt="central moments" style="width:50.0%" /><figcaption>central moments</figcaption>
</figure>
<h1 id="discussion-broader-considerations-for-clinicians-engineers-and-research-scientists-preparing-to-make-use-of-an-increasingly-hyper-connected-future"><span class="header-section-number">4</span> Discussion: Broader considerations for clinicians, engineers, and research scientists preparing to make use of an increasingly hyper-connected future</h1>
<h1 id="appendix" class="unnumbered">Appendix</h1>
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
<p><strong>Table of Contents</strong> <em>generated with <a href="https://github.com/thlorenz/doctoc">DocToc</a></em></p>
<ul>
<li><a href="#list-of-figures-unnumbered">List of Figures {.unnumbered}</a></li>
<li><a href="#introduction-background-and-literature-review">Introduction: Background and Literature Review</a></li>
<li><a href="#neural-interface-construction-fabrication-programming-and-assembly-of-of-an-automated-system-to-open-the-channel-to-your-mouses-mind">Neural Interface Construction: Fabrication, programming, and assembly of of an automated system to open the channel to your mouses mind.</a>
<ul>
<li><a href="#animal-tracking">Animal Tracking</a>
<ul>
<li><a href="#behavior-box">Behavior Box</a></li>
<li><a href="#mouse-in-a-bowl">Mouse in a Bowl</a></li>
<li><a href="#spherical-treadmill">Spherical Treadmill</a></li>
<li><a href="#headplate-holder">Headplate Holder</a></li>
<li><a href="#motion-sensors">Motion Sensors</a></li>
</ul></li>
<li><a href="#microscopes">Microscopes</a>
<ul>
<li><a href="#microscope-construction">Microscope Construction</a></li>
</ul></li>
</ul></li>
<li><a href="#neural-analytics-computational-considerations-and-approaches-to-manage-the-continuous-flow-of-neural-imaging-data-and-if-not-making-sense-of-it-perhaps-making-use-of-it">Neural Analytics: Computational considerations and approaches to manage the continuous flow of neural imaging data… and if not making sense of it, perhaps making use of it.</a>
<ul>
<li><a href="#image-processing">Image Processing</a>
<ul>
<li><a href="#image-processing-tonemapping-and-filtering">Image Processing: Tonemapping and Filtering</a></li>
</ul></li>
</ul></li>
<li><a href="#discussion-broader-considerations-for-clinicians-engineers-and-research-scientists-preparing-to-make-use-of-an-increasingly-hyper-connected-future">Discussion: Broader considerations for clinicians, engineers, and research scientists preparing to make use of an increasingly hyper-connected future</a></li>
<li><a href="#appendix-unnumbered">Appendix {.unnumbered}</a></li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->
<h1 id="list-of-figures" class="unnumbered">List of Figures</h1>

<h1 id="introduction-background-and-literature-review-1"><span class="header-section-number">5</span> Introduction: Background and Literature Review</h1>
<h1 id="neural-interface-construction-fabrication-programming-and-assembly-of-of-an-automated-system-to-open-the-channel-to-your-mouses-mind."><span class="header-section-number">6</span> Neural Interface Construction: Fabrication, programming, and assembly of of an automated system to open the channel to your mouses mind.</h1>
<h2 id="animal-tracking-1"><span class="header-section-number">6.1</span> Animal Tracking</h2>
<h3 id="behavior-box-1"><span class="header-section-number">6.1.1</span> Behavior Box</h3>
<figure>
<img src="img/behavior-box/task-schematic.jpg" alt="behaviorbox schematic" style="width:50.0%" /><figcaption>behaviorbox schematic</figcaption>
</figure>
<h3 id="mouse-in-a-bowl-1"><span class="header-section-number">6.1.2</span> Mouse in a Bowl</h3>
<div id="fig:mouse-in-a-bowl">
<p><img src="img/animal-tracking/01raw.jpg" alt="Raw frame of video being tacked" style="width:30.0%" /> <img src="img/animal-tracking/02black-and-white.jpg" alt="Area of detected mouse" style="width:30.0%" /> <img src="img/animal-tracking/03twoframes.jpg" alt="Overlay of 3 consecutive frames showing movement of mouse between each" style="width:30.0%" /></p>
<p><img src="img/animal-tracking/07mousedata1close.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/06mousedata1.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/08mousedata2.jpg" alt="video overlay showing tracked points" style="width:20.0%" /> <img src="img/animal-tracking/09mousedata1fiberon1.jpg" alt="video overlay showing tracked points" style="width:20.0%" /></p>
<p>Automated animal Tracking for “Mouse in a bowl” type experiments</p>
</div>
<h3 id="spherical-treadmill-1"><span class="header-section-number">6.1.3</span> Spherical Treadmill</h3>
<div id="fig:spherical-tradmill">
<p><img src="img/spherical-treadmill-VR/01-treadmill-mouse-running.jpg" alt="01-treadmill-mouse-running" style="width:30.0%" /> <img src="img/spherical-treadmill-water-delivery/01-water-port.jpg" alt="01-water-port" style="width:30.0%" /> <img src="img/spherical-treadmill-water-delivery/03-water-delivery-zoom.jpg" alt="03-water-delivery-zoom" style="width:30.0%" /></p>
<p>Spherical treadmill</p>
</div>
<h3 id="headplate-holder-1"><span class="header-section-number">6.1.4</span> Headplate Holder</h3>
<div id="fig:headplate-holder">
<p><img src="img/headplate-holder/photo-front.jpg" alt="front" style="width:30.0%" /> <img src="img/headplate-holder/photo-top.jpg" alt="top" style="width:30.0%" /> <img src="img/headplate-holder/photo-bottom.jpg" alt="bottom" style="width:30.0%" /></p>
<p>Headplate holder</p>
</div>
<h3 id="motion-sensors-1"><span class="header-section-number">6.1.5</span> Motion Sensors</h3>
<div id="fig:motion-sensors">
<p><img src="img/spherical-treadmill-motion-sensors/01-motion-sensors-installed.jpg" alt="01-motion-sensors-installed" style="width:30.0%" /> <img src="img/spherical-treadmill-motion-sensors/02-motion-sensors.jpg" alt="02-motion-sensors" style="width:30.0%" /> <img src="img/spherical-treadmill-motion-sensors/Striatum_Figure2.png" alt="Striatum_Figure2" style="width:30.0%" /></p>
<p>Motion sensors</p>
</div>
<h2 id="microscopes-1"><span class="header-section-number">6.2</span> Microscopes</h2>
<h3 id="microscope-construction-1"><span class="header-section-number">6.2.1</span> Microscope Construction</h3>
<div id="fig:widefield-microscope">
<figure>
<img src="img/microscope/widefield_microscope_diagram.png" alt="schamatic showing relation of microscope and mouse on spherical treadmill" style="width:50.0%" /><figcaption>schamatic showing relation of microscope and mouse on spherical treadmill</figcaption>
</figure>
<p><img src="img/microscope/setup1.jpg" alt="setup1" style="width:40.0%" /> <img src="img/microscope/setup2.jpg" alt="setup2" style="width:40.0%" /></p>
<p><img src="img/microscope/setup3-front.jpg" alt="setup3-front" style="width:30.0%" /> <img src="img/microscope/setup3-closeup.jpg" alt="setup3-closeup" style="width:30.0%" /> <img src="img/microscope/setup3-side.jpg" alt="setup3-side" style="width:30.0%" /></p>
<p><img src="img/microscope/setup4-front.jpg" alt="setup4-front" style="width:30.0%" /> <img src="img/microscope/setup4-closeup.jpg" alt="setup4-closeup" style="width:30.0%" /> <img src="img/microscope/setup4-side.jpg" alt="setup4-side" style="width:30.0%" /></p>
<p>Widefield fluorescence microscope</p>
</div>
<h1 id="neural-analytics-computational-considerations-and-approaches-to-manage-the-continuous-flow-of-neural-imaging-data-and-if-not-making-sense-of-it-perhaps-making-use-of-it."><span class="header-section-number">7</span> Neural Analytics: Computational considerations and approaches to manage the continuous flow of neural imaging data… and if not making sense of it, perhaps making use of it.</h1>
<h2 id="image-processing-1"><span class="header-section-number">7.1</span> Image Processing</h2>
<h3 id="image-processing-tonemapping-and-filtering-1"><span class="header-section-number">7.1.1</span> Image Processing: Tonemapping and Filtering</h3>
<figure>
<img src="img/sw-gui-interactive-parameter-selection-homomorphic-filter/Screenshot_20150608180058.png" alt="Screenshot_20150608180058" style="width:50.0%" /><figcaption>Screenshot_20150608180058</figcaption>
</figure>
<figure>
<img src="img/sw-fluopro/motion_correction_sample.png" alt="motion Correction" style="width:50.0%" /><figcaption>motion Correction</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-feature-generation.png" alt="feature generation" style="width:50.0%" /><figcaption>feature generation</figcaption>
</figure>
<figure>
<img src="img/2.png" alt="Pixel features useful for segmentation" style="width:50.0%" /><figcaption>Pixel features useful for segmentation</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-feature-pointwise-mutual-information.png" alt="sw-video-processing-feature-pointwise-mutual-information" style="width:50.0%" /><figcaption>sw-video-processing-feature-pointwise-mutual-information</figcaption>
</figure>
<figure>
<img src="img/sw-sequence-bw.png" alt="pixel statistics" style="width:50.0%" /><figcaption>pixel statistics</figcaption>
</figure>
<figure>
<img src="img/sw-video-processing-spatially-vs-temporally-adaptive-filter.png" alt="sw-video-processing-spatially-vs-temporally-adaptive-filter" style="width:50.0%" /><figcaption>sw-video-processing-spatially-vs-temporally-adaptive-filter</figcaption>
</figure>
<figure>
<img src="vid/trgb-013.gif" alt="trgb" style="width:50.0%" /><figcaption>trgb</figcaption>
</figure>
<figure>
<img src="img/sw-video-statistics/statistics_of_128_frames_contrast_enhanced.jpg" alt="central moments" style="width:50.0%" /><figcaption>central moments</figcaption>
</figure>
<h1 id="discussion-broader-considerations-for-clinicians-engineers-and-research-scientists-preparing-to-make-use-of-an-increasingly-hyper-connected-future-1"><span class="header-section-number">8</span> Discussion: Broader considerations for clinicians, engineers, and research scientists preparing to make use of an increasingly hyper-connected future</h1>
<h1 id="appendix-1" class="unnumbered">Appendix</h1>
<div id="refs" class="references">
<div id="ref-ince_presence_2009">
<p>Ince, R.A.A. et al., 2009. On the presence of high-order interactions among somatosensory neurons and their effect on information transmission. <em>J. Phys.: Conf. Ser.</em>, 197(1), p.012013. Available at: <a href="http://stacks.iop.org/1742-6596/197/i=1/a=012013">http://stacks.iop.org/1742-6596/197/i=1/a=012013</a>.</p>
</div>
</div>
            </body>
</html>

