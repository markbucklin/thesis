<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
    <title>video-processing - My Project</title>
    <meta name="description" content="My Stylish Documentation">
    <meta name="author" content="I, Me & Myself">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <link rel="icon" href="../themes/daux/img/favicon-blue.png" type="image/x-icon">

    <!-- Mobile -->
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Font -->
    
    <!-- CSS -->
    <link href='../themes/daux/css/theme-blue.min.css' rel='stylesheet' type='text/css'>
            <!-- Tipue Search -->
        <link href="../tipuesearch/tipuesearch.css" rel="stylesheet">
    
    <!--[if lt IE 9]>
    <script src="../themes/daux/js/html5shiv-3.7.3.min.js"></script>
    <![endif]-->
</head>
<body class=" ">
    <div class="Columns content">
    <aside class="Columns__left Collapsible">
        <button type="button" class="Button Collapsible__trigger">
            <span class="Collapsible__trigger__bar"></span>
            <span class="Collapsible__trigger__bar"></span>
            <span class="Collapsible__trigger__bar"></span>
        </button>

        <a class="Brand" href="../index.html">My Project</a>

    <div class="Search">
        <svg class="Search__icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 451 451">
            <path d="M447.05 428l-109.6-109.6c29.4-33.8 47.2-77.9 47.2-126.1C384.65 86.2 298.35 0 192.35 0 86.25 0 .05 86.3.05 192.3s86.3 192.3 192.3 192.3c48.2 0 92.3-17.8 126.1-47.2L428.05 447c2.6 2.6 6.1 4 9.5 4s6.9-1.3 9.5-4c5.2-5.2 5.2-13.8 0-19zM26.95 192.3c0-91.2 74.2-165.3 165.3-165.3 91.2 0 165.3 74.2 165.3 165.3s-74.1 165.4-165.3 165.4c-91.1 0-165.3-74.2-165.3-165.4z"/>
        </svg>
        <input type="search" id="tipue_search_input" class="Search__field" placeholder="Search..." autocomplete="on"
               results=25 autosave=text_search>
    </div>

        <div class="Collapsible__content">
            <!-- Navigation -->
            <ul class='Nav'><li class='Nav__item  has-children'><a href="#" class="aj-nav folder"><i class="Nav__arrow">&nbsp;</i>chapter1-project-prologue</a><ul class='Nav'><li class='Nav__item '><a href="../chapter1-project-prologue/animal-tracking.html">animal-tracking</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/behavior-box.html">behavior-box</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/closed-loop-diffuse-optogenetic-neuromodulation.html">closed-loop-diffuse-optogenetic-neuromodulation</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/cnc-fabrication.html">cnc-fabrication</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/intro.html">intro</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/photo-absorption-modeling.html">photo-absorption-modeling</a></li><li class='Nav__item '><a href="../chapter1-project-prologue/spherical-treadmill.html">spherical-treadmill</a></li></ul></li><li class='Nav__item  has-children'><a href="#" class="aj-nav folder"><i class="Nav__arrow">&nbsp;</i>chapter2-widefield-fluorescence-microscopy</a><ul class='Nav'><li class='Nav__item '><a href="../chapter2-widefield-fluorescence-microscopy/computer-workstation-build-and-configuration.html">computer-workstation-build-and-configuration</a></li><li class='Nav__item '><a href="../chapter2-widefield-fluorescence-microscopy/headplate-and-holder.html">headplate-and-holder</a></li><li class='Nav__item '><a href="../chapter2-widefield-fluorescence-microscopy/microscopy-1.html">microscopy-1</a></li><li class='Nav__item '><a href="../chapter2-widefield-fluorescence-microscopy/optical-window.html">optical-window</a></li></ul></li><li class='Nav__item  has-children'><a href="#" class="aj-nav folder"><i class="Nav__arrow">&nbsp;</i>chapter3-image-processing-pipeline</a><ul class='Nav'><li class='Nav__item '><a href="../chapter3-image-processing-pipeline/image-processing.html">image-processing</a></li></ul></li><li class='Nav__item Nav__item--open has-children'><a href="#" class="aj-nav folder"><i class="Nav__arrow">&nbsp;</i>chapter4-online-stream-processing</a><ul class='Nav'><li class='Nav__item Nav__item--active'><a href="../chapter4-online-stream-processing/video-processing.html">video-processing</a></li></ul></li><li class='Nav__item  has-children'><a href="#" class="aj-nav folder"><i class="Nav__arrow">&nbsp;</i>chapter5-developing-for-tomorrow</a><ul class='Nav'><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/cameras.html">cameras</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/compression.html">compression</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/data-scaling.html">data-scaling</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/distributed-dataflow-and-streaming.html">distributed-dataflow-and-streaming</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/information-and-informativity.html">information-and-informativity</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/microscopy-2.html">microscopy-2</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/sensors.html">sensors</a></li><li class='Nav__item '><a href="../chapter5-developing-for-tomorrow/siliskull.html">siliskull</a></li></ul></li><li class='Nav__item '><a href="../discussion.html">discussion</a></li><li class='Nav__item '><a href="../introduction.html">introduction</a></li></ul>

            <div class="Links">
                            </div>

                                <div class="CodeToggler">
                        <hr/>
                                                    <label class="Checkbox">Show Code Blocks                                <input type="checkbox" class="CodeToggler__button--main" checked="checked"/>
                                <div class="Checkbox__indicator"></div>
                            </label>
                                            </div>
                
                
                        </div>
    </aside>
    <div class="Columns__right Columns__right--full">
        <div class="Columns__right__content">
            <div class="doc_content">
                <article class="Page">

    <div class="Page__header">
        <h1><a href="../chapter4-online-stream-processing/video-processing.html">chapter4-online-stream-processing</a> <svg class="Page__header--separator" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 477.175 477.175"><path d="M360.73 229.075l-225.1-225.1c-5.3-5.3-13.8-5.3-19.1 0s-5.3 13.8 0 19.1l215.5 215.5-215.5 215.5c-5.3 5.3-5.3 13.8 0 19.1 2.6 2.6 6.1 4 9.5 4 3.4 0 6.9-1.3 9.5-4l225.1-225.1c5.3-5.2 5.3-13.8.1-19z"/></svg> <a href="../chapter4-online-stream-processing/video-processing.html">video-processing</a></h1>
                    </div>

    <div class="s-content">
        <h1 id="page_section_1">Video Processing</h1>
<!-- This section borrows from AIM-1 and AIM-2 from the prospectus. -->
<h2 id="page_section_2">Continuous Online Video Processing</h2>
<h2 id="page_section_3">Introduction</h2>
<p>The current generation of sCMOS cameras can capture full-frame resolution video at either 30 fps or 100 fps, depending on the data interface between camera and computer (USB3.0 or CameraLink). At 16-bits per pixel and 2048x2048 pixels, the maximum data rate for the USB3.0 camera is 240 MB/s. Imaging sessions typically last 30-minutes or less. However, pixels are typically binned down 2x2, and frame rate often reduced; processing speed and storage constraints are the primary motivation for doing so. The effect of doubling resolution on processing time when using the graphics card is nearly negligible, however. By identifying ROIs online and extracting the traces of neural activity allows us to discard acquired images and instead store the traces only, or feed them into an encoder for online analysis.</p>
<p>Graphics Processing Units were traditionally developed for the consumer gaming market. They are optimized for the process which involves translating a continuous stream of information into a two-dimensional image format for transfer to a computer monitor. In the context of gaming, the stream of information received by a GPU describes the state of objects in a dynamic virtual environment, and is typically produced by a video game engine. These processors are highly optimized for this task. However, they are equally efficient at performing the same type of procedure in reverse -- reducing a stream of images to structured streams of information about dynamic objects in the image -- and thus are popular for video processing and computer vision applications.</p>
<p>Any GPU architecture will consist of a hierarchy of parallel processing elements. NVIDIA's CUDA architecture refers to the lowest level processing element as &quot;CUDA Cores&quot; and the highest level as &quot;Symmetric Multiprocessors.&quot; Typically data is distributed across cores and multiprocessors by specifying a layout in C-code using different terminology, &quot;threads&quot; and &quot;blocks.&quot; Blocks are then said to be organized in a &quot;grid.&quot; Adapting traditional image processing or computer vision algorithms to run quickly on a GPU involves finding a way to distribute threads efficiently, ideally minimizing communication between blocks.</p>
<p>MATLAB makes processing data using the GPU seemingly trivial by overloading a large number of built in functions. Performance varies, however, and often the fastest way to implement a routine is by writing a kernel-type subfunction -- written as if it operates on single (scalar) elements only -- that can be called on all pixels at once, or all pixel-subscripts, which the function can then use to retrieve the pixel value at the given subscript. The kernel-type function is compiled into a CUDA kernel the first time it's called, then repeated calls call the kernel directly, having minimal overhead. Calls go through the arrayfun() function.</p>
<p>Data transfers between system memory and graphics memory is often the major bottle-neck. Therefore, this operation is best performed only once. However, once data is on the GPU, many complex operations can be performed to extract information from the image, all while staying under the processing-time limit imposed by the frame-rate of the camera sending the images.</p>
<h2 id="page_section_4">Method and Approach</h2>
<p>The entire procedure for processing images and extracting cell signals can be performed in substantially less time than most commonly available tools using the approach described in Aim 1, particularly the methods for restricting the spatial extent of pixel-association operations, and distributing operations across parallel processing cores using a Single Program Multiple Data (SPMD) archetype. However, the total time still exceeds that of the acquisition session. Inefficiency arises from the overhead involved with distributing data and passing information between separate parallel processes. Graphics cards, however execute in what's called Single Instruction Multiple Data (SIMD) fashion, to distribute computation across the thousands of processing cores.</p>
<p>The processing components are implemented using the MATLAB System-Object framework, which allows for slightly faster performance through internal optimizations having to do with memory allocation. Most system objects, each representing one step in the serial processing and signal-extraction procedure, also have companion functions that implement the computation-heavy components of each algorithm using a pre-compiled CUDA kernel.</p>
<h3 id="page_section_5">Benchmarking &amp; General Performance</h3>
<p>Built-in MATLAB functions that execute on the GPU can be profiled with benchmarking functions like <em>gputimeit()</em>, or with the <em>tic/toc</em> functions. When execution isn't fast enough, they need to be replaced with custom functions. The custom functions typically achieve the speed up necessary by enabling the operation to carried out on several frames at once. This reduces the over-head costs inposed for each function call by spreading it over several frames. This solution is not ideal, as it increases the latency of solutions, however does not preclude implementation in real-time system if the procedures are adapted to run on a real-time hybrid system-on-module like NVIDIA's Tegra X1, which should involve minimal effort once a standard set of successful procedures is realized. The current implementation tests the processing time of each stage of the process to ensure that the sum is less than the acquisition time for each frame dictated by the inverse of the frame-rate (30-50 milliseconds).</p>
<h3 id="page_section_6">Buffered Operations</h3>
<p>Combining frames for each operation can result in near linear speedup. For example, for the phase-correlation step required for motion correction, the FFT and IFFT are called on 16 image-frames at once, and the time take to accomplish is approximately the same as if the operation were called on 1 frame. This essentially leads to a 16x speedup, though the latency is also increased slightly. The best size to use is difficult to pre-determine, and typically must be measured for varying size 'chunks' using the benchmarking functions indicated above. The system objects manage the details necessary to allow buffered chunks of video to be passed to each stage without introducing artifacts at the temporal edges between chunks.</p>
<h3 id="page_section_7">Image Pre-Processing &amp; Motion Correction</h3>
<p>Pre-processing is implemented as with the offline procedure, with a few changes. Images are aligned in chunks, and they are aligned sequentially to two templates. One template is the most recent stable frame from the preceding chunk. The other is a recursively temporal-low-pass filtered image that mitigates slow drifts. Aligning to the first template is usually more stable as the brightness of cells in the recent image will be more similar to those in the current chunk than will be the brightness of cells in the slow-moving average.</p>
<p>The displacement of each frame is found to sub-pixel precision, then used with a custom bicubic resampling kernel that replaces any pixels at the edges with images from the moving average.</p>
<h3 id="page_section_8">Sequential Statistics</h3>
<p>A number of statistics for each pixel are updated online and can be used for normalization and segmentation procedures later in the process. These include the minimum and maximum pixel intensity, and the first four central moments, which are easily converted to the mean, variance, skewness, and kurtosis. The formulas for making these calculations are given below, and are performed in a highly efficient manner as data are kept local to each processing core, and repeat computations are minimized.</p>
<p>n = n + 1;</p>
<p>% GET PIXEL SAMPLE</p>
<p>f = F(rowIdx,colIdx,k);</p>
<p>% PRECOMPUTE &amp; CACHE SOME VALUES FOR SPEED</p>
<p>d = single(f) - m1;</p>
<p>dk = d/n;</p>
<p>dk2 = dk^2;</p>
<p>s = d*dk*(n-1);</p>
<p>% UPDATE CENTRAL MOMENTS</p>
<p>m1 = m1 + dk;</p>
<p>m4 = m4 + s*dk2*(n.^2-3*n+3) + 6*dk2*m2 - 4*dk*m3;</p>
<p>m3 = m3 + s*dk*(n-2) - 3*dk*m2;</p>
<p>m2 = m2 + s;</p>
<p>% UPDATE MIN &amp; MAX</p>
<p>fmin = min(fmin, f);</p>
<p>fmax = max(fmax, f);</p>
<p>Furthermore, the value used to update each central moment at each point in time can be used as a measure of change in the distribution of each pixel caused by the current pixel intensity, as explained next.</p>
<h3 id="page_section_9">Non-Stationarity &amp; Differential Moments</h3>
<p>Stationary refers to the property of a signal such that its statistics do not vary over time, i.e. its distribution is stable. Neural signals tend to specifically <em>not</em> have this property, in contrast to other measurable components such as those contributed by physiologic noise (heart-rate, respirations, etc.). Thus, by analyzing the evolution of statistical measures calculated for each pixel as frames are added in sequence gives a highly sensitive indicator of neural activity. This is done using a routine analogous to that for updating central moments given above, except the values returned are not only the updated moment, but also the updating component -- essentially the partial derivative with respect to time. This is illustrated below, including the normalization functions which convert the partial-moment values to their variance, skewness, and kurtosis analogues:</p>
<blockquote>
<p>% COMPUTE DIFFERENTIAL UPDATE TO CENTRAL MOMENTS</p>
<p>dm1 = dk;</p>
<p>m1 = m1 + dm1;</p>
<p>dm4 = s*dk2*(n^2-3*n+3) + 6*dk2*m2 - 4*dk*m3;</p>
<p>dm3 = s*dk*(n-2) - 3*dk*m2;</p>
<p>dm2 = s;</p>
<p>m2 = m2 + dm2;</p>
<p>% NORMALIZE BY VARIANCE &amp; SAMPLE NUMBER -&gt; CONVERSION TO dVar, dSkew, dKurt</p>
<p>dm2 = dm2/max(1,n-1);</p>
<p>dm3 = dm3*sqrt(max(1,n))/(m2^1.5);</p>
<p>dm4 = dm4*n/(m2^2);</p>
</blockquote>
<p>These functions run on images representing the image intensity, and also on images taken from sequential differences indicating the temporal derivative of image intensity. The combination of outputs from these operations indicate both when image intensities are significantly high relative to past distribution, and also when intensities are changing significantly faster than learned from their past distribution.</p>
<h3 id="page_section_10">Surface Classification: Peaks, Edges, Curvature</h3>
<p>Edge-finding methods are employed for establishing boundaries between cells, and first and second-order gradients are used to compute local measures of curvature from an eigenvalue decomposition of the local Hessian matrix. I won't go into detail, as the utility of these procedure in the most recent implementation has been lost, but nevertheless, the operation is optimized and ready to be plugged back in when further development calls for better accuracy informing cell-segmentation, or when a faster or more accurate motion-correction algorithm is called for.</p>
<h3 id="page_section_11">Online Cell Segmentation &amp; Tracking</h3>
<p>Cells are segmented by first running sequential statistics on the properties of identifiable regions on a pixel-wise basis. That is, as regions are identified in a method similar to that used offline in Aim 1, the region-properties are calculated (Centroid, Bounding-Box, etc.) and statistics for these properties are updated at each pixel covered by a proposed region. After sufficient evidence has gathered, Seeds are generated by finding the local peak of a seed-probability function that optimizes each pixel's proximity to a region centroid, and distance from any boundary. Regions are grown from these seed regions, and registered in a hierarchy that allows for co-labeling of cellular and sub-cellular components. Newly identified regions occur as new seeds, where as seeds overlapping with old regions are used to identify sub-regions, or to track regions over time.</p>
<h3 id="page_section_12">Signal Extraction from Subcellular Compartments</h3>
<p>I also have functions for the extraction of normalized Pointwise-Mutual-Information (nPMI), which can operate on a pixel-to-pixel basis or on a region-to-pixel basis. This operation accumulates mutually informative changes in all pixels in the maximal bounding-box (e.g. 64x64 pixels) surrounding each identified regions centroid. The weights given by this function can take on values between -1 and 1, and can be used to inform any reduction operations to follow. Additionally, spatial moments can indicate the subcellular distribution of activity across the identified region. In this context, the first spatial moment M~00~ indicates the mean signal intensity.</p>
<h3 id="page_section_13">User Interface for Parameter Tuning</h3>
<p>Some system-objects also incorporate a user interface to aid in parameter selection for tuning.</p>
    </div>

        <nav>
        <ul class="Pager">
            <li class=Pager--prev><a href="../chapter3-image-processing-pipeline/image-processing.html">Previous</a></li>            <li class=Pager--next><a href="../chapter5-developing-for-tomorrow/cameras.html">Next</a></li>        </ul>
    </nav>
    </article>

            </div>
        </div>
    </div>
</div>

    
    <!-- JS -->
    <script src="../themes/daux/js/jquery-1.11.3.min.js"></script><script src="../themes/daux/js/highlight.pack.js"></script><script src="../themes/daux/js/daux.js"></script>
            <script>
            
            window.searchLanguage = "";
            window.searchTranslation = {"Search_one_result":"1 result","Search_results":"!count results","Search_no_results":"Nothing found","Search_common_words_ignored":"Common words are largely ignored","Search_too_short":"Search too short","Search_one_character_or_more":"Should be one character or more","Search_should_be_x_or_more":"Should be !min characters or more","Link_previous":"Previous","Link_next":"Next"};
        </script>

        <!-- Tipue Search -->
        <script type="text/javascript" src="../tipuesearch/tipuesearch.js"></script>

        <script>
            window.onunload = function(){}; // force $(document).ready to be called on back/forward navigation in firefox
            $(function() {
                tipuesearch({
                    'base_url': '../'
                });
            });
        </script>
    
</body>
</html>
