<h1 id="forward">Forward</h1>
<p>I have structured this document to roughly coincide with a chronological account of 6 years spent in a neuro-oriented biomedical engineering lab. My role in the lab was centered around exploratory device design and development, mostly targeting application in neuroscience research, with intended users being neuroscientist colleagues. One of the lab’s most remarkable assets is the breadth and diversity of its constituents in terms of their skills and experience, both within and between the engineering/development and the science/medical sides of the lab. All efforts stood to benefit from the close proximity to skilled colleagues, most notably for the complementary guide and provide roles that assisted the development process of new devices and the experiments they were intended for.</p>
<p>My initial experience in optoelectronic device development was as an undergrad at Columbia University where I was advised by Elizabeth Hillman, and developed a device that combined thermography and near-infrared spectroscopy in a portable and inexpensive device intended to provide early detection of adverse neoplastic changes through at-home daily monitoring, particularly targeting use by patients with high-risk for breast cancer. I then went to the Das Lab where I developed macroscopic imaging systems used for intrinsic imaging in the visual cortex of awake primates. As a MD/PhD student, I attempt to maintain a potential to adapt the end-products of each development for clinical applicability. The story presented here is rather unusual in that success precedes failure. The volume of tangible presentable results is greatest toward the beginning stages of the work described here. This unusual inversion is what make this story worth hearing, however. Thank you for taking the time to read this. I hope that at least the technical information provided herein, if not the procedural insight, is valuable in your current or future endeavors.</p>
<!-- Every day my colleagues and I are surprised and amazed by the bizarre twists and turns we observe.  -->
<!-- Occasionally left bewildered, not quite capable --- or perhaps unwilling to recall --- the motivational catch phrase that is stamped and stamped again, the meat of the intro, and stamped again to bring discussion to an end... -->
<h1 id="abstract">Abstract</h1>
<p>The latest generation of genetically encoded sensors emerged from molecular engineering labs are highly sensitive. These -  combined with equally critical advances in the performance of affordable image sensor – have been put to use in labs conducting research neuroscience research to enable high-throughput detection of neural activity in behaving animals using both multi-photon and traditional wide-field fluorescence microscopy. Unfortunately, expanded sensing capability can generate a flow of data in proportions that challenge the standard procedures used to process, analyze, and store captured video. The torrent can easily overwhelm and debilitate, even when applying the latest and greatest from our ever-expanding arsenal of cluster computing resources. Sensing capabilities available to scientists, physicians and engineers will continue to grow exponentially, while traditional raw data storage and batch-processing routines will impose the same limits on throughput utilization.</p>
<p>The work presented here demonstrates the ease with which a dependable and affordable wide-field fluorescence imaging system can be assembled, and integrated with behavior control and monitoring system such as found in a typical neuroscience laboratory. Application of standard image processing and computer vision routines demonstrates the remarkable value of such a system, but also highlights the woeful inability of standard batch processing routines to manage the volume of data available. After describing a slew of marginally successful naive attempts to pre-shrink long streams of raw video data to more manageable proportions, a more likely plan is presented.</p>
<p>Here you will find the strategic ingredients to consider if your intent is to transform an abundant flow of raw data into proportionally informative knowledge. Certainly, aggressive deployment of streamed computation on graphics processing hardware will be vital component, but not solely sufficient. A likely solution will also recognize opportunities afforded by implementing performance-tuned data structures, modular and dynamically reconfigurable data processing elements, and graph oriented stream semantics coordinating data-flow. # Introduction</p>
<h2 id="optical-imaging-of-neural-activity">Optical Imaging of Neural Activity</h2>
<p>Optical techniques for observing neural activity have advanced recently owing to both an evolution of digital imaging technology, and the development of engineered proteins that act as fluorescent indicators of neural activity. Image sensors, like those found in scientific-CMOS (sCMOS) cameras are larger, faster, and more sensitive than what was previously available in science-grade cameras. Meanwhile, the latest generation of Genetically Encoded Calcium Indicators (GECIs), collectively called GCaMP6, reports fluctuations in neural activation with extremely high fidelity. This combination of developments enables neuroscientists to open a wider channel to the brain than previously possible – using conventional epifluorescence microscopy techniques – enabling simultaneous recording from hundreds to thousands of neurons. Expanding the fraction of the observable neurons in an interconnected network may provide insight into mechanistic properties of neural disease, or may lead to a better understanding of neural coding. Additionally, feeding a large set of neural response information to a machine learning algorithm in a neuroprosthetic application may provide improved predictive performance, even if the exact mechanism of prediction remains difficult to discern. However, a few major challenges currently prevent realization of the potential benefits that these new technologies offer:</p>
<ol type="1">
<li><p>The increased size of raw data from a single imaging session can easily overwhelm the computational resources typically used to process similar but smaller sets of data.</p></li>
<li><p>The accumulation of raw data on disk over multiple imaging sessions quickly exceeds the data-storage capacity of most lab-scale servers, forcing researchers to halt data collection to process and delete, a nightmare scenario for some.</p></li>
<li><p>The experimental design and data analysis procedures that neuroscientists are familiar with applying for network activity data when there are 5 to 10 cells will produce highly biased spurious results, unless provided with many more stimulus-response repetitions, i.e. trials. The number of repeated trials sufficient for producing an accurate description of the neural response to any stimulus is on the order of 2<sup>N</sup>, where N is the number of neurons being measured.</p></li>
</ol>
<p>The objective of this project is to establish procedures that can address these challenges, then use these procedures to evaluate the effect that expanding available neural response input has on performance of a closed-loop encoder. This closed-loop encoder will attempt to predict changes in motor state of a mouse running on a ball, using sensors on the ball to train the encoder. It will then use the predicted motor state to modulate motor state in another mouse using opsins. This can be thought of as a model neuroprosthetic whos function is to overcome dysfunction caused by pathologically disconnected brain areas, such as exists in Parkinson’s disease (PD). The goal will be to increase synchronization of mice beyond chance, such that they tend to run together and rest together.</p>
<p>Below I provide some background on the general procedure for offline video processing. I also discuss some of the issues with carrying out these procedures on a large dataset, and the variety of approaches that I and others have attempted for dealing with the issue. I then introduce the streaming approach (i.e. Aim 2), which is capable of processing video during acquisition and extracting signals directly, saving relevant signals only and discarding or compressing the raw video. This approach relies on GPU programming, so I also provide some background on the application of graphics cards for computationally demanding tasks. Using a graphics card for programming in the MATLAB environment is also discussed.</p>
<!-- Aim 1: Build a library of adaptable software that enables neuroscientists to acquire, process, analyze, and visualize large volumes of fluorescence imaging data from awake behaving animals. -->
<p>Capturing wide-field fluorescence images at high spatial and temporal resolution enables us to measure functional dynamic changes in many cells within a large interconnected network. Extracting a measure for each cell in a way that preserves spatial and temporal continuity with uniform/unbiased sampling of the observed signal is achievable, but implementing a procedure to accomplish the task can be made difficult by a number of factors. One class of computer-vision procedure commonly applied to this task is image-segmentation (cell-segmentation in histology applications), a procedure that seeks to represent distinct objects in an image by association of each image pixel with one of any number of abstract objects, or with the background. A variety of algorithms exist for performing this operation efficiently on single images. Most methods can be extended to operate in a 3<sup>rd</sup> dimension, applied to stacks of image frames to enable tracking cells at multiple depths, or equivalently over time.</p>
<p>However, motion induced by physiologic changes and animal movement necessitates alignment of all frames in the sequence. Moreover, the massive fluctuations in signal intensity from individual and spatially overlapping cells can breed unstable solutions for alignment and radically complicate cell identification routines by disrupting temporal continuity. Implementing a reliable procedure for identifying and tracking the same cells in each frame throughout the sequence thus becomes non-trivial.</p>
<h2 id="procedures-for-calcium-imaging">Procedures for Calcium Imaging</h2>
<p>The general goal of processing image data from functional fluorescence imaging experiments is to restructure raw image data in a way that maps pixels in each image frame to distinct individual cells or subcellular components, called ‘Regions-Of-Interest’ (ROI). Pixel-intensity values from mapped pixels are typically then reduced by combination to single dimensional ‘trace’ time-series. These traces indicates the fluorescence intensity of an individual neuron over time, and the collection approximates the distinct activity of each and every neuron in the microscope’s field of view. However, this task is made difficult by motion of the brain throughout the experiment, and also by the apparent overlap of cells in the image plane captured from the camera’s 2-dimensional perspective. These issues can be partially mitigated with a few image pre-processing steps – alignment of images to correct for motion being the most critical. These options are described in the Methods &amp; Approaches section below. Most software packages geared specifically toward functional imaging implement either of two basic classes of pixel-&gt;cell mapping algorithms. One approach is to use image-segmentation routines for computer vision, which seeks to combine adjacent pixels into distinct spatially segregated regions representing objects in the image.</p>
<p>The other common approach is to perform an eigenvalue decomposition on the covariance matrix from a stack of image frames (also called spectral decomposition, or Principal Component Analysis, PCA), resulting in an assembly of basis vectors defining the weighting coefficients for each pixel. Multiplying the basis-vectors (i.e. “components”) with all frames produces a one-dimensional trace for each component. The linear combination is similar to the weighted image-segmentation method in that it assigns fractional coefficients to pixels. However the procedure for computing the covariance matrix employed by PCA operates on as many pixels as are in the image, multiplying each with every other pixel – a problem with <em>np<sup>2</sup></em> complexity, where <em>p</em> is the number of pixels in the image. I mention these issues inherent to PCA not because this project will attempt to address them, but because this project was initiated following tremendous difficulty attempting to use PCA-based cell sorting methods with large datasets.</p>
<h2 id="computer-software-environments-for-image-processing">Computer Software Environments for Image Processing</h2>
<p>The widespread usage of MATLAB in neuroscience communities lends potential for greater usability and easier adaptation to software developed in this environment. While software development environments with a focus on “ease-of-use” have traditionally presumed crippling sacrifices to computational performance, this assumption is getting to be less accurate.</p>
<p>Standard programs include ImageJ, the built-in routines in MATLAB’s Image Processing Toolbox, Mosaic from Inscopix, which is merely a compiled version of MATLAB routines which uses the MATLAB engine, Sci-Kits Image for Python, and a remarkable diversity of other applications. MATLAB is a commercial software development platform which is geared toward fast production and prototyping of data processing routines in a high-level programming language. It implements several core libraries (LINPACK, BLAS, etc.) that make multithreaded operations on matrix type data highly efficient. While MATLAB has traditionally been a considered the standard across neuroscience research labs, it was also well recognized that its performance was lacking for routines that aren’t “vectorized”, when compared to applications developed using lower-level languages like FORTRAN, C, and C++. Nevertheless, it remained in common use, and recent releases have added features that can drastically mitigate its performance issues, particularly through the development of a “Just-In-Time” compiler that automatically optimizes the deployment of computation accelerator resources for standard MATLAB functions. This feature enables code that performs repeated operations using for-loops or while-loops nearly as fast as equivalent code written in C. Additionally, code can be compiled into executable format using the Matlab Compiler toolbox, or used to generate equivalent C or C++ code using Matlab Coder.</p>
<h2 id="computational-resources-for-processing-large-data-sets">Computational Resources for Processing Large Data Sets</h2>
<p>Routines for extracting the activity in each cell from a collection of raw imaging data rely on an ability to simultaneous access many pixels separated over space and time (and consequently separated on disk). For long recording sessions, however, the size of the collection of stored image data grows dramatically. This substantial increase in the size of data easily exceeds the capacity of system memory in the typical workstation computer available to researchers. Thus, performing the necessary processing routines using standard programs is often unfeasible.</p>
<p>Another popular approach to this challenge is the migration of processing routines to a cluster-based system. In this way image data can be distributed across many interconnected computer nodes capable of performing all locally restricted image processing procedures in parallel, then passing data to other nodes in the cluster for tasks that rely on comparisons made across time. Access to clusters capable of performing in this way has historically been restricted to those working in large universities or other large organization, and the diversity of cluster types is sizeable, with clusters often having very particular configuration requirements for implementing data processing jobs efficiently. These issues would pose some difficulty to the use and shared development of software libraries for image processing routines, although the growth of “cloud computing” services such as Amazon’s EC2 and the Google Compute Engine, and also collaborative computing facilities like the <a href="http://www.mghpcc.org">Massachusetts Green High-Performance Computing Center</a> mitigate many of these issues. Additionally, efforts to produce a standardized interface for accessing and distributing data, and for managing computing resources across diverse computing environments have seen appreciable success. Apache’s release of the open-source cluster computing framework, Hadoop, and a companion data-processing engine called <a href="http://spark.apache.org/">Spark</a>, has encouraged a massive growth in collaborative development projects, a consequently increased the availability of robust shared libraries for data processing in a variety of applications. The Spark API can be accessed using the open-source programming Python, and also using other languages like Java, Scala, or R. One project specifically geared for image processing of neural imaging data is the Thunder library, a Spark package released by the Freeman lab and developed in collaboration with a number of other groups at Janelia farm and elsewhere.</p>
<p>Many applications will find the recent improvements in accessibility and standardization make cluster computing an attractive and worthwhile option for processing a very large set of reusable data. However, this strategy would impose harsh limitations for a neuroscientist with a project that is continuously generating new data, as the time required to transfer entire imaging data sets across the internet may be prohibitive. Unfortunately, storage on the cloud is not so unlimited that it can manage an accumulated collection of imaging data generated at anything near the rate that sCMOS cameras are capable of producing. This rate imbalance is a central motivating issue for Aim 2 this project, and is discussed in more detail below.</p>
<!-- Aim 2: Extend the software for continuous real-time processing on a GPU.  -->
<!-- TODO:repeated later (video-procesing section) -->
<p>The current generation of sCMOS cameras can capture full-frame resolution video at either 30 fps or 100 fps, depending on the data interface between camera and computer (USB3.0 or CameraLink). At 16-bits per pixel and 2048x2048 pixels, the maximum data rate for the USB3.0 camera is 240 MB/s. Imaging sessions typically last 30-minutes or less. However, pixels are typically binned down 2x2, and frame rate often reduced; processing speed and storage constraints are the primary motivation for doing so. The effect of doubling resolution on processing time when using the graphics card is nearly negligible, however. By identifying ROIs online and extracting the traces of neural activity allows us to discard acquired images and instead store the traces only, or feed them into an encoder for online analysis.</p>
<p>Graphics Processing Units were traditionally developed for the consumer gaming market. They are optimized for the process which involves translating a continuous stream of information into a two-dimensional image format for transfer to a computer monitor. In the context of gaming, the stream of information received by a GPU describes the state of objects in a dynamic virtual environment, and is typically produced by a video game engine. These processors are highly optimized for this task. However, they are equally efficient at performing the same type of procedure in reverse – reducing a stream of images to structured streams of information about dynamic objects in the image – and thus are popular for video processing and computer vision applications.</p>
<p>Any GPU architecture will consist of a hierarchy of parallel processing elements. NVIDIA’s CUDA architecture refers to the lowest level processing element as “CUDA Cores” and the highest level as “Symmetric Multiprocessors.” Typically data is distributed across cores and multiprocessors by specifying a layout in C-code using different terminology, “threads” and “blocks.” Blocks are then said to be organized in a “grid.” Adapting traditional image processing or computer vision algorithms to run quickly on a GPU involves finding a way to distribute threads efficiently, ideally minimizimg communication between blocks.</p>
<p>MATLAB makes processing data using the GPU seemingly trivial by overloading a large number of built in functions. Performance varies, however, and often the fastest way to implement a routine is by writing a kernel-type subfunction – written as if it operates on single (scalar) elements only – that can be called on all pixels at once, or all pixel-subscripts, which the function can then use to retrieve the pixel value at the given subscript. The kernel-type function is compiled into a CUDA kernel the first time it’s called, then repeated calls call the kernel directly, having minimal overhead. Calls go through the <em>arrayfun()</em> function.</p>
<p>Data transfers between system memory and graphics memory is often the major bottle-neck. Therefore, this operation is best performed only once. However, once data is on the GPU, many complex operations can be performed to extract information from the image, all while staying under the processing-time limit imposed by the frame-rate of the camera sending the images.</p>
<!-- Aim 3: Detect motor states from extracted neural activity and apply to closed-loop neuromodulation. -->
<p>The function of the brain is to translate/encode sensory input into neural output that actuates an effect that promotes survival of the organism or propagates to promote the survival of offspring (generation of a response). It does this by communicating input through interconnected neurons via converging and diverging connections which comprise the neural network. One way we study the brain is by testing and observing the properties of individual neurons and the response to changing conditions at the direct connections they form with others. Another way is by observing a collection of neurons and to measure their response to variable conditions in their external environment, either by recording or stimulating variations in sensory input, or measuring an organisms physical/behavioral response.</p>
<p>One might presume that the expansion of information provided by being able to measure activity from a larger proportion of cells in a network would make it easier to analyze stimulus-response type experiments and gain insight about underlying functional mechanisms. Unfortunately, the correlation and information theoretic procedures traditionally used to make these associations suffer from a systematic bias that grows exponentially with the number responses considered for each stimulus (i.e. the number of cells included). The number of trials necessary for overcoming this bias gets exponentially large, though methods do exist for bias correction, such as through shuffling/resampling tests.</p>
<p>A systems neuroscience experiment will benefit from online feedback in one or both of two ways:</p>
<ol type="1">
<li><p>For an experiment that seeks to learn the neural response/pattern associated with a <em>specific</em> <em>stimulus</em>, it can inform the user whether the current number of trials – i.e. repeated presentations of the stimulus – will be sufficient for overcoming <em>limited sampling bias</em>. This could be done by testing pattern hypotheses online to subsets of the collected data and assessing their stability.</p></li>
<li><p>If the intention of the experiment is to study neural coding in general, for which it’s sufficient to have an <em>arbitrary stimulus</em>, then online pattern recognition feedback can aid in maximizing the information in the response about that stimulus, either by directing modification of the stimulus, or directing modification of the field-of-view.</p></li>
</ol>
<p>Streaming processing addresses the issues of processing and storing for sufficient learning from large networks possible. Additionally, I propose a strategy in the methods section by which incorporating this online processing stream into stimulus-response-type experiments could help correct <em>limited sampling bias</em>, enabling neural coding analysis in large populations of neurons <span class="citation" data-cites="ince_presence_2009">[@ince_presence_2009]</span>.</p>
<p>Overall, however, the third goal of this project will focus on the ability to use the expanded information made available by the first two project components to train an encoder that predicts intended motor states from one healthy mouse, and uses the predictions to direct neuromodulatory control of another mouse. This setup will simulate pathologic disconnection in a brain, and will test the ability to distinguish intention to start or stop running, and apply that in a way that performance is easily measureable.</p>
<h1 id="sec:project-prologue">Project Prologue</h1>
<p>This chapter describes several projects that were started early during my graduate studies. Each project is similar in that they are outside the realm of optical imaging of neural activity, which is the focus of the rest of this dissertation. Nevertheless, they are included here because the issues they bring up will later inform the approach I take in the work described in later chapters. The projects described in the following sections are also tied together by a common goal: to enable research in the neurosciences with translation potential for clinical applications.</p>
<h2 id="sec:neuroscience-technology-development-background">Neuroscience Technology Development Background</h2>
<ul>
<li>Electrophysiology, Histology, Functional brain imaging
<ul>
<li>pros/cons of each</li>
<li>very little compromise in tradeoffs</li>
</ul></li>
<li>Filling the gaps between these three approaches ## Behavior Box {#sec:behavior-box}</li>
</ul>
<p>I built an experiment apparatus for mice to enable a study being run by Jia-Min Zhuo. The goal of the study wasto elucidate the role of adult-born neurons on mouse behavior, specifically their performance in discrimination tasks. We called the apparatus the “Behavior Box” and modeled it after a commercially available but grossly over-priced box that itself came from other labs <span class="citation" data-cites="clelland2009">[@clelland2009, @creer2010]</span>.</p>
<p>The chamber was constructed with black plastic walls, extruded aluminum framing, and a perforated metal mesh floor 1 cm above a plastic waste tray. A 10-inch infrared touchscreen (ITouch Systems) was mounted over a 10-inch LCD monitor forming one wall of the chamber. An opaque mask with seven windows was placed over the screeen to limit where the mouse could touch. A water pump with infrared detector was located at teh other end of the chamber to provide reward for the water-deprived mice in the study. A white LED strip encircled the chamber from the top, and multiple speakers positioned outside to deliver sound cues. A web camera was fixed above the chamber to record and monitor mouse activity. My contribution to this project was the program for interact with all the system components. This program controlled and recorded experiment progress. I developed the program in MATLAB, and the main components of its function are described below.</p>
<h3 id="sec:ir-touchscreen">IR Touchscreen</h3>
<p>The IR touchscreen provided a robust measure of the location of any contact with the animal’s paws or nose. The screen was more reliable than either <em>resistive</em> or <em>capacitive</em> touchscreens, which are much more common in devices like POS systems and mobile phones respectively.</p>
<p>Files in this folder are used to run our “BehaviorBox” system, which features easily customizable control of experiments involving an infrared touchscreen and LCD display along with speakers, water-ports, lights, essentially anything that can be controlled electronically.</p>
<p>COGENT 2000 The graphics/visual-stimulation package used is missing from this folder due to size, but can be downloaded from the <a href="http://www.vislab.ucl.ac.uk/cogent_2000.php">source</a></p>
<h3 id="sec:framesynx-toolbox">FrameSynx Toolbox</h3>
<p>The FrameSynx toolbox for MATLAB was built to synchronize continuous image acquisition with experiments conducted in the neuroscience laboratory setting. While the experiments are conducted in separate software (and potentially on a different computer), FrameSynx listens for messages to start/stop the experiment, start a trial, etc. and responds accordingly by controlling one or multiple cameras and illumination devices, and synchronizing this information with the data acquired. The major contribution to the “Behavior Box” package, and also to later image processing packages is the procedure for definition and storage and of experimental data files, which will be touched on briefly in <span class="citation" data-cites="sec:data-file">[@sec:data-file]</span></p>
<h3 id="sec:data-file">Data File</h3>
<h2 id="sec:animal-tracking">Animal Tracking</h2>
<h3 id="sec:using-computer-vision-to-track-position-and-orientation">Using Computer Vision to track Position and Orientation</h3>
<p>A webcam-based motion tracking box constructed to analyze the movement of our unilaterally lesioned PD mouse model. Video is recorded at 15 frames per second and processed on-line or off-line using a function written in MATLAB. Briefly, this function converts each frame to a black and white image (logical matrix), uses morphological filtering functions to isolate the mouse (remove mouse excrement) and identify its body (remove the tail), then finds the center of mass in cartesian coordinates (maximum center of projection on x- and y-axes), and the rostral-caudal orientation measured in degrees off the x-axis. Orientation is determined by the index of maximum of a radon transform of the binary image. Processing is accomplished at a rate of 15-16 fps, using a single core, or 64 fps using parallel processing on a quad-core processor with multi-threading enabled. The advantage of this apparatus over the virtual-reality system is that it allows free movement of an untrained mouse, with real-time movement metrics at nearly the same rate as the spherical treadmill.</p>
<!--  for pandoc-crossref syntax use the following
<div id="fig:animal-tracking-bowl">

</div>
 -->
<!--  Science.md
![caption](location/name.png){#fig:label}
 -->
<div id="fig:animal-tracking-bowl">
<p><img src="../../../img/animal-tracking/01raw.jpg" alt="raw" id="fig:animal-tracking-bowl-raw" /> <img src="../../../img/animal-tracking/02black-and-white.jpg" alt="black-and-white" id="fig:animal-tracking-bowl-black-and-white" /> <img src="../../../img/animal-tracking/03twoframes.jpg" alt="twoframes" id="fig:animal-tracking-bowl-twoframes" /> <img src="../../../img/animal-tracking/05tail_ID.jpg" alt="tail_ID" id="fig:animal-tracking-bowl-tail_ID" /> <img src="../../../img/animal-tracking/06mousedata1.jpg" alt="mousedata1" id="fig:animal-tracking-bowl-mousedata1" /> <img src="../../../img/animal-tracking/07mousedata1close.jpg" alt="mousedata1close" id="fig:animal-tracking-bowl-mousedata1close" /> <img src="../../../img/animal-tracking/08mousedata2.jpg" alt="mousedata2" id="fig:animal-tracking-bowl-mousedata2" /> <img src="../../../img/animal-tracking/09mousedata1fiberon1.jpg" alt="mousedata1fiberon1" id="fig:animal-tracking-bowl-mousedata1fiberon1" /> <!-- Caption: Processing steps for automated rotation counting procedure used in hemiparkinsonian mouse study --></p>
</div>
<h2 id="spherical-treadmill-virtual-reality">Spherical Treadmill &amp; Virtual-Reality</h2>
<!-- ## Spherical Treadmill -->
<p>A virtual reality system was assembled, adopting methods from the Harvey lab lab <span class="citation" data-cites="harvey_intracellular_2009">[@harvey_intracellular_2009]</span>. This system allows placement of a head-restrained mouse on an 8-inch diameter polystyrene foam ball supported by a cushion of compressed air, surrounded by a toroidal projection screen. Ball rotation is tracked with two optical computer mice placed orthogonal to each other. Movement vectors are fed into a virtual-reality engine that updates the image projected onto a toroidal screen surrounding the ball, simulating movement through any arbitrary virtual world. Movement vectors are recorded as an arbitrarily scaled translation in the mouse-relative X and Y axes and rotation around the Z axis, at approximately 30 ms intervals. This behavioral apparatus has the advantage of allowing trivial measurement of the mouse’s movement ability while the mouse is head-fixed. The disadvantage is the time and potential confounds involved with training individual mice to use the system.</p>
<h3 id="treadmill-contstruction">Treadmill contstruction</h3>
<h4 id="spherical-treadmill">Figure:</h4>
<p><img src="img/0spherical-treadmill-VR/01-treadmill-mouse-running.jpg" alt="spherical-treadmill-VR/01-treadmill-mouse-running" id="spherical-treadmill-VR1-treadmill-mouse-running" /> <img src="img/0spherical-treadmill-VR/02-treadmill-front.jpg" alt="spherical-treadmill-VR/02-treadmill-front" id="spherical-treadmill-VR2-treadmill-front" /> <img src="img/0spherical-treadmill-VR/03-treadmill-top.jpg" alt="spherical-treadmill-VR/03-treadmill-top" id="spherical-treadmill-V03-treadmill-top" /> Caption: this treadmill supports behavior.</p>
<h3 id="water-delivery">Water Delivery</h3>
<h4 id="water-delivery">Figure:</h4>
<p><img src="../../../img/0spherical-treadmill-water-delivery/01-water-port.jpg" alt="spherical-treadmill-water-delivery/01-water-port" id="spherical-treadmill-water-deliver01-water-port" /> <img src="../../../img/0spherical-treadmill-water-delivery/02-water-delivery.jpg" alt="spherical-treadmill-water-delivery/02-water-delivery" id="spherical-treadmill-water-delivery2-water-delivery" /> <img src="../../../img/0spherical-treadmill-water-delivery/03-water-delivery-zoom.jpg" alt="spherical-treadmill-water-delivery/03-water-delivery-zoom" id="spherical-treadmill-water-delivery3-water-delivery-zoom" /> Caption: waterport</p>
<h3 id="motion-sensors">Motion Sensors</h3>
<p>Motion sensing was implemented using a linux computer and standard mice at first, and later using precision laser navigation sensors for “gaming” mice and custom firmware written to work with any arduino-compatible microcontroller. <!-- todo --></p>
<h4 id="generic-usb-computer-mouse-with-minimal-linux">Generic USB Computer Mouse with Minimal Linux</h4>
<p>Run “mouse_relay.py” on any computer running linux to send xy-data from 2 USB optical computer mice to another computer over an RS-232 serial-port connection. The receiving computer (in this implementation) uses MATLAB to read the values and translate the xy-values from 2 mice on the surface of a sphere into 3 values corresponding to rotation of that sphere around 3 orthogonal axes (XYZ) with their origin at the sphere’s center.</p>
<p>RECEIVING FUNCTIONS: The MATLAB class that receives the serial input (xy-values from both mice) is called “VrMovementInterface”</p>
<p>The MATLAB function that translates the double-stream of xy-values from the sphere’s surface into rotation around its center is called “moveBucklin.m” and is located in the VIRMEN “movements” folder.</p>
<p>SERIAL FORMAT: XY-Values are transmitted in ‘packets’ using an ascii formatted string terminated by a newline. Each packet contains the Sensor Number (s) that the reading is coming from, followed by the X-Value (dx), then the Y-Value (dy). The python code looks like the following:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="op">&gt;</span> datastring <span class="op">=</span> s <span class="op">+</span> <span class="st">&#39;x&#39;</span><span class="op">+</span>dx <span class="op">+</span> <span class="st">&#39;y&#39;</span><span class="op">+</span>dy <span class="op">+</span> <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span></a></code></pre></div>
<p>For example:</p>
<pre class="csv"><code>&gt; s1x34y-3</code></pre>
<h4 id="navigation-sensor-chip-with-arduino">Navigation Sensor Chip with Arduino</h4>
<p>Works with ADNS library (Mark Bucklin) to pass [dx,dy] measurements from two ADNS-9800 laser mouse sensors (placed 45-degrees apart on surface of styrofoam ball).</p>
<h4 id="motion-sensors">Figure:</h4>
<p><img src="../../../img/0spherical-treadmill-motion-sensors/01-motion-sensors-installed.jpg" alt="spherical-treadmill-motion-sensors/01-motion-sensors-installed" id="spherical-treadmill-motion-sensors-motion-sensors-installed" /> <img src="../../../img/0spherical-treadmill-motion-sensors/02-motion-sensors.jpg" alt="spherical-treadmill-motion-sensors/02-motion-sensors" id="spherical-treadmill-motion-sensors-motion-sensors" /> Caption: Motion Sensors for tracking ball movement. can be used to control and record speed and direction of mouse movement within the VR world. ## Closed-Loop Diffuse Optogenetic Neuromodulation</p>
<!-- carbon fiber electrode & LED -->
<!-- (Deep-Target Carbon-Fiber Electrode Array and Bilateral Illumination) -->
<p>Chronic implant for long-term multi-site recording and optogenetic neuromodulation. The implant is fixed to a mouse’s skull with dental cement, and sits on top of bilateral 5 mm craniotomies. Electrodes are driven through 32 guiding-tunnels along pre-determined trajectories to bilateral targets in thalamus, striatum, prefrontal cortex, motor cortex and auditory cortex. 2 High-intensity LEDs are coupled to optical fibers driven to mediodorsal and centromedial thalamic nuclei. Electrode trajectories are computed in stereotaxic coordinates and imported into CAD model to construct guiding-tunnel features that facilitate correct placement of electrode tips in brain targets and connection to a circuit board. A-C) CAD model of implant. D) Circuit board for electrode termination and LED power.</p>
<!-- from submitted NRSA/F31 Application -->
<p>Brain disease, often accompanied by enormous personal and economic costs, continues to emerge as among the most pressing contributors to the global disease burden. Unprecedented advances in biotechnology and in portable electronics support tremendous opportunity to conduct research with excellent potential for advanced understanding, improved treatments, and one day cures for these devastating diseases, disorders and conditions. The aim of this project was to would leverage technology from the fast-moving forefronts of electronics and biomedical research to build a next-generation neuroprosthetic.</p>
<h3 id="background">Background</h3>
<p>Deep Brain Stimulation (DBS) has been used clinically since the early 1990’s, and is currently approved by the FDA for treatment of Parkinson’s disease (PD) and essential tremor, with Humanitarian Device Exemptions for OCD and dystonia. In 1987 a French neurosurgeon observed during a thalamotomy procedure - where stimulation was applied for localization of ventralis intermedius - that high-frequency stimulation was able to suppress an extrapyramidal tremor (<span class="citation" data-cites="benabid_combined_1987">@benabid_combined_1987</span>; <span class="citation" data-cites="liker_deep_2008">@liker_deep_2008</span>). Subsequent studies in a non-human primate Parkinson’s model suggested high-frequency stimulation (~100-200 Hz) of the Subthalamic Nucleus (STN) was as effective as a lesion, in that it“jammed” neural activity at the tip of the electrode, yet it was tunable and reversible (<span class="citation" data-cites="eusebio_does_2012">@eusebio_does_2012</span>). Long term efficacy is around 50% using a standard clinical scale (UPDRS III)(<span class="citation" data-cites="beuter_delayed_2009">@beuter_delayed_2009</span>); DBS can provide symptomatic improvement in tremor, rigidity, and bradykinesia, and can also facilitate a substantial reduction in levodopa dosage and consequent reduction in its major side-effects, i.e. dyskinesia (<span class="citation" data-cites="eusebio_does_2012">@eusebio_does_2012</span>). While DBS has substantially improved our ability to treat PD and other movement disorders, the therapeutic effect remains insufficient and unpredictable.</p>
<p>The mechanism by which DBS suppresses parkinsonian symptoms is still uncertain. However, there are growing evidence to suggest that increases in synchronized oscillatory activity in basal ganglia and thalamo-cortical circuits – so-called <em>Beta-frequency</em> oscillations – represents a pathologic and moreover <em>symptomatic</em> state in PD, the disruption of which is accomplished by high-frequency stimulation (<span class="citation" data-cites="wilson_chaotic_2011">@wilson_chaotic_2011</span>). Meanwhile, synchronized activity in the beta range is also involved in many normal cognitive functions, and abnormal synchronization is likewise a mechanism for numerous other neurologic and psychiatric disorders (<span class="citation" data-cites="velazquez_biophysical_2012">@velazquez_biophysical_2012</span>),(<span class="citation" data-cites="andrews_neuromodulation_2010">@andrews_neuromodulation_2010</span>). Consequently DBS has potential to become an effective treatment for epilepsy (<span class="citation" data-cites="ii_dynamic_2013">@ii_dynamic_2013</span>), OCD(<span class="citation" data-cites="bourne_mechanisms_2012">@bourne_mechanisms_2012</span>), depression ( <span class="citation" data-cites="mayberg_deep_2005">@mayberg_deep_2005</span>, <span class="citation" data-cites="mayberg_targeted_2009">@mayberg_targeted_2009</span>), Alien Hand Syndrome and Tourette’s (<span class="citation" data-cites="krack_deep_2010">@krack_deep_2010</span>), obesity (<span class="citation" data-cites="taghva_obesity_2012">@taghva_obesity_2012</span>), alcoholism (<span class="citation" data-cites="heldmann_deep_2012">@heldmann_deep_2012</span>), addiction (<span class="citation" data-cites="luigjes_deep_2012">@luigjes_deep_2012</span>), autism (<span class="citation" data-cites="sturm_dbs_2013">@sturm_dbs_2013</span>), Alzheimer’s disease (<span class="citation" data-cites="laxton_deep_nodate">@laxton_deep_nodate</span>), and schizophrenia (<span class="citation" data-cites="white_dysregulated_2013">@white_dysregulated_2013</span>). There are a large number of case-reports describing off-label DBS treatments, and a few small clinical trials (<span class="citation" data-cites="kennedy_deep_2011-1">@kennedy_deep_2011-1</span>; <span class="citation" data-cites="sarem-aslani_industrial_2011">@sarem-aslani_industrial_2011</span>). In 2009 Medtronic received a Humanitarian Device Exemption (HDE) to market a DBS device for OCD, called <em>Reclaim</em> (“Reclaim<sup>TM</sup> Deep Brain Stimulation for Obsessive Compulsive Disorder (OCD) Therapy - H050003”2013). While closed-loop DBS might be even better suited for the complex dynamics of psychiatric disorders, most investigators have stuck with movement and seizure disorders (<span class="citation" data-cites="sarem-aslani_industrial_2011">@sarem-aslani_industrial_2011</span>), presumably feeling more comfortable with well-established animal models and FDA-approved applications; or perhaps feeling <em>less</em> comfortable with the ethical questions inherent to treating of behavior.</p>
<h3 id="project-plan">Project Plan</h3>
<p>Deep brain stimulation (DBS) has revolutionized the treatment of neurological and psychiatric diseases over the past couple decades. A DBS device is essentially a cardiac pacemaker reconfigured to stimulate a small region of brain near the tip of an electrode with high-frequency pulses of electrical current; despite this simplicity, DBS has been used widely for treating movement disorders, such as Parkinson’s disease (PD) and dystonia. The impressive performance of DBS on alleviating movement deficits associated with PD and dystonia has motivated a number of ongoing clinical trials on assessing its use in treating other brain disorders, such as obsessive compulsive disorder and depression. However, benefits of DBS treatment are unpredictable and show varying degrees and patterns of symptom suppression across patients. Unfortunately, this technology in its current iteration has been unable to overcome these limitations. This is likely due in part to a need for neural implant devices that are capable of more numerous and specific stimulation contact areas. Additionally, the DBS mechanism of action remains unknown and this further limits its ability to become wholly sufficient treatment. The central goal of this project is to develop a novel adaptive neuromodulation platform with increased neural contacts and their specificity as well as to examine the therapeutic mechanisms of DBS. My hypothesis was that aberrant neural network dynamics underlie the behavioral symptoms of brain disorders, and DBS therapeutic effect is through scrambling pathological neural network patterns. While I would primarily focus on PD here, the principles discovered through this study should be easily generalized to design novel therapies for other neurological and psychiatric disorders.</p>
<h4 id="sec:f31-aim1">Design a novel adaptive neuromodulation platform for closed loop control of neural activities</h4>
<p>Functional neural systems underlying brain disorders are likely complex, impacting interconnected brain areas. To reliably distinguish normal and pathologic neural network states, such as the “on/off” states in PD, it would be advantageous to simultaneously measure neural activities in multiple relevant brain areas. The platform proposed here consists of a bidirectional neural interface with penetrating silicon electrodes capable of simultaneously recording from multiple sites interspersed with LED-coupled optical-fibers for multi-point optogenetic neuromodulation. I would use the commercially available multicontact silicon electrodes, known as Michigan probes, that are 5um thick and capable of recording up to hundreds of locations simultaneously. To perform neuromodulation with high spatiotemporal precision, I would use optogenetic methods to activate or silence specific neurons expressing rhodopsins in mice using certain colors of light. Even though this system is designed for mice in this study, the principles demonstrated here have a clear translational path to humans.</p>
<h4 id="sec:f31-aim2">Implement a library of closed loop modulation protocols with machine-learning algorithms</h4>
<p>To efficiently determine the neural network states responsible for a specific disease condition, I would use machine-learning algorithms to implement a library of optimized strategies. Specifically, this system consists of two parts: The first is a closed-loop control system responsible for gathering and analyzing streams of neural and behavioral data in real-time to estimate a neurobehavioral state, and then applying a state-specific optogenetic control with patterned light into the brain. The second component is a slower and more complex routine that records and analyzes the streams of neural and behavioral data, and assesses the performance of state-estimates and controls applied by the real-time component. State-response strategy updates would be periodically fed from the thoughtful controller (2<sup>nd</sup> component) to the real-time decision maker (1<sup>st</sup> component) using variations of traditional machine-learning algorithms to generate successively optimized strategies.</p>
<h4 id="sec:f31-aim3">Systematically evaluate the neural circuit mechanisms underlying DBS therapeutic effect on PD</h4>
<p>Even though the precise therapeutic mechanism of DBS is unclear, DBS has revealed tremendous information about the brain’s functionings and dysfunctionings. For example, pathological beta oscillations, oscillations around 20Hz, have been widely observed in the cortical-basal gangion circuits in PD patients implanted with DBS electrodes. As the causal role of beta oscillations is yet to be established, it is likely that the neural network states identified in Aim 2 would conform and converge into distinct archetypes, such as beta oscillations within the cortical-basal gangion circuits relevant to PD. I hypothesize that neuromodulation outside of the classical PD neural circuit would be therapeutic, in so far as such neuromodulation is capable of scrambling pathological network dynamics. Once proven correct, the proposed therapeutic mechanisms of DBS through altering neural network pathological representationswould provide a basis for a new generation of neural circuit based neuromodulation therapies.</p>
<h3 id="significance">Significance</h3>
<p>This proposal focuses on developing a novel closed-loop neuromodulation system using optogenetics and an intelligent control algorithm. This approach enables automation of a search for individual-optimized stimulation patterns, and adaptation of these patterns over time in response to fluctuating symptoms.</p>
<h4 id="justification-for-this-approach">Justification for This Approach</h4>
<p>The novel closed-loop neuromodulation system as presented here has far reaching implications. These include improved patient symptoms, fewer surgical procedures ( for battery replacement) and broader application in terms of patient population and disease profiles. The increase in number and range of stimulation sites and rapid real-time response rate of the proposed mechanism would yield such results (Lozano and Lipsman 2013). Furthermore, the information on DBS mechanism gleamed from this study would have immediate effects on the current technology</p>
<h4 id="shortcomings">Shortcomings</h4>
<p>Any device that relies on optogenetics to deliver stimulation to neurons inherently shares the same hurdles to clinical translation, the requirement for gene-therapy and its associated risks. Several early trials of viral transfection of cells had adverse effects including a greatly increased risk of carcinoma. In these early studies, the DNA insertion location was uncontrolled leaving important regions of DNA tumor suppressor genes exposed to damage. New methods that improve the safety of gene therapy have been developed. Several of the more recent methods utilize adeno-associated virus (AAV) and show greater control regarding DNA insertion and decreased DNA damage. These more recent methods suggest a possibility that with continuing research, such a method may be developed without the potential for malignancy.</p>
<p>Working on a project that requires a technology that does not as of yet exist, represents one of the greatest educational benefits of this project. That leap of faith into a future that does not exist requires us to depend on each other as a team of collaborators, as each of our work depends on the others’ success. In order to succeed, we must do so together; and without eachother, our therapeutics would never reach their ultimate goal, the patient. As such, we would share each success and setback in the same way whether such events occur within our own labs or labs across the country.</p>
<h3 id="research-design-and-methods">Research Design and Methods</h3>
<p>The initial stages of this project were focused designing the physical components of the bi-directional neural interface – and the peripheral aspects of aim 2 – development of an automated behavioral assay for our prototypic animal model of Parkinson’s disease.</p>
<h4 id="neural-interface">Neural Interface</h4>
<p>A <em>chronic neural implant</em> is required for this project, as the machine-learning component of this closed-loop system would require time and signal stability to function effectively. While our lab currently employs chronically implanted drivable tetrode arrays, this project would benefit from dropping this complexity in favor of <em>static</em> electrodes that are set to their prescribed position during the initial surgery, and never thereafter.</p>
<p>Sustained penetration of brain tissue with large (15 microns) diameter electrodes provokes a chonic inflammatory reaction to the foreign body eventually encapsulates the electrode in a “glial scar” and inhibits regeneration of neural process in the vicinity (<span class="citation" data-cites="polikov_response_2005">@polikov_response_2005</span>). The chronic implant designed for this project would use silicon electrodes to record from deep brain targets, and for shallow targets would employ <em>carbon fiber electrodes</em> (4-10 microns) using methods developed in a nearby collaborating lab (<span class="citation" data-cites="guitchounts_carbon-fiber_2013">@guitchounts_carbon-fiber_2013</span>). Carbon fiber is cheaply available with a range of electrical properties, but the common properties of high modulus and small diameter – 5-10 micron vs. 30-50 microns for typical commercial microwire arrays (<span class="citation" data-cites="ward_toward_2009">@ward_toward_2009</span>) – allows carbon fiber to penetrate brain tissue without deforming, and to remain long-term without provoking an inflammatory reaction (<span class="citation" data-cites="kozai_ultrasmall_2012">@kozai_ultrasmall_2012</span>).</p>
<h4 id="leds">LEDs</h4>
<p>High-intensity LEDs would be incorporated into the implant in favor of a traditional fiber-coupled laser primarily for their ability to <em>diffusely</em> illuminate neurons over a <em>large area</em> and their <em>scalability</em>, which would be vital for <em>multi-site</em> and <em>chronic</em> modulation. LEDs do not couple efficiently to optical fibers of small diameter (~200 microns is typical in optogenetics labs), but they can provide a far greater <em>luminous intensity</em> in a greater variety of wavelengths than lasers, at a fraction of the cost. The proposed project would incorporate LEDs built into the implant, with their emitting surfaces <em>closely coupled to the</em> <em>brain surface</em>.</p>
<p>LED arrays, and Digital Micromirror Devices (DMDs) and Liquid Crystal on Silicon (LcoS) chips have been considered as a means of greater spatial control of light intensity; any of these would be incorporated if the technology improves during the lifetime of the project, but the current state of technology places heavy restrictions on space and/or power requirements to merit incorporation into the implant. Incorporating multiple LEDs (4 to 8) would enable semi-independent activation of neurons in widely-separated brain regions, the proposed project would need to <em>overlap</em> this diffuse control with more precise targeted delivery of opsins (<span class="citation" data-cites="packer_targeting_2013">@packer_targeting_2013</span>).</p>
<h4 id="pcb-top">Figure: PCB schematic for head-mounted carbon fiber electrode array</h4>
<p><img src="/images/pcb/pcb_top1.png" alt="top 1" id="pcb-top1" style="height:1in" /> <img src="/images/pcb/pcb_top2.png" alt="top 2" id="pcb-top2" style="height:1in" /> <img src="/images/pcb/pcb_top3.png" alt="top 3" id="pcb-top3" style="height:1in" /> Caption: Circuit Board for connection to carbon-fiber electrodes and LEDs</p>
<p>Here I describe the design strategy for the physical implant outlined above that would provide pathways to and from the mouse brain for the the electrodes and LEDs described above, as well as the circuit board that sits within this implant and consolidates all <em>input/output</em> signals for connection with a computer. This project would strive to ensure the bidirectional chronic neural implant is <em>resilient</em> to the abuse it would surely endure over the weeks to months of service <em>affixed to the cranium</em> of a laboratory mouse. The outcome we wish to avoid is <em>movement</em> of the penetrating electrodes within the brain tissue when external forces act on the implant, or when internal forces cause the animals brain to move relative to its skull. Moreover, if the electrode is unable to move with the brain there is substantial risk of electrode breakage.</p>
<p>We are designing the implant to address this potential issue from two directions: the first strategy is to make the implant as <em>tolerable</em> for the animal as possible by minimizing its weight, rounding all edges, and fabricating with a bioinert material such as polymethylmethacrylate (PMMA), polyetheretherketone (PEEK), and silicone elastomer. Elastomer or functionally similar material would be used to encase the printed circuit board (PCB) once electrodes are driven to their prescribed depth and electrical connections made. The encasement would cover the entirety of the board with the exception of two connectors for electrophysiology readout and LED power, and a heatsink connector for each LED . Heatsinks for the LEDs would either be copper tube or carbon fiber. The second strategy for minimizing electrode movement involves <em>floating</em> the connection between the electrodes and the PCB. The goal of this procedure is to allow the electrode – with one end in the brain and the other in a plated through-hole – to move vertically and laterally relative to the PCB while maintaining electrical contact with the pad. The method for accomplishing this is still being developed; fortunately its success is not critical to the project, but merely an improvement.</p>
<h4 id="electrophysiology">Electrophysiology</h4>
<p>The neural interface proposed in this document would record 32 signals simultaneously. To minimize the risk that poor trajectory planning could have on the results described in <span class="citation" data-cites="sec:f31-aim3">[@sec:f31-aim3]</span>, we would specify targets redundantly. Thus, for each round of testing with our Parkinson’s mouse model we would select 8 brain areas to record from redundantly and bilaterally. 7 of the 8 would be selected from a set of brain areas for which claims have been made in the literature suggesting functional relevance to Parkinson’s disease; as a <em>control</em> for each round, the 8<sup>th</sup> would be selected from a set of areas for which no connection with PD has ever been reported. Each round would begin with a modified CAD model of the implant (for modified electrode trajectories), which would be 3D printed or fabricated on a 4-axis CNC-mill in the lab.</p>
<h4 id="front-end-digitization">Front-end Digitization</h4>
<p>Recording would be accomplished with the RHD2132 digital electrophysiology interface chip produced and made freely available by Intan (“RHD2000 Amplifier Chips | Intan Technologies” 2013). The chip accomplishes as much as a traditional electrophysiology system at a price that is orders of magnitude less, and a package that takes up much less space.</p>
<h4 id="signal-filtering-lfp-and-mua">Signal Filtering, LFP and MUA</h4>
<p>Electrophysiology data would be digitally filtered on the Intan chip using parameters selected by the machine-learning routine. The only hard limits on bandwidth would be dictated by the requirements of the analog to digital converter (ADC), i.e. a high-pass filter to remove drifting DC offsets and a low-pass filter to prevent aliasing. These limits would vary depending on electrode impedance and sampling frequency, but would have cutoff frequencies or roughly 0.1 Hz and 10 KHz respectively. While there is much published in regard to the spatial extent and information content in common named frequency bands, the algorithm that selects filtering parameters would not incorporate any of these assumptions.</p>
<p>That being said, we expect the learning algorithm would find more <em>reliably relevant</em> features in the classic Local Field Potential (LFP) range, which is typically low-pass filtered with a cut-off around 300 Hz. It’s thought that LFP represents a sum of neural activity from cells within a 50-250 micron radius of the electrode tip, or more if activity is highly correlated (Lindén et al. 2011). This spatial scale allows for variability in electrode position across subjects, and for small movements of the electrode over time (Andersen, Musallam, and Pesaran 2004).</p>
<!-- Aim 2 – Optimization of closed-loop stimulation pattern with machine-learning algorithms:
----------------------------------------------------------------------------------------- -->
<p>Closed-loop input/output would be handled at three levels by multiple computers.</p>
<h4 id="inputoutput">Input/Output</h4>
<p>The lowest level – implemented on the Intan RHD2132 amplifier chip and RHD2000 evaluation board – handles analog-to-digital conversion and filtering of electrophysiology data. Also at this level – but implemented on a different computer – are the movement tracking functions described above.</p>
<h4 id="closed-loop-state-estimation-and-adaptive-neuromodulation">Closed-loop State Estimation and Adaptive Neuromodulation</h4>
<p>Streams of electrophysiology and behavior/movement data are sent to the second level, a <em>real-time</em> computer system encapsulating all the <em>closed-loop</em> functions of the platform. This computer would use the <em>Real-time eXperiment Interface</em> (RTXI; www.rtxi.org), an open-source Linux-based operating system supported by the NIH (Lin et al. 2010). This system provides modules for data acquisition, storage, synchronization of output, and more. RTXI would be installed on an ARM-based embedded system such as the open-source <em>Puggle</em>, or a similar embedded processor from Texas Instruments. What is <em>Puggle</em>? <em>“Puggle is ARM-based, real-time data acquisition and processing tool. It is designed to sense, process, and react to both analog and digital input signals in hard real-time. Puggle is designed for closed-loop electrophysiology applications…_ (<a href="http://www.puggleboard.com)" class="uri">http://www.puggleboard.com)</a>._”</em> The closed-loop routine running at this level would analyze neural and behavioral data using parameters from the next higher level. The analysis step would conclude with two estimates for <em>neural</em> and <em>behavioral state</em>, with this combination <em>mapped</em> to an output routine with parameters that are also passed down by the higher level. The specifics of the output routine would be handled by a connected microcontroller, would be interrupt-driven, and would ultimately control the power of optical output from each LED over time.</p>
<h4 id="generation-and-evaluation-of-stimulus-paradigm">Generation and Evaluation of Stimulus Paradigm</h4>
<p>The highest level of processing would be implemented in a <em>non-real-time</em> environment, MATLAB on Windows. Raw data and activity logs from the closed-loop routine would be written to disk continuously. A machine-learning routine would analyze these data and send updated parameters to the closed-loop computer as described below. All parameter updates would also be written to disk in synchrony with acquisition for retrospective analysis as described by <span class="citation" data-cites="sec:f31-aim3">[@sec:f31-aim3]</span>.</p>
<h4 id="machine-learning-optimization">Machine-Learning &amp; Optimization</h4>
<p>The process for selecting exactly how to stimulate a brain follows a common routine both in the clinic and research laboratory. After having spatially localized an electrode in the intended position, physicians (and neuroscientists) would begin stimulation with a temporal pattern that is fully defined with two or three parameters (e.g. a binary pulse-sequence with constant amplitude, frequency, and duty-cycle). Effects of stimulation are often immediately apparent. The physician or neuroscientist – let’s just call this person the “<em>decision-maker</em>” – would evaluate the subjects behavior and compare this evaluation to the desired <em>target</em> behavior. In the example case of PD the deviations from this “target behavior” include akinesia, gait-inbalances, and postural instability, but might also include iatrogenic responses such as dyskinesias or diplopia. The decision-maker then draws on their experience to estimate which parameters should be adjusted, and with what magnitude in order to shift behavior towards the intended target. In the absence of experience, the decision-maker relies on <em>education</em> (many years of it in either the physician or scientist’s case). The first adjustment is applied, and the effect on behavior is evaluated once again. The inexperienced decision-maker learns something new, and can use this new information in combination (or perhaps even <em>in contrast!)</em> with their prior education to estimate the effect of adjustments in any or all available parameters, and make a decision that begins the cycle again.</p>
<p>Humans are excellent learners – undoubtedly better than any other species on Earth. Computers are not; they are painfully literal and would not flip a bit without being told to. Their persistence, however, is their saving grace, and is the aspect that makes machine learning possible, given creatively designed seeds of human thought and recursive and inversive instructions for expanding this seed. The process of cyclic evaluation and decision-making described above would be used as a model for a machine-learning algorithm with two goals: optimizing the identification of <em>disease-relevant neural states</em>, and optimizing a neuromodulation routine for <em>disease-related symptom reduction</em>.</p>
<h4 id="identification-of-neural-state">Identification of Neural State</h4>
<p>The algorithm used to optimize closed-loop identification of neural neural state would be developed from a set of common <em>pattern-recognition</em> algorithms and <em>semi-supervised learning</em> models, including support vector machines (SVMs), logistic, polynomial, or linear regression, principal component regression (PCR), Kalman filtering, and hidden Markov models (HMM). These algorithms would be applied to recorded data and <em>selected for implementation</em> in the closed-loop routine based on measures of performance (discussed below) and ability to meet computation-time specifications (&lt;~250 ms).</p>
<h4 id="stimulus-sequence-generation">Stimulus Sequence Generation</h4>
<p>Each neural state would be mapped to a neuromodulation routine that is applied continuously (with <em>sub-millisecond</em> response time) until the next state-estimate and routine update. Each stimulation sequence would initially be defined by random selection from a set of sequence <em>archetypes</em>, and further specification by random selection of <em>archetype-specific-parameters</em>. The real-time process would handle application of the state-mapped sequence, but the sequence formation routines would be run in the non-real-time process. Both open-loop (for control/comparison) and closed-loop stimulation archetypes would be generated from three sources, with generic examples from each source in the table below:</p>
<ol type="1">
<li><p>Common achetypes reported in literature, e.g. DBS-style constant pulse generation.</p></li>
<li><p>Novel archetypes generated in creative thinking and discussion sessions with colleagues.</p></li>
<li><p>Concocted archetypes from chaotic-generation algorithm</p></li>
</ol>
<p><em>Constant DBS-type square wave</em> with parameters for amplitude, frequency, and pulse-width/duty-cycle (open-loop) LED power (stimulation) in each area of control proportional to <em>arctangent of phase difference</em> between two areas. Chaotically generate functions by recombining elements from the novel and common archetypes, and also by applying every function available in MATLAB (use <em>what</em> and <em>lookfor</em> functions applied in <em>try…catch…</em> blocks to find statements that work with given data) <em>Triggered impulse response</em> with parameters defining a threshold or feature from any channel that triggers stimulation of some shape that initiates at some delay (Rosin et al. 2011) (closed-loop) Stimulation is continuously proportional a <em>ratio of band-pass filtered signal magnitude</em> from arbitrarily selected channels.<br />
<em>Phase Cancellation</em> of filtered signal from any defined channel (Brittain et al. 2013) (closed-loop) Signal from one channel is used to construct <em>wavelet</em> then convolved with other channels with output determining pulse-shape</p>
<p>The number of possible archetypes used by sourcing in these ways would be extensive, so much so that trying them manually would be unmanageable. This is where the automated machine-learning algorithm which selects, applies, evaluates, and optimizes each routine for several hours every day becomes absolutely necessary.</p>
<h4 id="stimulus-optimization">Stimulus Optimization</h4>
<p>As the real-time component runs its state-response cycle and logs data, the non-real-time machine-learning component would evaluate its performance and update parameters in an attempt to optimize <em>symptom suppression</em>. This evaluation process would identify “hot” parameters (those with large effect), and continue sending parameter updates until symptom suppression cannot be optimized further. This process would be slow to allow observation of subtle effects of stimulation in noisy data. If manipulation of all available parameters yields no changes in behavior, a new stimulation archetype would be tried.</p>
<p>This type of process is sometimes referred to as a <em>Genetic</em> <em>Algorithm</em>, where the parameters for a particular archetype would be viewed as the <em>genes</em> that define a species. The <em>fitness</em> of each individual combination genes (i.e. parameters) determines survival and consequent ability to pass genes to the next generation. In our implementation, we’d be running multiple species (i.e. archetypes) as well, with the expectation that many would be quickly declared extinct.</p>
<h3 id="systematic-evaluation-of-potential-dbs-targets-and-mechanisms-in-pd-mouse-model">Systematic evaluation of potential DBS targets and mechanisms in PD mouse model:</h3>
<p>To that end we are currently inducing a quantifiable PD-like state in mice with a unilateral injection of the neurotoxin 6-hydroxydopamine (6-OHDA) into the striatum, and subsequent administration of apomorphine to provoke side-biased motor deficits (Iancu et al. 2005) . Side-biased“turning” behavior is quantified autonomously on two distinct platforms, a computer-vision system that allows free movement, and a virtual-reality spherical treadmill platform that simulates free movement.</p>
<h4 id="metrics-of-behavior">Metrics of Behavior</h4>
<p>Two testing platforms would be used to assess changes in behavior over time. Behavior is analyzed and quantified in real-time, and would be synchronized with electrophysiology and made available as another stream of input for the closed-loop routine. The quantification routine creates a signal that is representative of symptom severity. For our unilaterally lesioned mouse model of PD the most readily observable impairment is the inability to walk straight; mice would turn in circles contralateral to the lesion when given intraperitoneal apomorphine. The behavioral apparati are described below.</p>
<h1 id="microscopy-1">Microscopy 1</h1>
<p>This section describes the background in microscopy in the neurosciences, and also how it relates to imaging in healthcare and electrophysiology in neuroscience. It will also describe the basic elements necessary for the construction of a microscope in a laboratory where calcium imaging in an animal is available. It will also refer to later sections which cover the design and construction of mechanical elements for animal handling and optical access (i.e. the headplate and a chronic optical window).</p>
<!-- from the frontiers paper -->
<p>Optical imaging has traditionally involved wide-field imaging or two photon imaging, each with their own distinctive advantages and disadvantages</p>
<p>In recent years, two photon microscopy has been a preeminent choice for imaging in tissue, because of its high spatial resolution, and tissue penetrating features</p>
<p>Two photon calcium imaging has been broadly applied to individual cells or subcellular components of neurons including spines and axons</p>
<p>Because two photon microscopy uses a scanning mechanism, the signal to noise ratio is influenced by the time spent imaging each point, and the spatial resolution is determined by the number of points scanned to obtain each image. As a result, the size of the imaging field is inversely correlated with the overall temporal resolution while maintaining a relatively high signal-to-noise ratio, thus, two photon calcium imaging is often performed on a small area or on a sparse network of cells, when dynamic responses with high temporal fidelity is necessary.</p>
<p>Wide-field, or single photon imaging has been in use for several decades and was first used to characterize the functional architecture and hemodynamic responses in brain tissue</p>
<p>However, this technique has seen a renaissance recently due to its simple instrumentation, relatively inexpensive cost, and the improvements in neural signal indicators. Optical imaging and two photon microscopy have traditionally been performed in head-fixed preparations, but recent advances have also made it possible to perform wide-field calcium imaging in freely moving animals, through miniaturized and wearable microendoscope systems</p>
<p>While wide-field imaging lacks the spatial resolution to resolve fine subcellular structure or the penetrating properties available with two-photon, it is possible to obtain clear neurites and somatic features, including spike detection</p>
<p>Because a single photon microscope does not rely on scanning features, it can be used to sample a larger field of view without sacrificing sampling rates. Additionally, recording sessions may be less sensitive to fluorophore bleaching than other techniques, which makes it possible to perform sustained illumination and subsequent imaging for an extended period of time - a desired feature for analyzing neural networks during some behavior paradigms (<em>e.g.,</em> repeated trial learning paradigms). Thus, wide-field imaging offers an advantage if the objective is to simultaneously recording hundreds of neurons in the brain of a living and behaving animal with high temporal fidelity.</p>
<!-- from the frontiers paper -->
<h2 id="background-brain-imaging-and-microscopy-in-neuroscience">Background: Brain Imaging and Microscopy in Neuroscience</h2>
<h2 id="fluorescent-proteins-background">Fluorescent Proteins Background</h2>
<ul>
<li>GCamP
<ul>
<li>vs dyes</li>
</ul></li>
</ul>
<h3 id="fluorescence-microscope-types-background">Fluorescence Microscope Types Background</h3>
<ul>
<li>2p, confocal, wide-field
<ul>
<li>tissue scattering
<ul>
<li>depth comparison</li>
</ul></li>
<li>cost &amp; complexity</li>
<li>(address scalability later)</li>
</ul></li>
</ul>
<h3 id="widefield-microscope-configuration">Widefield Microscope COnfiguration</h3>
<ul>
<li>historical trend/shift from <em>finite</em> to <em>infinite</em> conjugate type
<ul>
<li>infinite type uses <em>infinity corrected lenses</em></li>
</ul></li>
</ul>
<h3 id="filters">Filters</h3>
<ul>
<li>excitation</li>
<li>emission</li>
<li>for epifluourescent microscope configuration: dichroic beamsplitter</li>
</ul>
<h3 id="lenses">Lenses</h3>
<p>The simplest configuration of infinite wide-field microscope requires an excitation</p>
<h3 id="excitation">Excitation</h3>
<h3 id="emission">Emission</h3>
<h3 id="emitted-light-collection-and-image-formation-emissioncollection">Emitted Light Collection and Image Formation (emission/collection)</h3>
<ul>
<li>Lens selection
<ul>
<li>objectives</li>
<li>SLR
<ul>
<li>spectral coating characteristics ### Mechanics</li>
</ul></li>
</ul></li>
</ul>
<h3 id="microscopy-and-functional-imaging-two-core-innovations-in-available">Microscopy and Functional Imaging Two core innovations in available</h3>
<ul>
<li>technology 1. Synthetic bio (i.e. GCaMP) 2. Cameras</li>
<li>scientific CMOS</li>
</ul>
<h2 id="brain-imaging-awake-animals">Brain Imaging Awake Animals</h2>
<h2 id="brain-regions">Brain Regions</h2>
<p>-hippocampus, cortex, striatum - cell types - cell sparsity</p>
<h2 id="analysis">Analysis</h2>
<h1 id="cameras-for-widefield-microscopy"># ## Cameras for Widefield Microscopy</h1>
<p>Traditional widefield microscope or macroscope builds incorporate ‘scientific grade’ cameras. Compared to cameras built for other markets (e.g. consumer, industrial, studio, etc.), these cameras are often well tested and certified to offer low or well-characterised noise at moderate speeds, and a linear photo-response profile. Unlike consumer or studio cameras which are invaribaly configured for RGB color, they are preferably configured with ‘monochrome’ sensors - essentially identical to the analagous color sensor, without the bayer filter. Of much greater importance, one must consider the unique connectivity and control interface that scientific cameras come with. Standards exist, but are typically unique to this segment of the industry, with poorly defined specifications for translation to other electronic communication and connection interface standards, such as those used in studio and broadcast video, or those used with consumer cameras. The trait that is the most worthy of consideration, however, is the cost. See <span class="citation" data-cites="discussion-cost-consumer">@discussion-cost-consumer</span> for details.</p>
<p>The in-vivo instrinsic-signal or fluorescent-dye imaging camera of 1 decade ago had a 0.5“-1” monochrome CCD sensor with 0.1-1 MegaPixels, a large well-depth, and moderately low noise at speeds around 30 to 60 fps. Connection was often LVDS, with custom electrial connectors unique to each camera. A particularly popular and long-running model was the Dalsa 1M30, followed by the 1M60 in later years <span class="citation" data-cites="takahashi_vivo_2006">[@takahashi_vivo_2006]</span>.</p>
<h3 id="accelerated-sensor-improvement">Accelerated Sensor improvement</h3>
<h4 id="sensor-size">Sensor Size</h4>
<h4 id="read-noise-and-speed">Read noise and Speed</h4>
<h4 id="interfaces-connection">Interfaces: Connection</h4>
<h4 id="interfaces-protocol">Interfaces: Protocol</h4>
<hr />
<h2 id="computer-workstation">Computer Workstation</h2>
<p>Go to <a href="http://www.pugetsystems.com">Puget Systems</a> to find a computer configuration that is well tested and uses commercially available components to deliver high performance. If you don’t feel like putting a computer together yourself, you could order directly from them.</p>
<h3 id="part-selection-and-assembly">Part Selection and Assembly</h3>
<table>
<thead>
<tr class="header">
<th>Motherboard:</th>
<th>Processing</th>
<th>Expansion Slots:</th>
<th>Memory:</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ASUS WS C621E SAGE</td>
<td>Socket: 3647</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<!-- CPU: Purley
Form Factor: SSI EEB
Chipset: Intel C620 -->
<!--
12 slots DDR4 ECC Registered 2666 MHz
(Max per Slot: 64 GB)
Max Supported: 768 GB


Rear USB 2.0: 2
Rear USB 3.0: 4
 -->
<h3 id="configuration">Configuration</h3>
<p>Must configure bios to optimize for high-throughput writes to disk.</p>
<h3 id="package-managment">Package Managment</h3>
<ul>
<li>Package Management &amp; Binary Distribution Sites</li>
<li>Bintools/JFrog</li>
<li>Conan.io</li>
<li>NPM</li>
<li>Nuget</li>
<li>VCPkg</li>
<li>Chocolatey (win)</li>
<li>Apt/yum (linux)</li>
<li>Pacman (msys/mingw)</li>
<li>Pact (babun/cygwin)</li>
<li>scoop</li>
<li>NPackd</li>
<li>Pypi</li>
</ul>
<h3 id="development-environment">Development Environment</h3>
<ul>
<li>Version-Control
<ul>
<li>Github</li>
<li>Bitbucket</li>
<li>Gitlab</li>
<li>Sourceforge</li>
<li></li>
<li><a href="https://en.wikipedia.org/wiki/Comparison_of_source_code_hosting_facilities">Comparison on Wikipedia</a></li>
</ul></li>
<li>Code Repositories
<ul>
<li>Matlab File Exchange</li>
<li>stack overflow</li>
<li><del>(Google Code)[code.google.com]</del></li>
</ul></li>
<li>Research Entity Web Sites
<ul>
<li>From Laboratory Manifest
<ul>
<li>Janelia</li>
<li>INRIA</li>
<li>Schartz Center for Computational Neuroscience (SCCN)
<ul>
<li>(SCCN Homepage)[https://sccn.ucsd.edu/wiki/SCCN_Software]</li>
<li>(SCCN Github)[https://github.com/sccn]</li>
<li>EEGLAB, BCILAB,Lab-Streaming-Layer</li>
</ul></li>
<li>Cohen Lab
<ul>
<li>LeverJS</li>
</ul></li>
</ul></li>
<li>NIH</li>
<li>NASA</li>
</ul></li>
<li>Binary Utilities
<ul>
<li>gtools</li>
<li>exetools</li>
<li>nirsoft</li>
<li>sysinternals</li>
<li>unxutils</li>
<li>x64tools</li>
<li>BU EngIT (butools)</li>
</ul></li>
<li>OSS Foundations
<ul>
<li>Apache</li>
</ul></li>
<li>Citations in Literature
<ul>
<li>from Biblio-Manifest</li>
</ul></li>
<li>SCC or Grid Utilities ## Head-Fixation Apparatus for Mice</li>
</ul>
<p>Whether using a microscope or electrophysiology apparatus with the spherical treadmill, the requirements for a rigid connection to the animal’s cranium are critical.</p>
<p><img src="/images/VR%20headplate%20holder%20in%20progress.PNG" alt="VR headplate holder in progress" /> <img src="/images/VR%20headplate%20holder%20in%20progress3.PNG" alt="VR headplate holder in progress3" /> <img src="/images/VR%20headplate%20holder%20in%20progress4.PNG" alt="VR headplate holder in progress4" /> <img src="/images/head-fixed-mouse-stationary-front.jpg" alt="head-fixed-mouse-stationary-front" /> <img src="/images/head-fixed-mouse-stationary-side.jpg" alt="head-fixed-mouse-stationary-side" /> <embed src="/images/headplate_holder_VR_v2.PDF" /></p>
<h1 id="pipeline-overview">Pipeline overview</h1>
<h2 id="phases">phases</h2>
<h3 id="preprocessing">preprocessing</h3>
<p>Image filtering operations Normalization Motion estimation and compensation Layer classification foreground/background Cell/neuropil/vessel</p>
<h4 id="time-series-analysis">time series analysis</h4>
<p>Prediction and factorization of temporal components/contaminants Model: Unidirectional trend (sources…) Oscillatory (“seasonal”) Unpredictable</p>
<h4 id="spatial-signal-deconvolution-unmixing">spatial signal deconvolution (unmixing)</h4>
<h3 id="signal-localization-and-extraction">signal localization and extraction</h3>
<p>Segmentation of cells Overlap and “disconnected” regions Initial vs incremental cell identification</p>
<h3 id="active-signal-analysis">active signal analysis</h3>
<p>Signal statistics Shape and stability of probabilistic distribution Pixel level vs cell level</p>
<h4 id="independent-cellular-activity-measures">independent cellular activity measures</h4>
<p>Quality Complexity Spatial Temporal</p>
<h4 id="network-analysis">network analysis</h4>
<h2 id="computing-cost">Computing cost:</h2>
<p>boundary analysis for iOS and config</p>
<h2 id="storage-cost">Storage cost:</h2>
<p>compression and imposed boundaries # Image Processing</p>
<p>This section borrows from AIM-1 and AIM-2 of the prospectus.</p>
<!-- # Introduction
# Procedures for Calcium Imaging
# Computer Software Environments for Image Processing
# Computational Resources for Processing Large Data Sets
# Method and Approach
# Image Pre-processing: Contrast Enhancement and Motion Correction
# Region of Interest (ROI) identification & segmentation:
# Region of Interest (ROI) merging:
# Visualization
# Predicting Activation State & Assessing Network Activity
# Parallel Processing
# Managing Continuity -->
<h2 id="procedures-for-calcium-imaging-1">Procedures for Calcium Imaging</h2>
<p>The general goal of processing image data from functional fluorescence imaging experiments is to restructure raw image data in a way that maps pixels in each image frame to distinct individual cells or subcellular components, called ‘Regions-Of-Interest’ (ROI). Pixel-intensity values from mapped pixels are typically then reduced by combination to single dimensional ‘trace’ time-series. These traces indicate the fluorescence intensity of an individual neuron over time, and the collection approximates the distinct activity of each and every neuron in the microscope’s field of view. However, this task is made difficult by motion of the brain throughout the experiment, and also by the apparent overlap of cells in the image plane captured from the camera’s 2-dimensional perspective. These issues can be partially mitigated with a few image pre-processing steps – alignment of images to correct for motion being the most critical. These options are described in the Methods &amp; Approaches section below. Most software packages geared specifically toward functional imaging implement either of two basic classes of pixel-&gt;cell mapping algorithms. One approach is to use image-segmentation routines for computer vision, which seeks to combine adjacent pixels into distinct spatially segregated regions representing objects in the image.</p>
<p>The other common approach is to perform an eigenvalue decomposition on the covariance matrix from a stack of image frames (also called spectral decomposition, or Principal Component Analysis, PCA), resulting in an assembly of basis vectors defining the weighting coefficients for each pixel. Multiplying the basis-vectors (i.e. “components”) with all frames produces a one-dimensional trace for each component. The linear combination is similar to the weighted image-segmentation method in that it assigns fractional coefficients to pixels. However the procedure for computing the covariance matrix employed by PCA operates on as many pixels as are in the image, multiplying each with every other pixel – a problem with np2 complexity, where p is the number of pixels in the image. I mention these issues inherent to PCA not because this project will attempt to address them, but because this project was initiated following tremendous difficulty attempting to use PCA-based cell sorting methods with large datasets.</p>
<h3 id="computer-software-environments-for-image-processing-1">Computer Software Environments for Image Processing</h3>
<p>The widespread usage of MATLAB in neuroscience communities lends potential for greater usability and easier adaptation to software developed in this environment. While software development environments with a focus on “ease-of-use” have traditionally presumed crippling sacrifices to computational performance, this assumption is getting to be less accurate.</p>
<p>Standard programs include ImageJ, the built-in routines in MATLAB’s Image Processing Toolbox, Mosaic from Inscopix, which is merely a compiled version of MATLAB routines which uses the MATLAB engine, Sci-Kits Image for Python, and a remarkable diversity of other applications. MATLAB is a commercial software development platform which is geared toward fast production and prototyping of data processing routines in a high-level programming language. It implements several core libraries (LINPACK, BLAS, etc.) that make multithreaded operations on matrix type data highly efficient. While MATLAB has traditionally been a considered the standard across neuroscience research labs, it was also well recognized that its performance was lacking for routines that aren’t “vectorized”, when compared to applications developed using lower-level languages like FORTRAN, C, and C++. Nevertheless, it remained in common use, and recent releases have added features that can drastically mitigate its performance issues, particularly through the development of a “Just-In-Time” compiler that automatically optimizes the deployment of computation accelerator resources for standard MATLAB functions. This feature enables code that performs repeated operations using for-loops or while-loops nearly as fast as equivalent code written in C. Additionally, code can be compiled into executable format using the Matlab Compiler toolbox, or used to generate equivalent C or C++ code using Matlab Coder.</p>
<h3 id="computational-resources-for-processing-large-data-sets-1">Computational Resources for Processing Large Data Sets</h3>
<p>Routines for extracting the activity in each cell from a collection of raw imaging data rely on an ability to simultaneous access many pixels separated over space and time (and consequently separated on disk). For long recording sessions, however, the size of the collection of stored image data grows dramatically. This substantial increase in the size of data easily exceeds the capacity of system memory in the typical workstation computer available to researchers. Thus, performing the necessary processing routines using standard programs is often unfeasible.</p>
<p>Another popular approach to this challenge is the migration of processing routines to a cluster-based system. In this way image data can be distributed across many interconnected computer nodes capable of performing all locally restricted image processing procedures in parallel, then passing data to other nodes in the cluster for tasks that rely on comparisons made across time. Access to clusters capable of performing in this way has historically been restricted to those working in large universities or other large organization, and the diversity of cluster types is sizeable, with clusters often having very particular configuration requirements for implementing data processing jobs efficiently. These issues would pose some difficulty to the use and shared development of software libraries for image processing routines, although the growth of “cloud computing” services such as Amazon’s EC2 and the Google Compute Engine, and also collaborative computing facilities like the Massachusetts Green High-Performance Computing Center (http://www.mghpcc.org) mitigate many of these issues. Additionally, efforts to produce a standardized interface for accessing and distributing data, and for managing computing resources across diverse computing environments have seen appreciable success. Apache’s release of the open-source cluster computing framework, Hadoop, and a companion data-processing engine called Spark (http://spark.apache.org/), has encouraged a massive growth in collaborative development projects, a consequently increased the availability of robust shared libraries for data processing in a variety of applications. The Spark API can be accessed using the open-source programming Python, and also using other languages like Java, Scala, or R. One project specifically geared for image processing of neural imaging data is the Thunder library, a Spark package released by the Freeman lab and developed in collaboration with a number of other groups at Janelia farm and elsewhere.</p>
<p>Many applications will find the recent improvements in accessibility and standardization make cluster computing an attractive and worthwhile option for processing a very large set of reusable data. However, this strategy would impose harsh limitations for a neuroscientist with a project that is continuously generating new data, as the time required to transfer entire imaging data sets across the internet may be prohibitive. Unfortunately, storage on the cloud is not so unlimited that it can manage an accumulated collection of imaging data generated at anything near the rate that sCMOS cameras are capable of producing. This rate imbalance is a central motivating issue for Aim 2 this project, and is discussed in more detail below.</p>
<h2 id="methods-and-approach">Methods and Approach</h2>
<p>Image processing is performed offline using MATLAB software. The goal of this procedure is to reduce the raw image sequence to a collection of one-dimensional traces, where each trace indicates the fluorescence intensity of an individual neuron over time, and the collection approximates the distinct activity of each and every neuron in the microscope’s field of view. We implement the process in 3 distinct stages as described below. The main novel contribution of this work is the efficient extension of segmented ROIs into the third dimension by clustering features of ROIs segmented separately in two dimensions. Online processing uses a similar approach, and the differences are explained in the next section.</p>
<h3 id="image-pre-processing-contrast-enhancement-and-motion-correction">Image Pre-processing: Contrast Enhancement and Motion Correction</h3>
<p>Alignment of each frame in the image sequence with all other frames is essential to the methods we use in subsequent steps for identifying and tracking cells over time. Thus, the goal of the first stage is to correct for any misalignment caused by movement of the brain relative to the microscope and camera.</p>
<p>Many algorithms for estimating and correcting image displacement exist and are well described in the medical imaging literature. We elected to use phase-correlation to estimate the induced motion in each frame, as we found this method to be highly stable, moderately accurate, and (most importantly) fast, especially when implemented in the frequency domain and using a decent graphics card.</p>
<p>Phase-correlation estimates the mean translational displacement between two frames, one being the template or “fixed” frame, and the other being the uncorrected or “moving” frame. In the spatial domain this is accomplished by computing the normalized cross-correlation, which implies a 2-dimensional convolution of large matrices. The equivalent operation in the frequency domain is a simple scalar dot-product of the discrete Fourier transforms of each image normalized by the square of the template, followed by the inverse Fourier transform. The intermediate result is the cross-correlation (or phase-correlation) matrix, which should have a peak in its center for correctly aligned images, or a peak near the center, the offset of which indicates the mean offset between the two images. This peak can be found with subpixel precision by interpolation to give a more accurate alignment, although at some moderate expense in computation time.</p>
<p>For the template image we used a moving average of previously aligned frames when processing frames sequentially, or alternatively a fixed mean of randomly sampled and sequentially aligned images from the entire set when processing files in parallel. The simplest way to perform this operation is to use the built-in MATLAB function normxcorr2, which makes optimization decisions based on image size and available hardware automatically. However, performance can be improved by tailoring the operation to your particular hardware and image size, i.e. using fft2 and ifft2 for large images and a good graphics card.</p>
<h3 id="region-of-interest-roi-identification-segmentation">Region of Interest (ROI) identification &amp; segmentation</h3>
<p>The ROI detection process used an adaptive threshold on the z-score of pixel intensity to reduce each frame to binary 1’s and 0’s (logical true or false). These binary frames were then processed using morphological operations to find and label connected components within each frame. For example, beginning with a z-score threshold of 1.5, all pixels that were more than 1.5 standard deviations above their mean were reduced to 1 (true), and all others reduced to 0 (false). Pixels reduced to 1 were often pixels overlying a cell that was significantly brighter during that frame due to activation of GCaMP. This initial threshold was adjusted up or down based on the number of non-zero pixels detected with each threshold. This was done to prevent spurious motion-induced shifts of the image frame from producing ROIs along high contrast borders. All morphological operations were performed using built-in MATLAB functions from the Image Processing Toolbox, which have fast parallel versions if the operation is run on a graphics card (e.g. imclose, imopen, etc.). Furthermore, the connected-component labeling and region formation operations were run using built-in MATLAB functions bwconncomp, and regionprops. Connected components were stored in a custom class and termed “single-frame ROIs,” and these were then passed to the 3rd stage of processing, which merges them into a “multi-frame ROI” that represents the location and spatial distribution of each cell identified over the entire video.</p>
<h3 id="region-of-interest-roi-merging">Region of Interest (ROI) merging</h3>
<p>The standard structure of region properties output by the MATLAB function regionprops (Area, BoundingBox, Centroid, etc.) are mimicked in a custom class called RegionOfInterest, where each field of the structure becomes a property of the custom class. We add additional properties for storing state information and data associated with each ROI, along with a number of methods for comparing, merging, manipulating, and visualizing the single-frame and multi-frame ROIs. The single-frame to multi-frame ROI merging procedure is essentially a clustering process that merges single-frame ROIs together using such criteria as the proximity of their centroids, as well as proximity of their bounding-box (upper-left and lower-right corners). Performing this operation quickly was highly dependent on pre-grouping ROIs based on centroid location in overlapping blocks of the image frame, as well as grouping by size. This enabled the clustering to be performed in parallel (across CPU cores) followed by a second iteration of clustering to deal with redundancy in overlapping regions.</p>
<h3 id="visualization">Visualization</h3>
<p>Once ROIs are established, all video data is reloaded and passed to a method in the <em>RegionOfInterest</em> class that extracts the 1-dimensional trace for each ROI representing the fluorescence intensity in that region over time. The ROIs and their traces can then be interactively visualized using another method in the <em>RegionOfInterest</em> class.</p>
<p>The <em>RegionOfInterest</em> class defines methods for rapid spatial comparison operations which can typically be viewed as an adjacency matrix using built-in image viewing commands. Visualization of the segmented cell overlay and 1D traces can be manipulated by assigning colors, removing ROIs, hiding ROIs, and more.</p>
<h3 id="predicting-activation-state-assessing-network-activity">Predicting Activation State &amp; Assessing Network Activity</h3>
<p>The continuous signal intensity signals can be reduced to binary indicators of activity for each frame. This enables simplified and fast calculation of information theory measures, such as activation probability, joint and conditional probabilities, response entropy, mutual information, etc. The conversion from continuous to binary uses several abstractions of the signal applied to a Gaussian Mixture Model (GMM). The abstractions are calculated from the following:</p>
<ol type="1">
<li><p>Linear least-squares fits to moving windows with variable size to find slope of the line surrounding each point.</p></li>
<li><p>Skewness and Kurtosis of finite windows surrounding each data point.</p></li>
<li><p>Temporal difference of fluorescence intensity.</p></li>
</ol>
<p>The gaussian mixture model employs all measures to infer periods of reliable distinct activation of neurons.</p>
<h3 id="parallel-processing">Parallel Processing</h3>
<p>Many built-in MATLAB functions are implemented using efficient multi-threaded procedures, and these are used to the extent that they can be. However, for procedures that must operate on data in irregular formats (i.e. any format other than N-dimensional arrays of primitive data types), one also has the option of performing explicitly defined parallel operations by distributing data across multiple parallel processes, each with their own memory space. Below I give examples of how implementing in a multi-threaded fashion can substantially boost performance, and also an example of a situation where multi-threaded operations aren’t possible without explicit calls for parallel distribution.</p>
<p>Standard elementwise operators like <em>plus</em> (+) and <em>times</em> (.*), as well as comparison operators like <em>equals</em> (==) and <em>less-than</em> (&lt;) will be performed efficiently using as many processing cores as available when applied to large n-dimensional arrays of the same size. However, when operand sizes differ a simple call to the built-in operation will not work. For example, if we wish to subtract the average from each pixel over time from all frames in the series we can accomplish this with a call to MATLAB’s <em>bsxfun</em> function, which stands for Binary-Singleton-eXpansion-FUNction, as shown below:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb3-1" title="1"></a>
<a class="sourceLine" id="cb3-2" title="2">      Fmeansub = bsxfun( @minus, F, mean(F,<span class="fl">3</span>) );</a></code></pre></div>
<p>This operation passes a function handle as the first argument (denoted by the ‘@’ symbol) indicating the operation to perform. It then passes the entire [IxJxK] array of image data as the second argument, and it’s temporal mean with size [IxJx1] is calculated once and passed as the third. The function efficiently expands the mean argument as needed for fast distribution across parallel threads.</p>
<h3 id="managing-continuity">Managing Continuity</h3>
<p>Data such as baseline frames and frames for alignment must be passed between parallel processors to maintain continuity between data divided temporally between processors. However, the efficient application of this approach was limited by the system memory and number of cores available, and was meant to be applied in a cluster environment.</p>
<p>Building the set of functions for offline processing enabled application to data already gathered, and also served as a framework that informed the necessary procedures to be included in the online extension of this toolbox.</p>
<h4 id="computing-power-and-connectivity">Computing Power and Connectivity</h4>
<ul>
<li>Remote Clusters (AWS)</li>
<li>Graphics Processing Units (NVIDIA GTX)</li>
<li>Embedded Units (NVIDIA Tegra X2) 2. Well developed libraries</li>
<li>ImageJ (so so)</li>
<li>OpenCV (uses OpenCL)</li>
<li>GStreamer (much better)</li>
<li>OpenGL</li>
<li>Shader</li>
</ul>
<h4 id="image-processing">Image Processing</h4>
<ul>
<li>Motion Correction</li>
<li>Image Enhancement</li>
</ul>
<h4 id="motion-correction-two-approaches-to-find-displacement">Motion Correction Two approaches to find displacement</h4>
<h4 id="spatially-homogeneous-phase-correlation">Spatially Homogeneous phase correlation</h4>
<ul>
<li>aka normalized cross correlation - Feature Matching</li>
<li>Detect features (i.e. corners) - Triangulate best</li>
</ul>
<h4 id="image-enhancement">Image Enhancement</h4>
<ol type="1">
<li>Contrast - Linear Scaling - Lookup Tables</li>
<li>Spatial and Temporal Filtering</li>
<li>Feature images - Gradients</li>
</ol>
<h4 id="feature-extraction">Feature Extraction</h4>
<ol type="1">
<li>Feature images (temporally independent)</li>
</ol>
<ul>
<li>Gradients - Surface Curvature 2. Long Term Memory - Statistics
<ul>
<li>changes (single pixel)</li>
</ul></li>
<li>Mutual information changes (inter-pixel)</li>
</ul>
<h4 id="acceleration-and-optimization-procedures-for-online-video-processing">Acceleration and Optimization Procedures for Online Video Processing</h4>
<h4 id="incremental-update-of-statistics">Incremental Update of Statistics</h4>
<h5 id="central-moments">central moments</h5>
<div class="sourceCode" id="cb4"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb4-1" title="1">      function [m1,m2,m3,m4,fmin,fmax] = updateStatistics(x,m1,m2,m3,m4))</a>
<a class="sourceLine" id="cb4-2" title="2">            n = n + <span class="fl">1</span>;</a>
<a class="sourceLine" id="cb4-3" title="3"></a>
<a class="sourceLine" id="cb4-4" title="4">            <span class="co">% GET PIXEL SAMPLE</span></a>
<a class="sourceLine" id="cb4-5" title="5">            f = F(rowIdx,colIdx,k);</a>
<a class="sourceLine" id="cb4-6" title="6"></a>
<a class="sourceLine" id="cb4-7" title="7">            <span class="co">% PRECOMPUTE &amp; CACHE SOME VALUES FOR SPEED</span></a>
<a class="sourceLine" id="cb4-8" title="8">            d = single(f) - m1;</a>
<a class="sourceLine" id="cb4-9" title="9">            dk = d/n;</a>
<a class="sourceLine" id="cb4-10" title="10">            dk2 = dk^<span class="fl">2</span>;</a>
<a class="sourceLine" id="cb4-11" title="11">            s = d*dk*(n-<span class="fl">1</span>);</a>
<a class="sourceLine" id="cb4-12" title="12"></a>
<a class="sourceLine" id="cb4-13" title="13">            <span class="co">% UPDATE CENTRAL MOMENTS</span></a>
<a class="sourceLine" id="cb4-14" title="14">            m1 = m1 + dk;</a>
<a class="sourceLine" id="cb4-15" title="15">            m4 = m4 + s*dk2*(n.^<span class="fl">2</span>-<span class="fl">3</span>*n+<span class="fl">3</span>) + <span class="fl">6</span>*dk2*m2 - <span class="fl">4</span>*dk*m3;</a>
<a class="sourceLine" id="cb4-16" title="16">            m3 = m3 + s*dk*(n-<span class="fl">2</span>) - <span class="fl">3</span>*dk*m2;</a>
<a class="sourceLine" id="cb4-17" title="17">            m2 = m2 + s;</a>
<a class="sourceLine" id="cb4-18" title="18"></a>
<a class="sourceLine" id="cb4-19" title="19">            <span class="co">% UPDATE MIN &amp; MAX</span></a>
<a class="sourceLine" id="cb4-20" title="20">            fmin = min(fmin, f);</a>
<a class="sourceLine" id="cb4-21" title="21">            fmax = max(fmax, f);</a>
<a class="sourceLine" id="cb4-22" title="22">      end</a></code></pre></div>
<h5 id="extract-features">Extract Features</h5>
<div class="sourceCode" id="cb5"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb5-1" title="1"></a>
<a class="sourceLine" id="cb5-2" title="2">      function [dm1,dm2,dm3,dm4] = getStatisticUpdate(x,m1,m2,m3,m4)</a>
<a class="sourceLine" id="cb5-3" title="3">            <span class="co">% COMPUTE DIFFERENTIAL UPDATE TO CENTRAL MOMENTS</span></a>
<a class="sourceLine" id="cb5-4" title="4">            dm1 = dk;</a>
<a class="sourceLine" id="cb5-5" title="5">            m1 = m1 + dm1;</a>
<a class="sourceLine" id="cb5-6" title="6">            dm4 = s*dk2*(n^<span class="fl">2</span>-<span class="fl">3</span>*n+<span class="fl">3</span>) + <span class="fl">6</span>*dk2*m2 - <span class="fl">4</span>*dk*m3;</a>
<a class="sourceLine" id="cb5-7" title="7">            dm3 = s*dk*(n-<span class="fl">2</span>) - <span class="fl">3</span>*dk*m2;</a>
<a class="sourceLine" id="cb5-8" title="8">            dm2 = s;</a>
<a class="sourceLine" id="cb5-9" title="9">            m2 = m2 + dm2;</a>
<a class="sourceLine" id="cb5-10" title="10">            <span class="co">% NORMALIZE BY VARIANCE &amp; SAMPLE NUMBER -&gt; CONVERSION TO dVar, dSkew, dKurt</span></a>
<a class="sourceLine" id="cb5-11" title="11">            dm2 = dm2/max(<span class="fl">1</span>,n-<span class="fl">1</span>);</a>
<a class="sourceLine" id="cb5-12" title="12">            dm3 = dm3*sqrt(max(<span class="fl">1</span>,n))/(m2^<span class="fl">1.5</span>);</a>
<a class="sourceLine" id="cb5-13" title="13">            dm4 = dm4*n/(m2^<span class="fl">2</span>);</a>
<a class="sourceLine" id="cb5-14" title="14">      end</a></code></pre></div>
<h4 id="simple-processing-on-gpu">Simple Processing on GPU</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb6-1" title="1">      [dm1,dm2,dm3,dm4] = arrayfun(@getStatisticUpdate(x,m1,m2,m3,m4)</a>
<a class="sourceLine" id="cb6-2" title="2">      [dm1,dm2,dm3,dm4] = arrayfun(@getStatisticUpdate(rowidx,colidx)</a></code></pre></div>
<h5 id="alternative-libraries">Alternative Libraries</h5>
<ul>
<li><a href="https://developer.nvidia.com/npp">NVIDIA Performance Primitives</a></li>
<li><a href="https://developer.nvidia.com/opencv">OpenCV</a></li>
<li><a href="http://www.vlfeat.org/">VLFeat</a></li>
<li>OpenGL</li>
<li>OpenCL</li>
<li>OpenVX</li>
<li>CLosedDoesNotExist (…?)</li>
<li>Shader Languages</li>
<li>GLSL</li>
<li>HLSL</li>
<li>WebGL</li>
<li>Halide</li>
<li><ul>
<li>FFmpeg</li>
</ul></li>
<li>GStreamer</li>
</ul>
<h3 id="choice-of-interface">Choice of Interface</h3>
<h4 id="procedural-framework-pipes-streams-graphs">Procedural Framework: Pipes, Streams, &amp; Graphs</h4>
<h5 id="concurrency-parallel-performance">Concurrency: Parallel = Performance?</h5>
<p>Not always, no. While concurrent processing of independent tasks or sequentially arriving data elements will almost always increase performance, this is not always the case. At a lower instruction-level than we typically program, synchronous operations can often be optimized in ways that asynchronous operations cannot, typically through strategic register allocation, or by taking cache-hit performance). “Globally Asynchronous Locally Synchronous”</p>
<h5 id="scheduling">Scheduling</h5>
<h5 id="adaptive">Adaptive</h5>
<h4 id="choice-of-operations">Choice of Operations</h4>
<ul>
<li>What is the goal?</li>
<li>Is it effective?</li>
<li>Is the computation cost worth the result?</li>
<li>Are there side-effects or artifacts?</li>
<li>Can they be reliably controlled or accounted for?</li>
</ul>
<h4 id="motion-correction">Motion Correction</h4>
<p>In our application, the goal of a motion correction operation is to artificially suppress translation of the brain tissue parallel to the image plane. <em>Phase-Correlation</em> (also referred to as <em>normalized cross-correlation</em>) has consistent performance across a range of image sources with varying spatial noise characteristics. However, a large non-uniform change from reference frames - such as occurs when cells with low baseline fluourescence are first activated - can cause drastic errors that must be recognized and corrected by a supervisory procedure. This can induce an undesirable, unpredicatable, and specifically inopportune latency Unfortunately in all the whole pipeline.</p>
<p>Unfortunately, “Globally Asynchronous Locally Synchronous”</p>
<p>as it’s In some experimental setups,</p>
<p>The phase correlation method of Motion Estimation - cost: 2-10 ms/frame - Frequently unstable (depending on video content)</p>
<h5 id="motion-compensation-interpolation">Motion Compensation &amp; Interpolation</h5>
<ul>
<li>cost: 400-800 us/frame</li>
<li>Requires infill with nearby or prior pixel values if frame size is to be maintained</li>
</ul>
<h1 id="microscopy-2">Microscopy 2</h1>
<h2 id="construction">Construction</h2>
<h3 id="standards">Standards</h3>
<h2 id="multispectral-imaging">Multispectral Imaging</h2>
<h3 id="filter-selection">Filter Selection</h3>
<h3 id="lenses-1">Lenses</h3>
<h3 id="mechanics-and-positioning">Mechanics and Positioning</h3>
<ul>
<li>Configuration
<ul>
<li>gantry</li>
<li>XY table with split Z</li>
<li>AB axial rotation mounts
<ul>
<li>center of rotation bi-coaxial with image plan</li>
</ul></li>
</ul></li>
<li>Bearings
<ul>
<li>ball bearing slides vs crossed roller</li>
<li>flexures</li>
<li>air bearings
<ul>
<li>hybrid vacuum and compressed air</li>
<li>magnet and compressed air</li>
</ul></li>
<li>simple teflon slip</li>
</ul></li>
</ul>
<h2 id="electronics">Electronics</h2>
<h3 id="photosensors">Photosensors</h3>
<h2 id="cameras">Cameras</h2>
<p>This section details the evolution of cameras sensors and other sensors that provide bio-relevant data. Emphasis is on</p>
<h3 id="scientific-cmos-scmos">Scientific CMOS (sCMOS)</h3>
<ul>
<li>Correlated double sampling</li>
<li>HDR</li>
<li>On-sensor Fusion</li>
<li>Commercial availability</li>
</ul>
<h3 id="data-transfer-camera-interfaces">Data-Transfer (Camera Interfaces)</h3>
<ul>
<li>USB</li>
<li>CameraLink</li>
<li>COaXPress</li>
<li>PCIe
<ul>
<li>gen2, gen3</li>
<li>x4, x8</li>
<li>copper, fiber</li>
</ul></li>
</ul>
<h2 id="image-acquisition">Image Acquisition</h2>
<ul>
<li>Libraries</li>
<li>Camera configuration</li>
<li>Windows vs. Linux</li>
<li>Setup and programming <!-- matrix vision has an amazing manual --></li>
</ul>
<h2 id="image-data-management">Image Data Management</h2>
<ul>
<li>Storage format</li>
<li>storage location</li>
<li>provenance ## Sensors</li>
</ul>
<p>Development boards Susie S. Cha<sup>1</sup>, Mark E. Bucklin<sup>1,2</sup>, Xue Han<sup>1</sup></p>
<p>Author affiliation</p>
<p><sup>1</sup>Boston University, Department of Biomedical Engineering, Boston, MA 02215</p>
<p><sup>2</sup>Boston University, School of Medicine, Boston, MA 02118</p>
<p>Keywords</p>
<p>wide-field optical imaging, in vivo, silicone elastomer, cranial window …</p>
<h1 id="abstract-1">Abstract</h1>
<p>Wide-field epifluorescence imaging of a living brain has substantially expanded our ability to perform high-throughput detection of neural activity at a spatial and temporal resolution sufficient to capture rich cellular dynamics of large interconnected networks of neurons. Yet optical signals are highly sensitive to light scattering, and the preservation of optical clarity through craniotomies is critically important for high-resolution imaging. Longitudinal imaging in cortex, however, is often limited due to granulation tissue growth between the brain and the imaging access window, and its inhomogeneous structure induces a significant level of optical scattering, thus severely compromising spatial resolution. To address this concern, we report the design of a polydimethylsiloxane (PDMS)-based cranial window system that can faithfully prevent the optically obstructive tissue from growing under the craniotomy. Additionally, the two-part system is constructed to allow for continued full tissue access after its initial installation. Utilizing the system, we demonstrate a year-long course of wide-field imaging recording the simultaneous activity in thousands of cortical neurons in awake mice.</p>
<ol type="1">
<li><p>Cranial window system that provides more functionalities to extend our capability using in vivo optical imaging to probe the complexity of the brain</p>
<ol type="a">
<li><p>Long-term maintenance of optical quality</p></li>
<li><p>Easy and repeatable tissue accessibility</p></li>
</ol></li>
<li><p>Goal of the project was to design a system that</p>
<ol start="3" type="a">
<li><p>Performs reliably</p></li>
<li><p>Easily adaptable</p></li>
<li><p>Easily adoptable</p></li>
</ol></li>
<li><p>To achieve, used silicone elastomer (vs. convention – glass)</p>
<ol start="6" type="a">
<li><p>Major advantage – can easily incorporate shapes and features to develop a system that can adapt to various experimental requirements</p></li>
<li><p>Provide equivalent optical quality to glass</p></li>
</ol></li>
<li><p>Achieved by developing a multi-stage system composed of a headplate with an integrated chamber and a cranial window</p>
<ol start="8" type="a">
<li><p>Window was casted incorporating features to block tissue growth within image field</p>
<ol type="i">
<li>Achieve image durations extending beyond a year to record calcium dynamics of hundreds of individual neurons using wide-field epifluorescence microscope</li>
</ol></li>
<li><p>Headplate provided the base for window installation which can be temporarily detached to perform acute manipulation on the underlying tissue</p></li>
</ol></li>
<li><p>Easily adaptable, demonstrated through providing an example of two systems with similar design concepts but incorporated distinct design features</p>
<ol start="10" type="a">
<li><p>Bilateral hemispheric windows</p></li>
<li><p>Whole-brain window</p></li>
</ol></li>
<li><p>Designs of the reported system are available in open source and can be easily adopted by developing the parts through rapid prototyping</p></li>
</ol>
<h1 id="introduction">Introduction</h1>
<p>In vivo optical imaging provides a means to visualize the structural features and functional dynamics of brain tissue, and to measure how these change over time<sup>1–3</sup>. Optical techniques for observing neural activity have advanced due to evolving digital imaging technology, and the development of increasingly effective functional indicators such as the genetically encoded fluorescent calcium sensor GCaMP6f<sup>4</sup>.</p>
<p>The ability to observe and record from the same brain region for extended periods is critical for longitudinal experiments tracking long term changes<sup>5–8</sup>. This ability relies heavily on maintaining a clear optical light path by forming a stable non-scattering interface with neural tissue overlying the targeted brain region. Using a wide-field fluorescence microscope with a scientific-CMOS camera, we can record activity in hundreds of distinct neurons in the hippocampus and other subcortical brain regions of awake behaving mice<sup>9</sup>. The method used to create an optical window in subsurface regions allows the quality of optical access to remain stable for several months. The approach is not without drawbacks, however; stable optical access is delayed by pooling and coagulation of blood on the window surface resulting from unavoidable vascular damage inherent to the implantation procedure<sup>5</sup>. The period of delay post implantation can range from 3-8 weeks before tissue repair and phagocytic debris removal processes subside to yield optical access to targeted brain tissue with stable imaging quality.</p>
<p>Attempts to image neocortical regions on the surface of mouse brain typically use a small glass disc fixed to the cranial surface to seal and protect the craniotomy, and to provide optical access to the brain<sup>10,11</sup>. This approach, however, is often challenged by progressive deterioration in image quality. Consequently, the reliability and average duration of optical access has been insufficient for long-term studies. The degradation is observed as a cloudy layer that gradually covers the imaging field, and is thought to arise from the natural inflammatory response that follows a craniotomy<sup>10–15</sup>. As granulation tissue grows, its inhomogeneous structure scatters light at the interface between brain tissue and optical window, which consequently degrades image quality and blurs fluorescence signals. Image quality in wide-field microscopy is more sensitive to scattering along the light path than scanning microscopes like two-photon or confocal, and is therefore particularly susceptible to this degradation<sup>1,16</sup>.</p>
<p>While glass is c­­hemically inert and comes in many shapes and sizes, cementing glass to the top of the skull leaves a fluid filled gap between the window and brain surface which is quickly filled by granulation tissue. This process, thought to be a mechanism for dura and bone regrowth, eventually disrupts the high optical clarity for chronic imaging<sup>12</sup>, and hence is a common target for efforts to extend image quality through a cranial window<sup>7,12</sup>.</p>
<p>Efforts to overcome this problem by adding purely mechanical features to the cranial window have involved attaching spacers made of agarose<sup>11,17</sup>, silicone<sup>18,19</sup> and glass<sup>7</sup> to the window’s brain-facing surface that compensate for the thickness of removed bone. These approaches report delaying tissue regrowth for up to a few months before optical quality deteriorates. These modest results indicate a valid basis underlying this approach and suggest that extending this strategy by starting with a design and material not limited by the fixed form of flat glass optical windows could yield some improvement. Additional elements of a chronic cranial imaging window intended to mitigate degradation by granulation tissue typically target the primary source stimulating the process, inflammation. These include the aseptic design of seals and features, selective use of biocompatible materials, and perioperative administration of anti-inflammatory and antibiotic drugs<sup>7,20,21</sup>.</p>
<p>While these designs have improved longevity, they remain limited in terms of long-term access to the cortical tissue. The ability to access and manipulate tissue during real-time imaging facilitates novel ability to characterize the dynamic processes in both physiological and pathological conditions<sup>22</sup>. Several strategies have been reported to gain access to regions below glass cranial windows by incorporating features such as an access port sealed with elastomer<sup>23</sup>, infusion cannula<sup>24</sup>, or the use of microfluidic channels<sup>17</sup>. Nonetheless, the approaches limit the tissue accessibility to a single designated site predetermined before an experiment begins and do not offer uniform access over the imaging area.</p>
<p>To address the relative restrictions using glass as cranial windows, a number of alternative efforts have highlighted the use of silicone elastomer for cranial windows<sup>25–29</sup>. For example, polydimethylsiloxane (PDMS) is optically clear, non-toxic and chemically inert and can be molded to take any shape or exhibit any desired feature, not necessarily sacrificing the imaging field of the window. These properties combine to offer a remarkably versatile material, particularly favorable for prototype development for projects with demanding specifications for biocompatibility and optical performance. A well-known and widely used example is the artificial dura for in vivo optical imaging in nonhuman primates<sup>25,26,30</sup>. This chronic implant device is placed in and covers a craniotomy and sits protected within a chronic cranial recording chamber. It mitigates tissue regrowth, and interfaces with a cylindrical insert – also made of PDMS – for optical imaging of neocortex. Additionally, the artificial dura is thin enough to enable access to underlying tissue for penetrating electrodes, which penetrate easily and leave a tight seal after withdrawal. Yet the efforts for translating this design windows for small research animals using silicone elastomer have thus far been limited. And a system with long-lasting high optical clarity and flexible tissue accessibility remains to be developed or explored for rodent models.</p>
<p>In this paper, we describe a design and demonstrate a cast silicone chronic cranial imaging window system, developed to facilitate longitudinal imaging experiments in mouse neocortex. The primary capability requirements for this design are:</p>
<ol type="1">
<li><p>Long-term stability of an optically clear light-path to cortical surface</p></li>
<li><p>Intermittent physical access to imaged region at any point in study</p></li>
</ol>
<p>The system was designed considering biocompatibility and optical performance to facilitate integration in place of the removed bone flap enabling us to achieve sustained periods of optical clarity, extending beyond a year in some mice and allowing for both high spatial and temporal resolution using a wide-field microscope. Additionally, the two-part system consisting of a fixed headplate with integrated neural access chamber and optical insert, allowed flexible access to the underlying tissue. The utility of our design is demonstrated through chronic optical imaging of calcium dynamics in the cortex and acute interventions to the tissue upon detachment and replacement of the window from the headplate. Adaptation</p>
<h1 id="results">Results</h1>
<p>Here we report the design for a head-fixation and cranial window device, and the procedures for surgical attachment. The sections below describe the features of each component, and also report the critical elements that contribute to the performance and capabilities of our optical imaging window system. The following sections provide a detailed report of the system performance observed during evaluation. Adaptation</p>
<h2 id="cranial-window-system">Cranial Window <strong><span class="smallcaps">System</span></strong></h2>
<p>Many design features, and procedures for implantation were introduced and developed to mitigate tissue growth for the sustained optical quality of the window. Other features were included to enhance imaging performance in awake behaving animals, to facilitate repeatable localization of image fields across sessions and animal subjects.</p>
<p>The cranial implant system is composed of two parts: a headplate with an integrated chamber, and a silicone optical insert. The headplate is bonded to the dorsal surface of the animal’s bare skull. The optical insert – sometimes referred to as a “cranial window” – seals the chamber and establishes an optical interface with the animal’s brain through craniotomy sites in the chamber floor (Fig 1a).</p>
<h3 id="headplate">Headplate</h3>
<p>The bottom surface of the headplate is curved to conform to the dorsal skull surface of a typical mouse. This feature aids alignment during installation, and a large surface area enables a strong adhesive bond to the skull surface. Adhesive cement is applied continuously along all points of contact to create a permanent bond along the entire perimeter. The cement applied along this joint effectively seals the bottom of the aseptic chamber and is critical for its long-term integrity.</p>
<p>The wide area of skull-to-headplate attachment provides a mechanically stable coupling between the animal’s skull and the headplate holder, which is fixed to the microscope table. The headplate is bonded to all skull plates, which stiffens the skull tremendously. Additional rigidity is provided by a central support structure that contacts the skull along the sagittal suture. All these features combine to provide a very rigid attachment to the mouse cranium, which drastically reduces its motion relative to the imaging system (Fig 1b). Remaining brain motion is then primarily movement relative to the skull, and may originate from physiological forces (i.e. cardiorespiratory) as much as behavioral forces from animal movement; suppressing this intracranial motion is addressed in the design of the silicone window insert described below.</p>
<h3 id="chamber">Chamber</h3>
<p>The chamber in the headplate center facilitates physical access to neural tissue by protecting the craniotomy sites between interventions. Once the headplate is bonded to the animal’s skull, the floor of the chamber is formed by the central support structure that traverses and fuses the sagittal suture, the skull surface surrounding each craniotomy, and a flat ledge that extends laterally. The joints between the skull surface edges of the central support, anterior and posterior walls, and the lateral ledge are sealed during the headplate attachment procedure. This bottom seal is crucial for maintaining an aseptic environment for the protection of the exposed brain tissue. When the dura mater is left intact during the craniotomy, the space within the chamber is continuous with the epidural space.</p>
<h3 id="optical-insert">Optical Insert</h3>
<p>The insert has optically flat top and bottom rectangular surfaces. The bottom brain-facing surfaces are positioned to form a flat interface with the intact dura through each craniotomy. The body of the insert provides a clear light-path between top and bottom surfaces. The walls of the body are tapered to increase the angle of unimpeded light collection/delivery at the image field. This increases the numerical aperture for imaging through high power lenses, and also expands options for off-axis illumination. The tapered body is extended to the brain surface via vertical-walled columns that traverse each craniotomy. These columns fill the space made by removal of the bone flap during craniotomy, and their bottom surface gently flattens the brain tissue, positioning the cortex in a horizontal plane for convenient wide-field imaging. Both the top and bottom surfaces are made optically clear by integrating microscope slides in the mold when casting. The dimensions of the insert are depicted in figure 1-b.</p>
<p>Inserts are fabricated in batches using an optically transparent silicone elastomer. We vacuum cast the part in a PTFE and glass mold with an aluminum frame inclusion that gets embedded near the upper surface. This frame provides a site for attachment and sealing to the rim of the chamber, as well as structural reinforcement. This helps to establish and maintain a flat optical surface at the top of the insert, parallel to the headplate (Fig 1b). We constructed inserts with the bottom optical surface parallel to the top, which works well for imaging medial cortical regions. For imaging lateral cortical regions (e.g. visual or auditory cortex) the mold can be adapted to produce inserts that form a flat image plane with consistent controllable angle relative to the headplate. For any desired angle, this capability greatly simplifies recording from a consistent image plane across sessions and animals. The medial cortical region imaged in the demonstration provided here was square in shape (2 mm X 2 mm), at a horizontal angle of 0 degrees, and extended from 0.83 mm to 2.83 mm symmetrically off the midline (Fig 1a).</p>
<h2 id="installation-and-usage">Installation and Usage</h2>
<p>The surgical installation procedures for this multi-stage implant device were adapted from a combination of procedures in common use for attachment of headplates and cranial windows in mice, and similar devices used for optical imaging in primates. The specific protocol evolved during 3 distinct trial runs, and the final protocol is summarized here and detailed in methods and materials below. The 18 mice reported here received the same version of headplate and optical insert. These mice were used for simultaneous development of other projects not described here. Minor changes were made to the surgical procedures from one batch to the next, each with discernable effect; see the discussion for details.</p>
<p>Because this is a multi-stage implant, the procedure for installation can be separated into multiple distinct surgeries depending on experimental requirements. The first stage includes headplate attachment to bare skull, centrally aligned along the AP axis with the bilateral sites over the cortical region of interest. Once the headplate is securely bonded, bilateral craniotomy can be made through the skull in the base of the chamber. If the second stage of installation is performed separately, the chamber is given a temporary silicone seal to protect the craniotomy. We delayed the second stage of installation for at least 2 to 3 days to allow for mouse recovery.</p>
<p>The second stage involves installation of the optical window, and may be directly preceded by injection of virus, pharmaceutical compounds, exogenous cells, or any other substance of interest. The optical window is installed in the chamber with the assistance of a stereotaxic holder, which enables fine height adjustment and holds the window’s position while being secured in place. The angle of the window’s top surface is held parallel with that of the headplate. The chamber is partially filled with sterile agarose to displace all air from the chamber when the optical insert is lowered into place. The height is adjusted to provide full contact between the insert’s bottom surface and the dura, which also places the insert’s frame in close proximity to the rim of the chamber. Dental cement is applied to form a joint between the headplate and the frame of the optical insert, fixing the insert in place and aseptically sealing the chamber.</p>
<p>The optical insert can be detached and reattached at any time to provide physical access to the neural tissue and/or for window replacement (i.e. for mid-experiment injections or window damage repair respectively). Detachment is relatively easy, accomplished by etching away the joint between headplate and optical insert. Window replacement uses the same procedure as the second-stage installation described above.</p>
<p>The replacement procedure was attempted 5 times, 4 of which were successful in preserving or restoring optical quality to “like-new” condition, without inflicting detectable tissue damage. Three windows were removed and replaced following damage to the top surface of the optical insert, inflicted by feisty cage-mates with sharp incisors (at 91, 83, and 172 days post-installation; 91 days case unsuccessful). The remaining two cranial windows were removed at 20 days post-installation to facilitate direct tissue access for a study requiring cellular graft to the imaged region. We found that the removal needs to be performed slowly, taking great care to avoid capillary rupture in the exposed brain and surrounding granulation tissue. During each of these procedures, we observed the pattern of granulation tissue growth into the peripheral area of the chambers. Photos of the typical growth (as observable with window removed) at day 172 is shown in figure 3-d, and described in more detail below.</p>
<h2 id="evaluation-of-system-performance">Evaluation of System Performance</h2>
<p>Throughout development we implanted several prototypes to test the effect of various features and conditions. The cranial window design and surgical procedures described in this paper were attempted with 18 mice. Cranial window condition was evaluated by direct observation and evaluation of fluorescence dynamics in processed video recorded during periodic 5-minute imaging sessions. Direct (bright-field) observation with a stereoscopic microscope was useful for evaluating quality of the optical interface with brain tissue, as well as for tracking progression of granulation tissue growth in the surrounding space at the edges of the craniotomy. Analysis of cell dynamics measures from processed fluorescence imaging video indicated actual usability of the window for longitudinal studies requiring activity metrics at a cellular level.</p>
<h3 id="experimental-batches">Experimental Batches</h3>
<p>The first batch served as a short trial-run for the prototype and procedures whose performance in early tests suggested strong potential for long-term reliability. We ran the first batch for 4-6 weeks to get a better assessment of what we could expect for long-term viability. With this design and minor modifications to the surgical procedure, we felt comfortable using the window in studies with long-term requirements that would also allow for continuous assessment of the window’s performance in parallel. The first batch of windows was installed in April-May 2017 (N = 7) and was evaluated 2-3 times/week for just over 1 month. Several more were implanted for use in a long-term imaging study beginning with a second batch in June 2 017 (N = 6), and a third in October 2017 (N = 5). The results of these runs are reported below, summarized in figure 3-b.</p>
<h3 id="sustained-optical-quality-extended-over-a-year">Sustained Optical Quality Extended over a Year</h3>
<p>In the first batch of 7 mice, image quality provided by the window was more than sufficient to record cell dynamics across both image regions beginning 4 days after the window-installation and fluorophore-transfection procedure and was sustained for several weeks. At 4-6 weeks post-implant this batch of mice was evaluated and 6 of the 7 mice were discontinued and the installation procedure was adjusted for the next batch. The decision to discontinue in each case was based on observed deterioration in either the health of the mouse (4 out of the 6) or the optical quality of the window (2 out of the 6). See the discussion section for the mechanisms we suspected to underlay and procedural adjustments made to address these issues.</p>
<p>We continued to observe and image the 7th mouse to this day. Progression of the optical quality and fluorophore expression characteristics in bilateral image regions is depicted in figure 4 for this mouse. Optical quality at the window-tissue interface has remained consistent for longer than 18 months at the time of this manuscript’s preparation. The structure of granulation tissue surrounding the window at 12 months is described in detail below and depicted for this mouse in figure 3-d.</p>
<p>Similar to the first batch, the second batch of 6 mice was observed and recorded for some time (3 – 5 months) before discontinuing all except one most exceptional mouse. This mouse received a window replacement at day 83, and was imaged periodically for 11 months before terminating due to a health concern unrelated to the surgical procedure.</p>
<p>The imaging period for the last batch was extended beyond what was required for the stem-cell study to more thoroughly test the longer-term limits of sustained optical quality. Of 5 mice, 1 mouse did not recover as promptly as expected following the craniotomy procedure and was immediately discontinued. We observed consistent performance on long-term optical sustainability, extending over 12 months on average among the 8 windows. Half of these windows remain to be imaged to this day, and the rest were discontinued either followed by capillary rupture within image field or the deterioration of the health of the aged animals.</p>
<p>Figure 3-a presents transfected cells surrounding the initial virus injection site that can be identified throughout the field-of view of 1.3 mm x 1.3 mm. In this data set, estimates of cell number ranged from XX to XX with a relatively small variation among few randomly selected days between XX and XX.</p>
<h3 id="direct-observation-of-cranial-chamber">Direct Observation of Cranial Chamber</h3>
<p>We periodically examined the imaging chamber condition in all implanted mice using a stereoscopic microscope. Degradation of the optical interface was found frequently in prototypes/procedures that preceded the one mentioned here. This was observed as progressive extension of a cloudy white inhomogeneous layer across the brain-facing surface of the optical insert. Using the design and procedures reported in this paper, this type of degradation rarely occurred, limited to the cases reported above in Batch 1.</p>
<p>Remarkably, but not unexpectedly, tissue growth surrounding the insert was evident in all cases, regardless of window clarity. The tissue appeared highly vascularized, and grew from the craniotomy edge outward along the chamber floor (figure 3-c). This growth is a natural response to the tissue damage inflicted by any craniotomy procedure. The difference observed here is only that the growth does not extend under the window. Instead, it forms a non-adhesive interface with the vertical walls of the transcranial columns and diverges upward into the aseptic chamber, replacing the agarose filling between the optical insert and the adhesive cement covering the skull and chamber floor. To further investigate the structure of granulation tissue growth into the peripheral chamber areas we detached the optical insert for unobstructed observation in several mice. An especially gnarly example from a 6-month duration window is pictured in figure 3-d.</p>
<h3 id="section"></h3>
<h3 id="in-vivo-tracking-of-transplanted-stem-cells-over-long-term">In vivo Tracking of Transplanted Stem Cells over Long Term</h3>
<p>The ability to separate the installation procedure into Stage 1 and 2 was critical to capture the integration process of transplanted stem cells from day 4 and day 563 (figure 4). In addition to window installation, the experiment entails dissection of progenitor cells from E13.5 mouse embryos prior to transplantation which sequences require to be carried out in a timely manner. Tailored to this specific application, we performed Stage 1 of the installation procedure two days in advance, allowing time for tissue dissection immediately before conducting Stage 2 on day 0 (figure 4-b).</p>
<p>Figure 4-a shows one of the exceptional mice transplanted with two distinct cellular sources of glutamatergic and GABAergic neurons in left and right hemispheres, respectively. The optical interface of the cranial window sustained to</p>
<h2 id="section-1"></h2>
<h2 id="section-2"></h2>
<h2 id="section-3"></h2>
<h2 id="section-4"></h2>
<h2 id="section-5"></h2>
<h2 id="section-6"></h2>
<h2 id="section-7"></h2>
<h2 id="section-8"></h2>
<h2 id="section-9"></h2>
<h2 id="section-10"></h2>
<h2 id="section-11"></h2>
<h2 id="section-12"></h2>
<h2 id="section-13"></h2>
<h2 id="section-14"></h2>
<h2 id="section-15"></h2>
<h2 id="section-16"></h2>
<h2 id="section-17"></h2>
<h2 id="section-18"></h2>
<h2 id="section-19"></h2>
<h2 id="section-20"></h2>
<h2 id="section-21"></h2>
<h2 id="section-22"></h2>
<h2 id="section-23"></h2>
<h2 id="adaptability">Adaptability</h2>
<h2 id="section-24"></h2>
<ul>
<li><p>we then explored the ability of the cranial implant system to be adapted to provide greater utility</p></li>
<li><p>building upon the original configuration, we adapted the system to accommodate an enlarged window, proving both complete coverage of dorsal cortex and similar overall functionality of the original system</p></li>
<li><p>while much of the functional performance of the original system were realized by its physical structures, we customized existing features to meet the new design requirements and constraints for proper functioning</p></li>
<li><p>we also incorporated new features to enhance system performance</p></li>
<li><p>below, detailed technical descriptions associated with the reconfiguration</p></li>
</ul>
<p>---</p>
<ul>
<li><p>1. the headplate and the window frame were scaled to integrate a larger window</p></li>
<li><p>2. raised rims were added to the bottom surface of the headplate where it contacts squamosal suture of mouse cranium to supplement skull-to-headplate attachment</p>
<ul>
<li>to accommodate for the restricted attachment area due to the removal of a larger fraction of parietal and frontal bone</li>
</ul></li>
<li><p>3. a thin skirt was added to the perimeter of the bottom surface of the optical insert which gently wraps around the intact tissue and prevents tissue ingrowth</p>
<ul>
<li><p>in replacement of the vertical column</p></li>
<li><p>to accommodate for endocranium curves</p></li>
</ul></li>
<li><p>4. incorporated nuts and bolts to facilitate attachment/detachment between the modules</p>
<ul>
<li><p>in replacement of using dental cement</p></li>
<li><p>allows fine height adjustment, positioning, and installation of the window without the use of stereotaxic apparatus</p></li>
<li><p>added silicone coating to the chamber’s inner walls that form a seal with the optical insert upon its attachment</p></li>
</ul></li>
<li><p>5. incorporated an auxiliary module that protects the integrity of the window using a magnetically-coupled protective cap</p>
<ul>
<li>in replacement of tape</li>
</ul></li>
</ul>
<p>---</p>
<ul>
<li><p>wide-field imaging of vasculatures</p>
<ul>
<li><p>the mechanical barrier effectively blocks tissue growth (figure)</p></li>
<li><p>figure</p></li>
</ul></li>
<li><p>2-photon imaging of transplanted cancer cells</p></li>
</ul>
<h1 id="discussion">Discussion</h1>
<p><strong>Opening paragraph (key concepts to include)</strong></p>
<ul>
<li><p>we developed a cranial window system for in vivo optical imaging in mice</p></li>
<li><p>specifically, we developed a system that can provide sustained optical quality and tissue accessibility demonstrated through stem cell imaging</p></li>
<li><p>the performance was achieve by using silicone elastomer to incorporate the critical features</p></li>
<li><p>silicone-based window can easily incorporate features to be functional and provides good optical interface to record cellular activities through wide-field imaging</p></li>
<li><p>device design, two part component window designed to block tissue growth</p></li>
<li><p>performance, duration, access by taking off</p></li>
<li><p>additionally, the reported system can be adapted for different applications demonstrated through the two versions</p></li>
<li><p>special focus on searching for the critical features to achieve the desired performance without compromising the optical quality</p></li>
<li><p>also easier to systematically test features and optimize towards fulfilling desired outcomes</p></li>
</ul>
<p>The goal of this project has been to facilitate long-term studies requiring sustained optical access and intermittent physical access to the neocortex of intact brains in small animal research models – such as rats and mice. Specifically, we require a bilateral cortical windows suitable for wide-field imaging, and access to the underlying tissue for virus-mediated gene delivery and injection of exogenous labeled cells. We needed this access to be available as soon as possible post-installation, and for the optical quality to be sustained for several months. Experiment duration is limited using current window designs by progressive degradation of the optical light-path at the brain-to-window interface caused by highly scattering tissue growth. The device/system described here successfully fulfills the requirements of this objective, giving a stable and reliable optical window with unrestricted access for longer than one year.</p>
<p><strong><br />
</strong></p>
<p><strong>Main finding: sustained optical quality (Fig 3)</strong></p>
<ul>
<li><p>the implant effectively blocks the granulation tissue from growing within the image field, yielding a prolonged image duration often extending beyond a year</p></li>
<li><p>critical elements that facilitate the maintenance of the long-term optical quality</p>
<ul>
<li><p>system design features</p>
<ol type="1">
<li><p>mechanical barrier <sup>(Ref)</sup></p></li>
<li><p>adjustable height</p>
<p>provides a better fitting to replace the removed bone flap</p>
<p>incorporating both methods may have a synergetic effect on impeding the tissue growth</p>
<p>observed divergence of granulation tissue <sup>(Ref)</sup></p></li>
</ol></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p>other factors</p>
<ol type="1">
<li><p>anti-inflammatory drugs <sup>(Ref)</sup></p>
<ul>
<li><p>cortical steroid</p></li>
<li><p>NSAID</p></li>
</ul></li>
<li><p>air-tight seal within the chamber</p>
<ul>
<li>specific to the system design</li>
</ul></li>
</ol></li>
</ul>
<!-- -->
<ul>
<li><p>all four elements are equally critical to achieving the sustained optical quality</p></li>
<li><p>implementing the reported method may eliminate the primary reason for pre-terminating the image session due to tissue ingrowth</p></li>
<li><p>the system can facilitate studies that require long-term observation such as aging or disease progression</p></li>
</ul>
<p>Refer to the methods section for the specifics of surgical procedures for headplate installation and insert attachment. These procedures were established after testing the variable formulations in protocol. Of particular note, we found that administration of both a corticosteroid and a nonsteroidal anti-inflammatory drug had a substantial impact on the viability of the optical interface. Additionally, the silicone insert must be attached at the correct height, which must be determined by observation of the contact between the dura and the bottom surface of the insert. The insert must be depressed very slightly until full contact is made across the entire window, but pressing beyond necessary will quickly exert an undesired increase in intracranial pressure, increasing inflammation and adverse outcomes. Lastly, sealing the chamber is absolutely vital to the viability of the optical interface, as well as health of the animal. This includes a permanent seal between the chamber and skull, and a reversible seal between the chamber rim and optical insert. One must also ensure the agarose fill displaces all air within the chamber prior to sealing. Any remaining air pockets will be susceptible to bacteria growth and may disrupt normal intracranial and intermembrane pressures.</p>
<p><strong><br />
</strong></p>
<p><strong>Main finding: tissue accessibility (Fig 4)</strong></p>
<ul>
<li><p>the implant configuration consisting of two parts facilitates the installation procedure to be separable into distinctive stages carried out across days</p></li>
<li><p>similarly, the reversed sequence can be performed for repeatable tissue access at later time points</p></li>
<li><p><sup>Ref:</sup> the process may be comparable to a previously reported method of removing the entire glass window to access the tissue</p></li>
<li><p>here, however, we employed a faster and simpler method to reversibly detach and successively reattach the window without risking tissue damage compared to the laborious process required of glass windows</p></li>
<li><p>the approach is advantageous over designating a fixated access port <sup>(Ref)</sup>, providing full access without compromising the image field</p></li>
<li><p>the practicality of separating the installation operations of a complete system has yet been explored</p></li>
<li><p>the example of a comprehensive recording of the integration process of transplanted stem cells reported in this paper particularly well demonstrates the advantages of the ability</p>
<ul>
<li><p>it may save time and resources – especially during the prototype stages – by allowing time to ensure each implanted animal fully recovers from the initial procedure</p></li>
<li><p>the delay allows normalization of the immune response or the heightened inflammation triggered by craniotomy before attempting a tissue intervention that is sensitive to these conditions (e.g. viral or cell injections)</p></li>
<li><p>the most remarkable advantage, however, is the ability to image the first tissue intervention from day 0</p></li>
</ul></li>
</ul>
<p>The ability to separate the operations necessary for installation of a complete system has a number of advantages, and is particularly well supported using the reported system. It may save time and resources – especially during the prototype stages – by allowing time to ensure each implanted animal fully recovers from the initial procedure. Additionally, the delay allows normalization of the immune response or the heightened inflammation triggered by craniotomy before attempting a tissue intervention that is sensitive to these conditions (e.g. viral or cell injections). The most remarkable advantage, however, is the ability to image the first tissue intervention from day 0.</p>
<p><strong><br />
</strong></p>
<p><strong>Main finding: adaptation (Fig 5)</strong></p>
<ul>
<li><p>the system framework is adaptable to accommodate various applications or to enhance its performance to reflect new technologies and demands</p></li>
<li><p>here, we demonstrated the technical feasibility to future-proof the original system to adapt to continuously evolving image sensor technologies, allowing access to cellular interactions across multiple brain regions using wide-field imaging <sup>Ref</sup></p></li>
<li><p>as an inherent aspect of any design process, the adaptation of the original design evolved over the course of prototyping and testing to ensure consistent overall functional performance</p></li>
<li><p>the iterative process, however, is much feasible with the major progresses of manufacturing and its increased versatility, providing better quality, customization, lower cost and shorter production time <sup>Ref</sup></p></li>
<li><p>in an effort to compare various manufacturing technologies, we explored manufacturing the finalized product design through a number of companies and advanced with 3D metal printing with overall satisfaction at i.materialise – we had also developed the parts through other rapid prototyping companies including Shapeways and Sculpteo</p></li>
<li><p>additionally, various features and functions of the silicone insert were transformed and extended to conform to new design requirements, some requiring distinctively different design approaches</p></li>
<li><p>we could benefit from the versatility of silicone elastomer to cover a spectrum of design strategies to optimize its configuration</p></li>
<li><p>in fact, we found that a seemingly subtle physical difference can contribute to more than one function</p></li>
<li><p>for example, the inclusion of a thin skirt extending below the optical insert, which was incorporated to provide protection against tissue growth within the image field, may also facilitate the brain to conform to the optical interface of the window over time resulting in a flat imaging plane, optimal for wide-field imaging, which was unachievable previously <sup>Ref</sup></p></li>
<li><p>overall, the design principles established from the initial development are robust and can be applied to new developments or refinements while preserving all qualities of the original implant</p></li>
<li><p>the CAD designs of the reported systems are accessible in open source and can be modified and extended accordingly to the evolving demands and technologies</p></li>
<li><p>We, the authors, are calling for replication, adaptation, evaluation (i.e. continued open / shared development).</p></li>
</ul>
<p><strong>Further improvements</strong></p>
<ul>
<li><p>primarily explored the ability to mold precise and complex features using silicone elastomer to discover configurations to improve image performance</p></li>
<li><p>encapsulate electrodes or optical guides <sup>Ref</sup></p>
<ul>
<li><p>Replace with combination optical + integrated electrode window</p>
<ul>
<li>Imaging as well as optogenetics stimulation</li>
</ul></li>
</ul></li>
<li><ul>
<li>More significantly, the encapsulation of carbon, metal colloidal particles or quantum dots into polymer hydrogel networks will impart them with exclusive thermal, sonic, optical, electrical or magnetic properties</li>
</ul></li>
<li><p>the polymer interface may provide means to penetrate through directly <sup>Ref</sup> for electrophysiological recording or drug infusion allowing recording and/or manipulation during imaging session</p></li>
</ul>
<!-- -->
<ul>
<li><p>thickness of the window, chromatic aberration, wide-field and 2-photon imaging</p></li>
<li><p>lenses</p></li>
</ul>
<!-- -->
<ul>
<li><p>As an innovative means to</p></li>
<li><p>casting into practically any shape, size, or form</p></li>
<li><p>embed/ Integrate</p></li>
<li><p>new materials for fabrication</p></li>
</ul>
<p><strong>Conclusion</strong></p>
<ul>
<li><p>here, we took an unique approach of using silicone elastomer to develop a cranial window system with the specific goals to build a system that performs reliable to provide long-term maintenance of optical quality and an easy and repeatable tissue access</p></li>
<li><p>additionally, we prove the system can be easily adaptable for different applications</p></li>
<li><p>the develop of such device will eliminate the physical barrier we have to probe the complexity of the brain – capability to facilitate longitudinal optical imaging experiment in mice</p></li>
</ul>
<p>http://npi-med.com/news/top-5-reasons-consider-silicone-molding-medical-device-project/ - we used silicone elastomer to develop the window which provides versatility to add functions</p>
<ol type="a">
<li><ol start="2" type="a">
<li><ol type="i">
<li><ol start="2" type="i">
<li></li>
</ol></li>
</ol></li>
</ol></li>
<li><ol start="3" type="i">
<li><ol start="4" type="i">
<li></li>
</ol></li>
</ol></li>
</ol>
<!-- -->
<ol type="a">
<li><ol type="i">
<li></li>
</ol></li>
<li><ol start="3" type="a">
<li><ol start="4" type="a">
<li><ol start="5" type="a">
<li><ol start="2" type="i">
<li></li>
</ol></li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<!-- -->
<ol type="a">
<li><ol start="2" type="a">
<li></li>
</ol></li>
</ol>
<!-- -->
<ol type="a">
<li><ol start="2" type="a">
<li></li>
</ol></li>
</ol>
<h1 id="materials-and-methods">Materials and Methods</h1>
<h2 id="device-development-and-fabrication">Device development and fabrication</h2>
<p>Components were designed using SolidWorks. Prototypes were fabricated using CamBam to generate toolpaths in G-code for machining on a CNC mill. The headplate and window frame were milled from aluminum plate. The mold for casting PDMS-based windows was designed in three parts. The middle component was milled from PTFE. The outer components were made using a laser-cutter and acrylic sheet (Supplementary). CAD files are shared on XXXXX(github?)XXXXX.</p>
<h2 id="window-casting-procedure">Window casting procedure</h2>
<p>The PDMS-based windows were fabricated through vacuum casting procedure. Prior to casting, window frames and two glass coverslips (Corning, 2947-75x38), prepared in advance through plasma etching (30 sec, Power setting) and silanization using Trichlorosilane (Sigma-Aldrich, 448931-10G), were inserted into the mold. The mold was then placed between two custom-made acrylic plates with silicone gaskets in between and was assembled using bolts around the perimeter. The pressure control port (McMaster-Carr, 5454K61) was connected to the house vacuum line, and the fill port (McMaster-Carr, 2844K11) was connected to uncured PDMS polymer (Dow Corning Sylgard 184) (1:10 by weight), thoroughly mixed and degassed in advance. The liquid-state polymer was then drawn into the mold filling the volume in between the two coverslips using vacuum. Once polymer displaced all air, vacuum was released and positive pressure was applied through the pressure control port after plugging the fill port. While maintaining positive pressure, the polymer was cured at 75ºC for 12 hours. Finally, the windows were released from the mold and trimmed using scalpels. Windows were handled so as to protect the top and bottom surfaces from damage or debris. The completed window was sterilized in an autoclave before use.</p>
<h2 id="surgical-procedures">Surgical procedures</h2>
<p>Animal care for surgical procedures is described below, and the details specific to each procedure are in the sections that follow. All procedures were approved by the Institutional Animal Care and Use at Boston University. Stereotaxic surgeries were performed on 6 to 8 weeks old female C57BL/6 mice (Charles River Laboratories). Pre-operative care for the initial headplate and craniotomy procedure included subcutaneous administration of meloxicam (NSAID, 2.5 ug/g) and buprenorphine (opioid analgesics, 0.3 ug/g), and intramuscular injection of dexamethasone (corticosteroid, 5 ug/g) one hour before surgery. Meloxicam and buprenorphine were continued postoperatively every 12 hours for 48 hours. Meloxicam was also given before and after procedures where brain tissue was exposed, i.e. those for intracerebral injection and window replacement. For all procedures described below, mice were placed under general anesthesia with isoflurane mixed with oxygen.</p>
<h2 id="headplate-installation-and-craniotomy">Headplate installation and craniotomy</h2>
<p>We shaved the top of the mouse’s head and sterilized the skin using 70% alcohol and 7.5% Betadine. We made a 1cm midline sagittal incision through the scalp using surgical scissors, and retracted laterally using a self-retaining retractor (WPI, 501968). To prepare the cranial surface, we applied 3% hydrogen peroxide to oxidize and facilitate removal of periosteal tissue with cotton tip swabs. The surface was then marked up before headplate installation followed by craniotomy. We targeted laterally symmetric craniotomies with edge length 2.2 mm centered at coordinates ±1.83 mm lateral to sagittal suture and 1.00 mm anterior to bregma. First, we used a surgical skin marker (FST, 18000-30) to roughly indicate the site of each craniotomy and enhance contrast of the edges to be etched. We etched the corners and edges using a dental drill with a FG 1/4 round carbide burr (Fig. 2 Step 1(L)). This way of marking the edges facilitates headplate placement and also aids recovery of the intended craniotomy position despite being covered by semi-transparent adhesive cement in the following steps.</p>
<p>We used a custom stereotaxic attachment (Supplementary) to position the headplate symmetrically aligned with the marked sites, and to hold it horizontal while bonding to skull. The headplate was anchored directly to the skull using either opaque or semi-clear quick adhesive cement (Parkell, S380). Subsequently, we began each craniotomy by drilling along the marked edges (Fig. 2 Step 2(L)). We frequently stopped to flush debris from the site using sterile saline and an aspirator. Once separated from the surrounding skull, the bone flap was carefully removed using a pair of sharp forceps (FST, 91150-20) and a 45º micro probe (FST, 10066-15) while keeping the dura intact (Fig. 2 Step 1(R)). At this point, we either attached the optical window or sealed the area with a layer of non-toxic silicone adhesive (WPI, KWIK-SIL).</p>
<h2 id="optical-insert-attachment">Optical insert attachment</h2>
<p>The optical insert attachment can be performed immediately following the craniotomy or deferred to the day of injection as described below (Fig. 2 Step 4). First, we filled the headplate chamber with sterile 0.5% agarose solution, immersing the exposed brain. Enough agarose was added so as to overflow the walls of the chamber as the window is inserted, ensuring no air gaps remain in the space between the walls of the chamber and the window, below the joint to be sealed. Next, the window was placed in the chamber, directly over the craniotomy, in gentle contact with the exposed tissue. We used a ttaxic apparatus to adjust the window height and secure its position during attachment. This was aided by an attachment – similar to that used for headplate installation – which fixed the angle of the window’s top surface parallel with that of the headplate. The height adjustment required depressing the window until full contact was observed between the inner window surface and the dura. The window was tacked in place by applying an accelerated light-cured composite (Pentron Clinic, Flow-It ALC) in at least three points, bonding the window frame to the anterior and posterior walls of the headplate. At this point the guide was removed and the joint was prepped for sealing. Excess agarose (polymerized overflow from the window insertion step) was aspirated away to expose and clean the headplate surface surrounding the window. The chamber was sealed by filling the joint between headplate and optical insert with dental cement (Stoelting, 51458) using a P200 pipette. The window surface was protected by applying a double layer adhesive strip made of gaffers tape over a transparent adhesive film dressing (3M: Tegaderm, 70200749201).</p>
<h2 id="window-detachment-and-replacement">Window detachment and replacement</h2>
<p>The dental cement connecting the window and headplate was etched away using a dental drill. Before removing the window, we thoroughly flushed debris from the surrounding surfaces with sterile saline. Replacement windows were installed using the same procedures described above for initial attachment. Localizing the replacement window to the same position was aided by the expansion of granulation tissue up to the walls of the prior window.</p>
<h2 id="injection">Injection</h2>
<p>The exposed brain was flushed with sterile saline before and after each injection. Injections were made using pulled glass micropipettes with inner tip diameter ranging from 40 and 80 um (WPI, 504949). The micropipette was initially back-filled with mineral oil, then secured onto a microprocessor controlled injector (WPI, NANOLITER2010). The micropipette was then front-loaded with virus or cells using a controller (WPI, Sys-Micro4). In general, an injection of 230 nL of cells labeled with AAV9.CAG.GCaMP6f (AV-9-PV3081, Penn Vector Core) at 10<sup>6</sup> cells/uL, or 230 nL of AAV9.Syn.GCaMP6f (AV-9-PV2822, Penn Vector Core). Injection was performed approximately 500 um deep into the cortex at the rate of 46 nL/min near the center of the imaging field, while avoiding blood vessels to maximize the observable cells around the injection site (Fig. 2 Step 3). The micropipette was left to sit for an additional 2 min at the injection site before slow withdrawal.</p>
<h2 id="wide-field-in-vivo-imaging-and-microscope-setup">Wide-field in vivo imaging and microscope setup</h2>
<p>Wide-field epifluorescence imaging was accomplished using a custom microscope equipped with a sCMOS camera (Hamamatsu, ORCA Flash 4.0XX), 470 nm LED (Thorlabs, M470L3), excitation and emission filters of XX and XX, a dichroic mirror (XX), and a 10x objective lens (Mitutoyo, 378-803-3). Mice were positioned under the microscope for imaging using a custom headplate holder (Supplementary) and allowed to run on an air-supported spherical treadmill<sup>18</sup> as pictured in Figure 2 Step 5. The camera recorded a field-of-view of approximately 1.3 mm x 1.3 mm using an image resolution of 2048 x 2048 or more commonly 1024 x 1024. Continuous image sequences were acquired at 40 to 60 frames-per-second for 5 to 7 minutes. We selected the field to image within each site by roughly centering around the injection site. To focus the microscope on labeled cells in the superficial layers of cortex, we focused on the surface vasculature to find a stable reference, then advanced the focal plane 50 to 150 um until multiple cells were distinguishable. A reference image of the selected image was recorded for each site and used later to reacquire the same field across image sessions. Alignment to this reference image relied primarily on using the major blood vessels as landmarks to guide microscope position in the XY plane. Image sequences were stored for subsequent processing and analysis.</p>
<h1 id="reference">Reference</h1>
<h1 id="yang-w.-yuste-r.-in-vivo-imaging-of-neural-activity.-nature-methods-2017.-doi10.1038nmeth.42302.-portera-cailliau-c.-weimer-r.-m.-de-paola-v.-caroni-p.-svoboda-k.-diverse-modes-of-axon-elaboration-in-the-developing-neocortex.-plos-biol.-2005.-doi10.1371journal.pbio.00302723.-grienberger-c.-konnerth-a.-imaging-calcium-in-neurons.-neuron-73-86285-2012.4.-chen-t.-w.-et-al.-ultrasensitive-fluorescent-proteins-for-imaging-neuronal-activity.-nature-499-295300-2013.5.-holtmaat-a.-et-al.-long-term-high-resolution-imaging-in-the-mouse-neocortex-through-a-chronic-cranial-window.-nat.-protoc.-4-112844-2009.6.-kim-t.-h.-et-al.-long-term-optical-access-to-an-estimated-one-million-neurons-in-the-live-mouse-cortex.-cell-rep.-2016.-doi10.1016j.celrep.2016.12.0047.-goldey-g.-j.-et-al.-removable-cranial-windows-for-long-term-imaging-in-awake-mice.-nat.-protoc.-9-251538-2014.8.-sheintuch-l.-et-al.-tracking-the-same-neurons-across-multiple-days-in-ca2-imaging-data.-cell-rep.-21-11021115-2017.9.-mohammed-a.-i.-et-al.-an-integrative-approach-for-analyzing-hundreds-of-neurons-in-task-performing-mice-using-wide-field-calcium-imaging.-sci.-rep.-2016.-doi10.1038srep2098610.-tran-c.-h.-t.-gordon-g.-r.-acute-two-photon-imaging-of-the-neurovascular-unit-in-the-cortex-of-active-mice.-front.-cell.-neurosci.-9-11-2015.11.-shih-a.-y.-et-al.-two-photon-microscopy-as-a-tool-to-study-blood-flow-and-neurovascular-coupling-in-the-rodent-brain.-j-cereb-blood-flow-metab-32-12771309-2012.12.-wilson-f.-a.-w.-ryou-j.-w.-kim-b.-h.-greenberg-p.-a.-amelioration-of-dural-granulation-tissue-growth-for-primate-neurophysiology.-j.-neurosci.-methods-144-203205-2005.13.-dorand-r.-d.-barkauskas-d.-s.-evans-t.-a.-petrosiute-a.-huang-a.-y.-comparison-of-intravital-thinned-skull-and-cranial-window-approaches-to-study-cns-immunobiology-in-the-mouse-cortex.-intravital-2014.-doi10.4161intv.2972814.-li-x.-et-al.-skin-suturing-and-cortical-surface-viral-infusion-improves-imaging-of-neuronal-ensemble-activity-with-head-mounted-miniature-microscopes.-j.-neurosci.-methods-291-238248-2017.15.-moshayedi-p.-et-al.-the-relationship-between-glial-cell-mechanosensitivity-and-foreign-body-reactions-in-the-central-nervous-system.-biomaterials-35-39193925-2014.16.-helmchen-f.-denk-w.-deep-tissue-two-photon-microscopy.-nat.-methods-2-93240-2005.17.-takehara-h.-et-al.-lab-on-a-brain-implantable-micro-optical-fluidic-devices-for-neural-cell-analysis-in-vivo.-sci.-rep.-4-6721-2014.18.-dombeck-d.-a-khabbaz-a.-n.-collman-f.-adelman-t.-l.-tank-d.-w.-imaging-large-scale-neural-activity-with-cellular-resolution-in-awake-mobile-mice.-neuron-56-4357-2007.19.-fenrich-k.-k.-et-al.-long-term-in-vivo-imaging-of-normal-and-pathological-mouse-spinal-cord-with-subcellular-resolution-using-implanted-glass-windows.-j.-physiol.-590-36653675-2012.20.-alieva-m.-ritsma-l.-giedt-r.-j.-weissleder-r.-van-rheenen-j.-imaging-windows-for-long-term-intravital-imaging-general-overview-and-technical-insights.-intravital-2014.-doi10.4161intv.2991721.-mostany-r.-portera-cailliau-c.-a-craniotomy-surgery-procedure-for-chronic-brain-imaging.-j.-vis.-exp.-2008.-doi10.379168022.-garaschuk-o.-milos-r.-i.-konnerth-a.-targeted-bulk-loading-of-fluorescent-indicators-for-two-photon-brain-imaging-in-vivo.-nat.-protoc.-1-3806-2006.23.-roome-c.-j.-kuhn-b.-chronic-cranial-window-with-access-port-for-repeated-cellular-manipulations-drug-application-and-electrophysiology.-front.-cell.-neurosci.-8-379-2014.24.-zuluaga-ramirez-v.-rom-s.-persidsky-y.-craniula-a-cranial-window-technique-for-prolonged-imaging-of-brain-surface-vasculature-with-simultaneous-adjacent-intracerebral-injection.-fluids-barriers-cns-12-24-2015.25.-arieli-a.-grinvald-a.-optical-imaging-combined-with-targeted-electrical-recordings-microstimulation-or-tracer-injections.-j.-neurosci.-methods-2002.-doi10.1016s0165-02700200022-526.-arieli-a.-grinvald-a.-slovin-h.-dural-substitute-for-long-term-imaging-of-cortical-activity-in-behaving-monkeys-and-its-clinical-implications.-j.-neurosci.-methods-114-119133-2002.27.-heo-c.-et-al.-a-soft-transparent-freely-accessible-cranial-window-for-chronic-imaging-and-electrophysiology.-cell-rep.-21-1013-2017.28.-jackson-n.-muthuswamy-j.-artificial-dural-sealant-that-allows-multiple-penetrations-of-implantable-brain-probes.-j.-neurosci.-methods-171-147152-2008.29.-spitler-k.-m.-gothard-k.-m.-a-removable-silicone-elastomer-seal-reduces-granulation-tissue-growth-and-maintains-the-sterility-of-recording-chambers-for-primate-neurophysiology.-j.-neurosci.-methods-169-2326-2008.30.-li-m.-liu-f.-jiang-h.-lee-t.-s.-tang-s.-long-term-two-photon-imaging-in-awake-macaque-monkey.-neuron-93-10491057.e3-2017.figures">1. Yang, W. &amp; Yuste, R. In vivo imaging of neural activity. <em>Nature Methods</em> (2017). doi:10.1038/nmeth.42302. Portera-Cailliau, C., Weimer, R. M., De Paola, V., Caroni, P. &amp; Svoboda, K. Diverse modes of axon elaboration in the developing neocortex. <em>PLoS Biol.</em> (2005). doi:10.1371/journal.pbio.00302723. Grienberger, C. &amp; Konnerth, A. Imaging calcium in neurons. <em>Neuron</em> <strong>73,</strong> 862–85 (2012).4. Chen, T. W. <em>et al.</em> Ultrasensitive fluorescent proteins for imaging neuronal activity. <em>Nature</em> <strong>499,</strong> 295–300 (2013).5. Holtmaat, A. <em>et al.</em> Long-term, high-resolution imaging in the mouse neocortex through a chronic cranial window. <em>Nat. Protoc.</em> <strong>4,</strong> 1128–44 (2009).6. Kim, T. H. <em>et al.</em> Long-Term Optical Access to an Estimated One Million Neurons in the Live Mouse Cortex. <em>Cell Rep.</em> (2016). doi:10.1016/j.celrep.2016.12.0047. Goldey, G. J. <em>et al.</em> Removable cranial windows for long-term imaging in awake mice. <em>Nat. Protoc.</em> <strong>9,</strong> 2515–38 (2014).8. Sheintuch, L. <em>et al.</em> Tracking the Same Neurons across Multiple Days in Ca2+ Imaging Data. <em>Cell Rep.</em> <strong>21,</strong> 1102–1115 (2017).9. Mohammed, A. I. <em>et al.</em> An integrative approach for analyzing hundreds of neurons in task performing mice using wide-field calcium imaging. <em>Sci. Rep.</em> (2016). doi:10.1038/srep2098610. Tran, C. H. T. &amp; Gordon, G. R. Acute two-photon imaging of the neurovascular unit in the cortex of active mice. <em>Front. Cell. Neurosci.</em> <strong>9,</strong> 11 (2015).11. Shih, A. Y. <em>et al.</em> Two-photon microscopy as a tool to study blood flow and neurovascular coupling in the rodent brain. <em>J Cereb Blood Flow Metab</em> <strong>32,</strong> 1277–1309 (2012).12. Wilson, F. A. W., Ryou, J. W., Kim, B. H. &amp; Greenberg, P. A. Amelioration of dural granulation tissue growth for primate neurophysiology. <em>J. Neurosci. Methods</em> <strong>144,</strong> 203–205 (2005).13. Dorand, R. D., Barkauskas, D. S., Evans, T. A., Petrosiute, A. &amp; Huang, A. Y. Comparison of intravital thinned skull and cranial window approaches to study cns immunobiology in the mouse cortex. <em>IntraVital</em> (2014). doi:10.4161/intv.2972814. Li, X. <em>et al.</em> Skin suturing and cortical surface viral infusion improves imaging of neuronal ensemble activity with head-mounted miniature microscopes. <em>J. Neurosci. Methods</em> <strong>291,</strong> 238–248 (2017).15. Moshayedi, P. <em>et al.</em> The relationship between glial cell mechanosensitivity and foreign body reactions in the central nervous system. <em>Biomaterials</em> <strong>35,</strong> 3919–3925 (2014).16. Helmchen, F. &amp; Denk, W. Deep tissue two-photon microscopy. <em>Nat. Methods</em> <strong>2,</strong> 932–40 (2005).17. Takehara, H. <em>et al.</em> Lab-on-a-brain: implantable micro-optical fluidic devices for neural cell analysis in vivo. <em>Sci. Rep.</em> <strong>4,</strong> 6721 (2014).18. Dombeck, D. a, Khabbaz, A. N., Collman, F., Adelman, T. L. &amp; Tank, D. W. Imaging large-scale neural activity with cellular resolution in awake, mobile mice. <em>Neuron</em> <strong>56,</strong> 43–57 (2007).19. Fenrich, K. K. <em>et al.</em> Long-term in vivo imaging of normal and pathological mouse spinal cord with subcellular resolution using implanted glass windows. <em>J. Physiol.</em> <strong>590,</strong> 3665–3675 (2012).20. Alieva, M., Ritsma, L., Giedt, R. J., Weissleder, R. &amp; Van Rheenen, J. Imaging windows for long-term intravital imaging: General overview and technical insights. <em>IntraVital</em> (2014). doi:10.4161/intv.2991721. Mostany, R. &amp; Portera-Cailliau, C. A Craniotomy Surgery Procedure for Chronic Brain Imaging. <em>J. Vis. Exp.</em> (2008). doi:10.3791/68022. Garaschuk, O., Milos, R.-I. &amp; Konnerth, A. Targeted bulk-loading of fluorescent indicators for two-photon brain imaging in vivo. <em>Nat. Protoc.</em> <strong>1,</strong> 380–6 (2006).23. Roome, C. J. &amp; Kuhn, B. Chronic cranial window with access port for repeated cellular manipulations, drug application, and electrophysiology. <em>Front. Cell. Neurosci.</em> <strong>8,</strong> 379 (2014).24. Zuluaga-Ramirez, V., Rom, S. &amp; Persidsky, Y. Craniula: A cranial window technique for prolonged imaging of brain surface vasculature with simultaneous adjacent intracerebral injection. <em>Fluids Barriers CNS</em> <strong>12,</strong> 24 (2015).25. Arieli, A. &amp; Grinvald, A. Optical imaging combined with targeted electrical recordings, microstimulation, or tracer injections. <em>J. Neurosci. Methods</em> (2002). doi:10.1016/S0165-0270(02)00022-526. Arieli, A., Grinvald, A. &amp; Slovin, H. Dural substitute for long-term imaging of cortical activity in behaving monkeys and its clinical implications. <em>J. Neurosci. Methods</em> <strong>114,</strong> 119–133 (2002).27. Heo, C. <em>et al.</em> A soft, transparent, freely accessible cranial window for chronic imaging and electrophysiology. <em>Cell Rep.</em> <strong>21,</strong> 10–13 (2017).28. Jackson, N. &amp; Muthuswamy, J. Artificial dural sealant that allows multiple penetrations of implantable brain probes. <em>J. Neurosci. Methods</em> <strong>171,</strong> 147–152 (2008).29. Spitler, K. M. &amp; Gothard, K. M. A removable silicone elastomer seal reduces granulation tissue growth and maintains the sterility of recording chambers for primate neurophysiology. <em>J. Neurosci. Methods</em> <strong>169,</strong> 23–26 (2008).30. Li, M., Liu, F., Jiang, H., Lee, T. S. &amp; Tang, S. Long-Term Two-Photon Imaging in Awake Macaque Monkey. <em>Neuron</em> <strong>93,</strong> 1049–1057.e3 (2017).Figures</h1>
<h2 id="figure-1-design-assembly-and-installation-of-the-polymeric-optical-window.">Figure 1 Design, assembly and installation of the polymeric optical window.</h2>
<p>(a) Schematic of the window system assembled and installed to the animal. Cross section shows the placement of the window as the bottom plane contacts the brain as it extends below the surface of the skull. Side view shows the adjustable height between the window and the headplate to compensate the cavity from skull removal. (b) Top and side view of the polymer window and the headplate. The window frame and the headplate are made of aluminum, and the window is molded in PDMS. Exploded view shows how the two pieces can be assembled. Front side of the headplate includes a feature that avoids hitting the eyes of the animal. (c) Top and (d) side view of the parts. (f) Parts assembled and installed to the animal.</p>
<h2 id="figure-2-flexible-experimental-timeline-for-surgery-injection-and-imaging.">Figure 2 Flexible experimental timeline for surgery, injection and imaging.</h2>
<p>Two-part assembly allows flexible timeline to separate the procedure of headplate installation and craniotomy from injection which helps with optical clarity during the early period imaging. On Day -2 which can extend upto Day -7, headplate installation and craniotomy is performed. Step 1 shows the placement of the headplate above the intended craniotomy region (before) and the fixation to the skull using opaque Metabond (after). After at least 48 hours of recovery period, injection and window installation is performed on Day 0. Step 3 shows injection using a micropipette filled with Evans Blue dye for demonstration only. Step 4 shows window installed above headplate using dental cement. Any cavity between the two pieces are filled with sterile 0.5% agarose. Lastly, imaging session can start from Day 4. Step 5 shows a mouse freely running on a treadmill during a imaging session while its head movement is restrained. The microscope is installed above the animal. Step 6 shows after window removal (before) that allows full tissue access, and a new window installed (after) that allows continuation of imaging over the same region.</p>
<h2 id="figure-3-extended-optical-clarity-by-biocompatible-design-of-the-system.">Figure 3 Extended optical clarity by biocompatible design of the system.</h2>
<p>(a) The space between the window and the skull filled with agarose is progressively replaced by granulation tissue over time. At 6 month, granulation tissue surrounds the polymer window and vasculature formation is visible resembling a wound healing process. By 1 year, the intermediate space is filled with tissue embedded with vasculatures. The figures are from different animal that represent the corresponding time-point from the initially window installation - the window was installed using opaque Metabond on the samples for Day 0 and 1 year and semi-clear Metabond on the sample for 6 month. (b) Snapshots with the field-of-view of 1.3 x 1.3 mm were taken over the same regions on both hemispheres using the major blood vessels as landmarks (circled same regions). The optical clarity maintains over time to detect cellular dynamics of individual cells. Some GCaMP6f labeled cells are visible. In the left hemisphere, vascular remodelings are notable during the early period.</p>
<h1 id="video-processing">Video Processing</h1>
<!-- This section borrows from AIM-1 and AIM-2 from the prospectus. -->
<h2 id="continuous-online-video-processing">Continuous Online Video Processing</h2>
<h2 id="introduction-1">Introduction</h2>
<p>The current generation of sCMOS cameras can capture full-frame resolution video at either 30 fps or 100 fps, depending on the data interface between camera and computer (USB3.0 or CameraLink). At 16-bits per pixel and 2048x2048 pixels, the maximum data rate for the USB3.0 camera is 240 MB/s. Imaging sessions typically last 30-minutes or less. However, pixels are typically binned down 2x2, and frame rate often reduced; processing speed and storage constraints are the primary motivation for doing so. The effect of doubling resolution on processing time when using the graphics card is nearly negligible, however. By identifying ROIs online and extracting the traces of neural activity allows us to discard acquired images and instead store the traces only, or feed them into an encoder for online analysis.</p>
<p>Graphics Processing Units were traditionally developed for the consumer gaming market. They are optimized for the process which involves translating a continuous stream of information into a two-dimensional image format for transfer to a computer monitor. In the context of gaming, the stream of information received by a GPU describes the state of objects in a dynamic virtual environment, and is typically produced by a video game engine. These processors are highly optimized for this task. However, they are equally efficient at performing the same type of procedure in reverse – reducing a stream of images to structured streams of information about dynamic objects in the image – and thus are popular for video processing and computer vision applications.</p>
<p>Any GPU architecture will consist of a hierarchy of parallel processing elements. NVIDIA’s CUDA architecture refers to the lowest level processing element as “CUDA Cores” and the highest level as “Symmetric Multiprocessors.” Typically data is distributed across cores and multiprocessors by specifying a layout in C-code using different terminology, “threads” and “blocks.” Blocks are then said to be organized in a “grid.” Adapting traditional image processing or computer vision algorithms to run quickly on a GPU involves finding a way to distribute threads efficiently, ideally minimizing communication between blocks.</p>
<p>MATLAB makes processing data using the GPU seemingly trivial by overloading a large number of built in functions. Performance varies, however, and often the fastest way to implement a routine is by writing a kernel-type subfunction – written as if it operates on single (scalar) elements only – that can be called on all pixels at once, or all pixel-subscripts, which the function can then use to retrieve the pixel value at the given subscript. The kernel-type function is compiled into a CUDA kernel the first time it’s called, then repeated calls call the kernel directly, having minimal overhead. Calls go through the arrayfun() function.</p>
<p>Data transfers between system memory and graphics memory is often the major bottle-neck. Therefore, this operation is best performed only once. However, once data is on the GPU, many complex operations can be performed to extract information from the image, all while staying under the processing-time limit imposed by the frame-rate of the camera sending the images.</p>
<h2 id="method-and-approach">Method and Approach</h2>
<p>The entire procedure for processing images and extracting cell signals can be performed in substantially less time than most commonly available tools using the approach described in Aim 1, particularly the methods for restricting the spatial extent of pixel-association operations, and distributing operations across parallel processing cores using a Single Program Multiple Data (SPMD) archetype. However, the total time still exceeds that of the acquisition session. Inefficiency arises from the overhead involved with distributing data and passing information between separate parallel processes. Graphics cards, however execute in what’s called Single Instruction Multiple Data (SIMD) fashion, to distribute computation across the thousands of processing cores.</p>
<p>The processing components are implemented using the MATLAB System-Object framework, which allows for slightly faster performance through internal optimizations having to do with memory allocation. Most system objects, each representing one step in the serial processing and signal-extraction procedure, also have companion functions that implement the computation-heavy components of each algorithm using a pre-compiled CUDA kernel.</p>
<h3 id="benchmarking-general-performance">Benchmarking &amp; General Performance</h3>
<p>Built-in MATLAB functions that execute on the GPU can be profiled with benchmarking functions like <em>gputimeit()</em>, or with the <em>tic/toc</em> functions. When execution isn’t fast enough, they need to be replaced with custom functions. The custom functions typically achieve the speed up necessary by enabling the operation to carried out on several frames at once. This reduces the over-head costs inposed for each function call by spreading it over several frames. This solution is not ideal, as it increases the latency of solutions, however does not preclude implementation in real-time system if the procedures are adapted to run on a real-time hybrid system-on-module like NVIDIA’s Tegra X1, which should involve minimal effort once a standard set of successful procedures is realized. The current implementation tests the processing time of each stage of the process to ensure that the sum is less than the acquisition time for each frame dictated by the inverse of the frame-rate (30-50 milliseconds).</p>
<h3 id="buffered-operations">Buffered Operations</h3>
<p>Combining frames for each operation can result in near linear speedup. For example, for the phase-correlation step required for motion correction, the FFT and IFFT are called on 16 image-frames at once, and the time take to accomplish is approximately the same as if the operation were called on 1 frame. This essentially leads to a 16x speedup, though the latency is also increased slightly. The best size to use is difficult to pre-determine, and typically must be measured for varying size ‘chunks’ using the benchmarking functions indicated above. The system objects manage the details necessary to allow buffered chunks of video to be passed to each stage without introducing artifacts at the temporal edges between chunks.</p>
<h3 id="image-pre-processing-motion-correction">Image Pre-Processing &amp; Motion Correction</h3>
<p>Pre-processing is implemented as with the offline procedure, with a few changes. Images are aligned in chunks, and they are aligned sequentially to two templates. One template is the most recent stable frame from the preceding chunk. The other is a recursively temporal-low-pass filtered image that mitigates slow drifts. Aligning to the first template is usually more stable as the brightness of cells in the recent image will be more similar to those in the current chunk than will be the brightness of cells in the slow-moving average.</p>
<p>The displacement of each frame is found to sub-pixel precision, then used with a custom bicubic resampling kernel that replaces any pixels at the edges with images from the moving average.</p>
<h3 id="sequential-statistics">Sequential Statistics</h3>
<p>A number of statistics for each pixel are updated online and can be used for normalization and segmentation procedures later in the process. These include the minimum and maximum pixel intensity, and the first four central moments, which are easily converted to the mean, variance, skewness, and kurtosis. The formulas for making these calculations are given below, and are performed in a highly efficient manner as data are kept local to each processing core, and repeat computations are minimized.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb7-1" title="1">n = n + <span class="fl">1</span>;</a>
<a class="sourceLine" id="cb7-2" title="2">f = F(rowIdx,colIdx,k);</a>
<a class="sourceLine" id="cb7-3" title="3">d = single(f) - m1;</a>
<a class="sourceLine" id="cb7-4" title="4">dk = d/n;</a>
<a class="sourceLine" id="cb7-5" title="5">dk2 = dk\^<span class="fl">2</span>;</a>
<a class="sourceLine" id="cb7-6" title="6">s = d\*dk\*(n-<span class="fl">1</span>);</a>
<a class="sourceLine" id="cb7-7" title="7"></a>
<a class="sourceLine" id="cb7-8" title="8"><span class="co">% UPDATE CENTRAL MOMENTS</span></a>
<a class="sourceLine" id="cb7-9" title="9">m1 = m1 + dk;</a>
<a class="sourceLine" id="cb7-10" title="10">m4 = m4 + s\*dk2\*(n.\^<span class="fl">2</span>-<span class="fl">3</span>\*n+<span class="fl">3</span>) + <span class="fl">6</span>\*dk2\*m2 - <span class="fl">4</span>\*dk\*m3;</a>
<a class="sourceLine" id="cb7-11" title="11">m3 = m3 + s\*dk\*(n-<span class="fl">2</span>) - <span class="fl">3</span>\*dk\*m2;</a>
<a class="sourceLine" id="cb7-12" title="12">m2 = m2 + s;</a>
<a class="sourceLine" id="cb7-13" title="13"><span class="co">% UPDATE MIN &amp; MAX</span></a>
<a class="sourceLine" id="cb7-14" title="14">fmin = min(fmin, f);</a>
<a class="sourceLine" id="cb7-15" title="15">fmax = max(fmax, f);</a></code></pre></div>
<p>Furthermore, the value used to update each central moment at each point in time can be used as a measure of change in the distribution of each pixel caused by the current pixel intensity, as explained next.</p>
<h3 id="non-stationarity-differential-moments">Non-Stationarity &amp; Differential Moments</h3>
<p>Stationary refers to the property of a signal such that its statistics do not vary over time, i.e. its distribution is stable. Neural signals tend to specifically <em>not</em> have this property, in contrast to other measurable components such as those contributed by physiologic noise (heart-rate, respirations, etc.). Thus, by analyzing the evolution of statistical measures calculated for each pixel as frames are added in sequence gives a highly sensitive indicator of neural activity. This is done using a routine analogous to that for updating central moments given above, except the values returned are not only the updated moment, but also the updating component – essentially the partial derivative with respect to time. This is illustrated below, including the normalization functions which convert the partial-moment values to their variance, skewness, and kurtosis analogues:</p>
<blockquote>
<p>% COMPUTE DIFFERENTIAL UPDATE TO CENTRAL MOMENTS</p>
<p>dm1 = dk;</p>
<p>m1 = m1 + dm1;</p>
<p>dm4 = s*dk2*(n^2-3*n+3) + 6*dk2*m2 - 4*dk*m3;</p>
<p>dm3 = s*dk*(n-2) - 3*dk*m2;</p>
<p>dm2 = s;</p>
<p>m2 = m2 + dm2;</p>
<p>% NORMALIZE BY VARIANCE &amp; SAMPLE NUMBER -&gt; CONVERSION TO dVar, dSkew, dKurt</p>
<p>dm2 = dm2/max(1,n-1);</p>
<p>dm3 = dm3*sqrt(max(1,n))/(m2^1.5);</p>
<p>dm4 = dm4*n/(m2^2);</p>
</blockquote>
<p>These functions run on images representing the image intensity, and also on images taken from sequential differences indicating the temporal derivative of image intensity. The combination of outputs from these operations indicate both when image intensities are significantly high relative to past distribution, and also when intensities are changing significantly faster than learned from their past distribution.</p>
<h3 id="surface-classification-peaks-edges-curvature">Surface Classification: Peaks, Edges, Curvature</h3>
<p>Edge-finding methods are employed for establishing boundaries between cells, and first and second-order gradients are used to compute local measures of curvature from an eigenvalue decomposition of the local Hessian matrix. I won’t go into detail, as the utility of these procedure in the most recent implementation has been lost, but nevertheless, the operation is optimized and ready to be plugged back in when further development calls for better accuracy informing cell-segmentation, or when a faster or more accurate motion-correction algorithm is called for.</p>
<h3 id="online-cell-segmentation-tracking">Online Cell Segmentation &amp; Tracking</h3>
<p>Cells are segmented by first running sequential statistics on the properties of identifiable regions on a pixel-wise basis. That is, as regions are identified in a method similar to that used offline in Aim 1, the region-properties are calculated (Centroid, Bounding-Box, etc.) and statistics for these properties are updated at each pixel covered by a proposed region. After sufficient evidence has gathered, Seeds are generated by finding the local peak of a seed-probability function that optimizes each pixel’s proximity to a region centroid, and distance from any boundary. Regions are grown from these seed regions, and registered in a hierarchy that allows for co-labeling of cellular and sub-cellular components. Newly identified regions occur as new seeds, where as seeds overlapping with old regions are used to identify sub-regions, or to track regions over time.</p>
<h3 id="signal-extraction-from-subcellular-compartments">Signal Extraction from Subcellular Compartments</h3>
<p>I also have functions for the extraction of normalized Pointwise-Mutual-Information (nPMI), which can operate on a pixel-to-pixel basis or on a region-to-pixel basis. This operation accumulates mutually informative changes in all pixels in the maximal bounding-box (e.g. 64x64 pixels) surrounding each identified regions centroid. The weights given by this function can take on values between -1 and 1, and can be used to inform any reduction operations to follow. Additionally, spatial moments can indicate the subcellular distribution of activity across the identified region. In this context, the first spatial moment M<sub>00</sub> indicates the mean signal intensity.</p>
<h3 id="user-interface-for-parameter-tuning">User Interface for Parameter Tuning</h3>
<p>Some system-objects also incorporate a user interface to aid in parameter selection for tuning. ## Compression: as a Tool, a Goal, as an Explanation</p>
<p>This section describes general compression algorithms as well as compression algorithms specifically tailored to application in video, accomplished by searching for both temporal and spatial redunancy.</p>
<hr />
<h2 id="data-management">Data Management</h2>
<p>Essential to the long-term success of any operation is an effective strategy for the management I’ve all data coming in and out of the system. Especially important is the insured facility for continued access and translation of data as systems evolve or as data is passed to other infrastructure. Strategies that meet the need of data generators and users often if it’s somewhere on a spectrum of strictly defined data structures that minimize errors but are strict and difficult to evolve, versus those that require verbose documentation which hinders performance but are adaptable.</p>
<p>The new structures that scientists typically use are on the flexible and self-describing Spectrum and, these include two files for large image formats, fits files, cifs, Matlab structure definitions, which are an implementation of hdf5 file structures. And similarly standard JavaScript object notation in parentheses Json structures. At the other end exist Raw structures compression codex and other processing procedures, and cannot be accessed reliably without thorough knowledge of the source code for napi, or extensive packing and guessing. In the middle exist and number of serialization formats such as Google protobuf format, Thrift, Avril, Captain T, ND growing number of other formats that try to offer flexibility in evolution over time as well as variable levels of self-description and performance. These formats are often to find in simple files and are used by code generators 2 produce computational e efficient code for serialization and D serialization without losing cross platform compatibility or risking rapid deprecation as the needs for the format evolve with the needs of the users. Types can be added to provide new functionality without damaging backwards compatibility. Additionally this model is off in accompanied by a translator herbal component to JavaScript object notation which makes the format ultimately universal in the case of a desperate need for compatibility when performance no longer matters</p>
<p>Tags: data, directory, format, programming, serialization ## Data Scaling</p>
<p>This section describes the reality of how data scales as more and more sensors are added. Typically we humans think linearly, but the ramifications of increasing something like sensor size is often exponential. Furthermore, many operations we perform have costs that scale exponentially, and in my humble opinion are not even worth attempting if any such procedure must applied continuously on a data=stream. ## Distributed Dataflow and Streaming</p>
<h3 id="language-is-matlab-the-best-tool-for-this-job">Language: Is MATLAB the best tool for this job?</h3>
<ul>
<li>Standard language in many engineering programs</li>
<li>Proprietary</li>
<li>Performance</li>
<li>Compatibility</li>
<li>Need for a “SandBox”</li>
<li>Lacks modularity</li>
<li></li>
</ul>
<h3 id="alternatives-languages">Alternatives Languages</h3>
<ul>
<li>Python</li>
<li>C/C++</li>
<li>Java/Scala</li>
<li>Javascript/Node</li>
<li>GO, Haskell</li>
</ul>
<h3 id="databases">Databases</h3>
<h3 id="big-data">Big Data</h3>
<ul>
<li>not exactly...</li>
<li>disparate simple queries across</li>
</ul>
<h3 id="computational-models">Computational Models</h3>
<ul>
<li>Dataflow Processing</li>
<li>Actors model</li>
<li>Petri Nets</li>
<li>Graph Processing - i.e. Tensorflow</li>
</ul>
<hr />
<h1 id="information-and-informativity">Information and Informativity</h1>
<p>This section presents a background in information theory in the context of behavior sensors and video streams. I further elaborate on what types of information neuroscientists and healthcare providers would hope to extract from these data, focusing on a larger picture of connecting the process of extraction directly the ultimate application.</p>
<p>Whereas previous attempts to process data focus largely on putting it in an intermediate state that is workable with current limitations on computation, visualization, and developer interaction with the data-streams, we now have an opportunity to skip intermediate states and develop with applications in mind.</p>
<p>Transition into compression by showing that previous attempts were essentially compression algorithms developed specifically for the data and preconceived notions about results, but that we can also apply very well developed general compression algorithms, with slight modifications to extract features such as localized bitrate that quantify (in an unbiased way) information content in an selectable region of a video-frame as it changes over time.</p>
<hr />
<h1 id="discussion-1">Discussion</h1>
<h2 id="points-to-address">Points to Address</h2>
<ul>
<li>“Biomimicry” in Visual Processing
<ul>
<li>This section describes how image and video processing in the computer relate to visual processing in the mammalian brain. The overall goal is to emphasize the advantage and importance of biomimetic development.</li>
<li>Neuromorphic computing
<ul>
<li>On chip image processing
<ul>
<li>relation “edge computing”</li>
</ul></li>
<li>event-based image sensors
<ul>
<li>“Artificial retina”</li>
<li>tittto</li>
<li>situations where event streams are advantageous</li>
<li>asynchronous</li>
<li>threshold-based</li>
</ul></li>
<li>convolutional neural-nets and deep learning for specific tasks
<ul>
<li>similarities</li>
<li>differences (substantial)</li>
</ul></li>
<li>genetic programming approaches to procedure optimization
<ul>
<li>minimize latency</li>
<li>maximize sensitivity and accuracy</li>
<li>minimize computational cost
<ul>
<li>minimize energy expenditure (metabolic efficiency)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Visual stream processing
<ul>
<li>feature extraction</li>
<li>motion estimation and compensation</li>
</ul></li>
<li>Asymmetry of learning/training time and desired inference computation time</li>
</ul></li>
<li>Common themes across projects
<ul>
<li>finding common standards
<ul>
<li>sticking to non-proprietary <em>open source</em> conventions
<ul>
<li>optical parts (lens threads)</li>
<li>file formats</li>
<li>software libraries</li>
<li>programming languages</li>
</ul></li>
<li>file transmission
<ul>
<li>web-based</li>
</ul></li>
</ul></li>
<li>Borrow from related sectors
<ul>
<li>better developed in many cases</li>
<li>surveillance</li>
<li>media streaming for web/entertainment</li>
<li>sports</li>
<li>astronomy/telescopes</li>
<li>medical imaging</li>
<li>automotive</li>
</ul></li>
<li></li>
</ul></li>
</ul>
<div id="fig:animal-tracking">
<figure>
<img src="img/animal-tracking/01raw.jpg" alt="{{subfigcaption(01raw.jpg)}}" id="fig:01raw.jpg" /><figcaption>{{subfigcaption(01raw.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/02black-and-white.jpg" alt="{{subfigcaption(02black-and-white.jpg)}}" id="fig:02black-and-white.jpg" /><figcaption>{{subfigcaption(02black-and-white.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/03twoframes.jpg" alt="{{subfigcaption(03twoframes.jpg)}}" id="fig:03twoframes.jpg" /><figcaption>{{subfigcaption(03twoframes.jpg)}}</figcaption>
</figure>
<figure>
<embed src="img/animal-tracking/04from_set_of_reference_images_after_processing.tif" id="fig:04from_set_of_reference_images_after_processing.tif" /><figcaption>{{subfigcaption(04from_set_of_reference_images_after_processing.tif)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/05tail_ID.jpg" alt="{{subfigcaption(05tail_ID.jpg)}}" id="fig:05tail_ID.jpg" /><figcaption>{{subfigcaption(05tail_ID.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/06mousedata1.jpg" alt="{{subfigcaption(06mousedata1.jpg)}}" id="fig:06mousedata1.jpg" /><figcaption>{{subfigcaption(06mousedata1.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/07mousedata1close.jpg" alt="{{subfigcaption(07mousedata1close.jpg)}}" id="fig:07mousedata1close.jpg" /><figcaption>{{subfigcaption(07mousedata1close.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/08mousedata2.jpg" alt="{{subfigcaption(08mousedata2.jpg)}}" id="fig:08mousedata2.jpg" /><figcaption>{{subfigcaption(08mousedata2.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/animal-tracking/09mousedata1fiberon1.jpg" alt="{{subfigcaption(09mousedata1fiberon1.jpg)}}" id="fig:09mousedata1fiberon1.jpg" /><figcaption>{{subfigcaption(09mousedata1fiberon1.jpg)}}</figcaption>
</figure>
<figure>
<embed src="img/animal-tracking/10rotation_viewer.tif" id="fig:10rotation_viewer.tif" /><figcaption>{{subfigcaption(10rotation_viewer.tif)}}</figcaption>
</figure>
<p>{{figcaption(animal-tracking)}}</p>
</div>
<div id="fig:behavior-box">
<figure>
<img src="img/behavior-box/01elife-fig2crop.jpg" alt="{{subfigcaption(01elife-fig2crop.jpg)}}" id="fig:01elife-fig2crop.jpg" /><figcaption>{{subfigcaption(01elife-fig2crop.jpg)}}</figcaption>
</figure>
<p>{{figcaption(behavior-box)}}</p>
</div>
<div id="fig:beveled-fiber-bundle">
<figure>
<img src="img/beveled-fiber-bundle/fullfield_bundle_overlay.png" alt="{{subfigcaption(fullfield_bundle_overlay.png)}}" id="fig:fullfield_bundle_overlay.png" /><figcaption>{{subfigcaption(fullfield_bundle_overlay.png)}}</figcaption>
</figure>
<p><img src="img/beveled-fiber-bundle/heightMap_Substack_(1-250).png" alt="{{subfigcaption(heightMap_Substack_(1-250).png)}}" />{#fig:heightMap_Substack_(1-250).png}</p>
<figure>
<img src="img/beveled-fiber-bundle/microspheres-1080um.png" alt="{{subfigcaption(microspheres-1080um.png)}}" id="fig:microspheres-1080um.png" /><figcaption>{{subfigcaption(microspheres-1080um.png)}}</figcaption>
</figure>
<figure>
<img src="img/beveled-fiber-bundle/microspheres-1102um.png" alt="{{subfigcaption(microspheres-1102um.png)}}" id="fig:microspheres-1102um.png" /><figcaption>{{subfigcaption(microspheres-1102um.png)}}</figcaption>
</figure>
<figure>
<img src="img/beveled-fiber-bundle/microspheres-1351um.png" alt="{{subfigcaption(microspheres-1351um.png)}}" id="fig:microspheres-1351um.png" /><figcaption>{{subfigcaption(microspheres-1351um.png)}}</figcaption>
</figure>
<p><img src="img/beveled-fiber-bundle/raw_Substack_(1-250).png" alt="{{subfigcaption(raw_Substack_(1-250).png)}}" />{#fig:raw_Substack_(1-250).png}</p>
<p>{{figcaption(beveled-fiber-bundle)}}</p>
</div>
<div id="fig:bin">
<figure>
<embed src="img/bin/fix-extension-to-lowercase.sh" id="fig:fix-extension-to-lowercase.sh" /><figcaption>{{subfigcaption(fix-extension-to-lowercase.sh)}}</figcaption>
</figure>
<figure>
<embed src="img/bin/replace-spaces-in-filenames-with-underscore.sh" id="fig:replace-spaces-in-filenames-with-underscore.sh" /><figcaption>{{subfigcaption(replace-spaces-in-filenames-with-underscore.sh)}}</figcaption>
</figure>
<p>{{figcaption(bin)}}</p>
</div>
<div id="fig:FluoPro">
<figure>
<img src="img/FluoPro/cap1.png" alt="{{subfigcaption(cap1.png)}}" id="fig:cap1.png" /><figcaption>{{subfigcaption(cap1.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/cap2.png" alt="{{subfigcaption(cap2.png)}}" id="fig:cap2.png" /><figcaption>{{subfigcaption(cap2.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/cap3.png" alt="{{subfigcaption(cap3.png)}}" id="fig:cap3.png" /><figcaption>{{subfigcaption(cap3.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless_2_.png" alt="{{subfigcaption(diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless_2_.png)}}" id="fig:diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless_2_.png" /><figcaption>{{subfigcaption(diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless_2_.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless.png" alt="{{subfigcaption(diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless.png)}}" id="fig:diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless.png" /><figcaption>{{subfigcaption(diffdemon_motion-corrected_frame_low_correlation_but_fixed_nonetheless.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/diffdemon_snip_1.png" alt="{{subfigcaption(diffdemon_snip_1.png)}}" id="fig:diffdemon_snip_1.png" /><figcaption>{{subfigcaption(diffdemon_snip_1.png)}}</figcaption>
</figure>
<p><img src="img/FluoPro/Foreground_Detection(1).png" alt="{{subfigcaption(Foreground_Detection(1).png)}}" />{#fig:Foreground_Detection(1).png}</p>
<p><img src="img/FluoPro/Foreground_Detection(2).png" alt="{{subfigcaption(Foreground_Detection(2).png)}}" />{#fig:Foreground_Detection(2).png}</p>
<figure>
<img src="img/FluoPro/homomorphic_filter_before-after_in_falsecolor.png" alt="{{subfigcaption(homomorphic_filter_before-after_in_falsecolor.png)}}" id="fig:homomorphic_filter_before-after_in_falsecolor.png" /><figcaption>{{subfigcaption(homomorphic_filter_before-after_in_falsecolor.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/homomorphic_filter_before-after_sidebyside.png" alt="{{subfigcaption(homomorphic_filter_before-after_sidebyside.png)}}" id="fig:homomorphic_filter_before-after_sidebyside.png" /><figcaption>{{subfigcaption(homomorphic_filter_before-after_sidebyside.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/motion_correction_sample.png" alt="{{subfigcaption(motion_correction_sample.png)}}" id="fig:motion_correction_sample.png" /><figcaption>{{subfigcaption(motion_correction_sample.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/motion_correction_unstable_1.png" alt="{{subfigcaption(motion_correction_unstable_1.png)}}" id="fig:motion_correction_unstable_1.png" /><figcaption>{{subfigcaption(motion_correction_unstable_1.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/RoiGenerator_snip1.png" alt="{{subfigcaption(RoiGenerator_snip1.png)}}" id="fig:RoiGenerator_snip1.png" /><figcaption>{{subfigcaption(RoiGenerator_snip1.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/RoiGenerator_snip2.png" alt="{{subfigcaption(RoiGenerator_snip2.png)}}" id="fig:RoiGenerator_snip2.png" /><figcaption>{{subfigcaption(RoiGenerator_snip2.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/RoiGenerator_snip3.png" alt="{{subfigcaption(RoiGenerator_snip3.png)}}" id="fig:RoiGenerator_snip3.png" /><figcaption>{{subfigcaption(RoiGenerator_snip3.png)}}</figcaption>
</figure>
<figure>
<img src="img/FluoPro/RoiGenerator_snip4.png" alt="{{subfigcaption(RoiGenerator_snip4.png)}}" id="fig:RoiGenerator_snip4.png" /><figcaption>{{subfigcaption(RoiGenerator_snip4.png)}}</figcaption>
</figure>
<p><img src="img/FluoPro/Semi-Processed_Image_(BLUE),_Overlayed_with_Absolute_X_(RED)_and_Y_(GREEN)_Gradients_Ali26.png" alt="{{subfigcaption(Semi-Processed_Image_(BLUE),Overlayed_with_Absolute_X(RED)and_Y(GREEN)_Gradients_Ali26.png)}}" />{#fig:Semi-Processed_Image_(BLUE),<em>Overlayed_with_Absolute_X</em>(RED)<em>and_Y</em>(GREEN)_Gradients_Ali26.png}</p>
<p>{{figcaption(FluoPro)}}</p>
</div>
<div id="fig:headplate-holder">
<figure>
<img src="img/headplate-holder/drawing-v2.png" alt="{{subfigcaption(drawing-v2.png)}}" id="fig:drawing-v2.png" /><figcaption>{{subfigcaption(drawing-v2.png)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/HeadplateHolder2_V2.png" alt="{{subfigcaption(HeadplateHolder2_V2.png)}}" id="fig:HeadplateHolder2_V2.png" /><figcaption>{{subfigcaption(HeadplateHolder2_V2.png)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/HeadplateHolder_V2.png" alt="{{subfigcaption(HeadplateHolder_V2.png)}}" id="fig:HeadplateHolder_V2.png" /><figcaption>{{subfigcaption(HeadplateHolder_V2.png)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/photo-bottom.jpg" alt="{{subfigcaption(photo-bottom.jpg)}}" id="fig:photo-bottom.jpg" /><figcaption>{{subfigcaption(photo-bottom.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/photo-front.jpg" alt="{{subfigcaption(photo-front.jpg)}}" id="fig:photo-front.jpg" /><figcaption>{{subfigcaption(photo-front.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/photo.jpg" alt="{{subfigcaption(photo.jpg)}}" id="fig:photo.jpg" /><figcaption>{{subfigcaption(photo.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/headplate-holder/photo-top.jpg" alt="{{subfigcaption(photo-top.jpg)}}" id="fig:photo-top.jpg" /><figcaption>{{subfigcaption(photo-top.jpg)}}</figcaption>
</figure>
<p>{{figcaption(headplate-holder)}}</p>
</div>
<div id="fig:microscope">
<figure>
<img src="img/microscope/50-14-nai-KEN_0053-big.jpg" alt="{{subfigcaption(50-14-nai-KEN_0053-big.jpg)}}" id="fig:50-14-nai-KEN_0053-big.jpg" /><figcaption>{{subfigcaption(50-14-nai-KEN_0053-big.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/7564_CFH2-F_SGL.jpg" alt="{{subfigcaption(7564_CFH2-F_SGL.jpg)}}" id="fig:7564_CFH2-F_SGL.jpg" /><figcaption>{{subfigcaption(7564_CFH2-F_SGL.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup1.jpg" alt="{{subfigcaption(setup1.jpg)}}" id="fig:setup1.jpg" /><figcaption>{{subfigcaption(setup1.jpg)}}</figcaption>
</figure>
<figure>
<embed src="img/microscope/setup1-Striatum_Figure1.tif" id="fig:setup1-Striatum_Figure1.tif" /><figcaption>{{subfigcaption(setup1-Striatum_Figure1.tif)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup2.jpg" alt="{{subfigcaption(setup2.jpg)}}" id="fig:setup2.jpg" /><figcaption>{{subfigcaption(setup2.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup3-closeup.jpg" alt="{{subfigcaption(setup3-closeup.jpg)}}" id="fig:setup3-closeup.jpg" /><figcaption>{{subfigcaption(setup3-closeup.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup3-front.jpg" alt="{{subfigcaption(setup3-front.jpg)}}" id="fig:setup3-front.jpg" /><figcaption>{{subfigcaption(setup3-front.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup3-side.jpg" alt="{{subfigcaption(setup3-side.jpg)}}" id="fig:setup3-side.jpg" /><figcaption>{{subfigcaption(setup3-side.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup4-closeup.jpg" alt="{{subfigcaption(setup4-closeup.jpg)}}" id="fig:setup4-closeup.jpg" /><figcaption>{{subfigcaption(setup4-closeup.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup4-front.jpg" alt="{{subfigcaption(setup4-front.jpg)}}" id="fig:setup4-front.jpg" /><figcaption>{{subfigcaption(setup4-front.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/setup4-side.jpg" alt="{{subfigcaption(setup4-side.jpg)}}" id="fig:setup4-side.jpg" /><figcaption>{{subfigcaption(setup4-side.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/microscope/widefield_microscope_diagram.png" alt="{{subfigcaption(widefield_microscope_diagram.png)}}" id="fig:widefield_microscope_diagram.png" /><figcaption>{{subfigcaption(widefield_microscope_diagram.png)}}</figcaption>
</figure>
<p>{{figcaption(microscope)}}</p>
</div>
<div id="fig:monkey-related">
<figure>
<img src="img/monkey-related/brain-cranial-window.jpg" alt="{{subfigcaption(brain-cranial-window.jpg)}}" id="fig:brain-cranial-window.jpg" /><figcaption>{{subfigcaption(brain-cranial-window.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/monkey-related/withLight.jpg" alt="{{subfigcaption(withLight.jpg)}}" id="fig:withLight.jpg" /><figcaption>{{subfigcaption(withLight.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/monkey-related/withoutLight.jpg" alt="{{subfigcaption(withoutLight.jpg)}}" id="fig:withoutLight.jpg" /><figcaption>{{subfigcaption(withoutLight.jpg)}}</figcaption>
</figure>
<p>{{figcaption(monkey-related)}}</p>
</div>
<div id="fig:neuromodulation-implant">
<figure>
<embed src="img/neuromodulation-implant/bottom-thorough.tif" id="fig:bottom-thorough.tif" /><figcaption>{{subfigcaption(bottom-thorough.tif)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/bottom.tif" id="fig:bottom.tif" /><figcaption>{{subfigcaption(bottom.tif)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/front-thorough.tif" id="fig:front-thorough.tif" /><figcaption>{{subfigcaption(front-thorough.tif)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/front.tif" id="fig:front.tif" /><figcaption>{{subfigcaption(front.tif)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/side-thorough.tif" id="fig:side-thorough.tif" /><figcaption>{{subfigcaption(side-thorough.tif)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/side.tif" id="fig:side.tif" /><figcaption>{{subfigcaption(side.tif)}}</figcaption>
</figure>
<figure>
<img src="img/neuromodulation-implant/top-thorough.png" alt="{{subfigcaption(top-thorough.png)}}" id="fig:top-thorough.png" /><figcaption>{{subfigcaption(top-thorough.png)}}</figcaption>
</figure>
<figure>
<embed src="img/neuromodulation-implant/top.tif" id="fig:top.tif" /><figcaption>{{subfigcaption(top.tif)}}</figcaption>
</figure>
<p>{{figcaption(neuromodulation-implant)}}</p>
</div>
<div id="fig:neuromodulation-pcb">
<figure>
<img src="img/neuromodulation-pcb/01pcb_bottom.png" alt="{{subfigcaption(01pcb_bottom.png)}}" id="fig:01pcb_bottom.png" /><figcaption>{{subfigcaption(01pcb_bottom.png)}}</figcaption>
</figure>
<figure>
<img src="img/neuromodulation-pcb/02pcb_top.png" alt="{{subfigcaption(02pcb_top.png)}}" id="fig:02pcb_top.png" /><figcaption>{{subfigcaption(02pcb_top.png)}}</figcaption>
</figure>
<figure>
<img src="img/neuromodulation-pcb/03pcb_combined.png" alt="{{subfigcaption(03pcb_combined.png)}}" id="fig:03pcb_combined.png" /><figcaption>{{subfigcaption(03pcb_combined.png)}}</figcaption>
</figure>
<figure>
<img src="img/neuromodulation-pcb/04pcb_bottom-photo.png" alt="{{subfigcaption(04pcb_bottom-photo.png)}}" id="fig:04pcb_bottom-photo.png" /><figcaption>{{subfigcaption(04pcb_bottom-photo.png)}}</figcaption>
</figure>
<figure>
<img src="img/neuromodulation-pcb/05pcb-top-photo.jpg" alt="{{subfigcaption(05pcb-top-photo.jpg)}}" id="fig:05pcb-top-photo.jpg" /><figcaption>{{subfigcaption(05pcb-top-photo.jpg)}}</figcaption>
</figure>
<p>{{figcaption(neuromodulation-pcb)}}</p>
</div>
<div id="fig:Pipeline">
<figure>
<img src="img/Pipeline/pstep_illumcorrect.png" alt="{{subfigcaption(pstep_illumcorrect.png)}}" id="fig:pstep_illumcorrect.png" /><figcaption>{{subfigcaption(pstep_illumcorrect.png)}}</figcaption>
</figure>
<figure>
<img src="img/Pipeline/pstep_normalizedbasesub.png" alt="{{subfigcaption(pstep_normalizedbasesub.png)}}" id="fig:pstep_normalizedbasesub.png" /><figcaption>{{subfigcaption(pstep_normalizedbasesub.png)}}</figcaption>
</figure>
<figure>
<img src="img/Pipeline/pstep_raw.png" alt="{{subfigcaption(pstep_raw.png)}}" id="fig:pstep_raw.png" /><figcaption>{{subfigcaption(pstep_raw.png)}}</figcaption>
</figure>
<figure>
<img src="img/Pipeline/pstep_segmentedALLroi.png" alt="{{subfigcaption(pstep_segmentedALLroi.png)}}" id="fig:pstep_segmentedALLroi.png" /><figcaption>{{subfigcaption(pstep_segmentedALLroi.png)}}</figcaption>
</figure>
<figure>
<img src="img/Pipeline/pstep_spatialfilter.png" alt="{{subfigcaption(pstep_spatialfilter.png)}}" id="fig:pstep_spatialfilter.png" /><figcaption>{{subfigcaption(pstep_spatialfilter.png)}}</figcaption>
</figure>
<p>{{figcaption(Pipeline)}}</p>
</div>
<div id="fig:Scicadelic">
<p><img src="img/Scicadelic/Absolute_Minimum_Pixel_Value_(RED)_vs._Minimum_Estimate_(BLUE).jpg" alt="{{subfigcaption(Absolute_Minimum_Pixel_Value_(RED)vs._Minimum_Estimate(BLUE).jpg)}}" />{#fig:Absolute_Minimum_Pixel_Value_(RED)<em>vs._Minimum_Estimate</em>(BLUE).jpg}</p>
<p><img src="img/Scicadelic/Average_Pixel_Displacement_over_Time_(from_MOTION_CORRECTION).png" alt="{{subfigcaption(Average_Pixel_Displacement_over_Time_(from_MOTION_CORRECTION).png)}}" />{#fig:Average_Pixel_Displacement_over_Time_(from_MOTION_CORRECTION).png}</p>
<p><img src="img/Scicadelic/Cell_and_Vessel_(composite)_Label_Initialization_(first_8_frames).jpg" alt="{{subfigcaption(Cell_and_Vessel_(composite)Label_Initialization(first_8_frames).jpg)}}" />{#fig:Cell_and_Vessel_(composite)<em>Label_Initialization</em>(first_8_frames).jpg}</p>
<figure>
<img src="img/Scicadelic/differential_kurtosis1.jpg" alt="{{subfigcaption(differential_kurtosis1.jpg)}}" id="fig:differential_kurtosis1.jpg" /><figcaption>{{subfigcaption(differential_kurtosis1.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/differential_kurtosis2.jpg" alt="{{subfigcaption(differential_kurtosis2.jpg)}}" id="fig:differential_kurtosis2.jpg" /><figcaption>{{subfigcaption(differential_kurtosis2.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/differential_kurtosis3.jpg" alt="{{subfigcaption(differential_kurtosis3.jpg)}}" id="fig:differential_kurtosis3.jpg" /><figcaption>{{subfigcaption(differential_kurtosis3.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/differential_skewness.2jpg.jpg" alt="{{subfigcaption(differential_skewness.2jpg.jpg)}}" id="fig:differential_skewness.2jpg.jpg" /><figcaption>{{subfigcaption(differential_skewness.2jpg.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/differential_skewness.jpg" alt="{{subfigcaption(differential_skewness.jpg)}}" id="fig:differential_skewness.jpg" /><figcaption>{{subfigcaption(differential_skewness.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/F_segmented_using_2-pUnstable.jpg" alt="{{subfigcaption(F_segmented_using_2-pUnstable.jpg)}}" id="fig:F_segmented_using_2-pUnstable.jpg" /><figcaption>{{subfigcaption(F_segmented_using_2-pUnstable.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1252AM.jpg" alt="{{subfigcaption(Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1252AM.jpg)}}" id="fig:Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1252AM.jpg" /><figcaption>{{subfigcaption(Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1252AM.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1256AM.jpg" alt="{{subfigcaption(Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1256AM.jpg)}}" id="fig:Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1256AM.jpg" /><figcaption>{{subfigcaption(Hybrid-Median-Filter_vs._MATLAB_Built-In_medfilt2_2015Aug12_1256AM.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/log1p_of_stat-M2_after_2nd_Chunk_2015Aug12_139AM.jpg" alt="{{subfigcaption(log1p_of_stat-M2_after_2nd_Chunk_2015Aug12_139AM.jpg)}}" id="fig:log1p_of_stat-M2_after_2nd_Chunk_2015Aug12_139AM.jpg" /><figcaption>{{subfigcaption(log1p_of_stat-M2_after_2nd_Chunk_2015Aug12_139AM.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/Motion-induced_BLURRING_of_ROI_with_max-min-mean.png" alt="{{subfigcaption(Motion-induced_BLURRING_of_ROI_with_max-min-mean.png)}}" id="fig:Motion-induced_BLURRING_of_ROI_with_max-min-mean.png" /><figcaption>{{subfigcaption(Motion-induced_BLURRING_of_ROI_with_max-min-mean.png)}}</figcaption>
</figure>
<p><img src="img/Scicadelic/pUnstable_(mean_gradients_of_fSplit).jpg" alt="{{subfigcaption(pUnstable_(mean_gradients_of_fSplit).jpg)}}" />{#fig:pUnstable_(mean_gradients_of_fSplit).jpg}</p>
<figure>
<img src="img/Scicadelic/statistics_of_128_frames_contrast_enhanced.jpg" alt="{{subfigcaption(statistics_of_128_frames_contrast_enhanced.jpg)}}" id="fig:statistics_of_128_frames_contrast_enhanced.jpg" /><figcaption>{{subfigcaption(statistics_of_128_frames_contrast_enhanced.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/Scicadelic/stat-M3_after_2nd_Chunk_2015Aug12_140AM.jpg" alt="{{subfigcaption(stat-M3_after_2nd_Chunk_2015Aug12_140AM.jpg)}}" id="fig:stat-M3_after_2nd_Chunk_2015Aug12_140AM.jpg" /><figcaption>{{subfigcaption(stat-M3_after_2nd_Chunk_2015Aug12_140AM.jpg)}}</figcaption>
</figure>
<p>{{figcaption(Scicadelic)}}</p>
</div>
<div id="fig:sfn-poster">
<figure>
<img src="img/sfn-poster/hc-image306.png" alt="{{subfigcaption(hc-image306.png)}}" id="fig:hc-image306.png" /><figcaption>{{subfigcaption(hc-image306.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image332.png" alt="{{subfigcaption(hc-image332.png)}}" id="fig:hc-image332.png" /><figcaption>{{subfigcaption(hc-image332.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image3341.png" alt="{{subfigcaption(hc-image3341.png)}}" id="fig:hc-image3341.png" /><figcaption>{{subfigcaption(hc-image3341.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image336.png" alt="{{subfigcaption(hc-image336.png)}}" id="fig:hc-image336.png" /><figcaption>{{subfigcaption(hc-image336.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image342.png" alt="{{subfigcaption(hc-image342.png)}}" id="fig:hc-image342.png" /><figcaption>{{subfigcaption(hc-image342.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image345.png" alt="{{subfigcaption(hc-image345.png)}}" id="fig:hc-image345.png" /><figcaption>{{subfigcaption(hc-image345.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image348.png" alt="{{subfigcaption(hc-image348.png)}}" id="fig:hc-image348.png" /><figcaption>{{subfigcaption(hc-image348.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image363.png" alt="{{subfigcaption(hc-image363.png)}}" id="fig:hc-image363.png" /><figcaption>{{subfigcaption(hc-image363.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image366.png" alt="{{subfigcaption(hc-image366.png)}}" id="fig:hc-image366.png" /><figcaption>{{subfigcaption(hc-image366.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/hc-image375.png" alt="{{subfigcaption(hc-image375.png)}}" id="fig:hc-image375.png" /><figcaption>{{subfigcaption(hc-image375.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image338.png" alt="{{subfigcaption(image338.png)}}" id="fig:image338.png" /><figcaption>{{subfigcaption(image338.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image341.png" alt="{{subfigcaption(image341.png)}}" id="fig:image341.png" /><figcaption>{{subfigcaption(image341.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image378.png" alt="{{subfigcaption(image378.png)}}" id="fig:image378.png" /><figcaption>{{subfigcaption(image378.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image381.png" alt="{{subfigcaption(image381.png)}}" id="fig:image381.png" /><figcaption>{{subfigcaption(image381.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image3851.png" alt="{{subfigcaption(image3851.png)}}" id="fig:image3851.png" /><figcaption>{{subfigcaption(image3851.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image387.png" alt="{{subfigcaption(image387.png)}}" id="fig:image387.png" /><figcaption>{{subfigcaption(image387.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image3881.png" alt="{{subfigcaption(image3881.png)}}" id="fig:image3881.png" /><figcaption>{{subfigcaption(image3881.png)}}</figcaption>
</figure>
<figure>
<img src="img/sfn-poster/image390.png" alt="{{subfigcaption(image390.png)}}" id="fig:image390.png" /><figcaption>{{subfigcaption(image390.png)}}</figcaption>
</figure>
<p>{{figcaption(sfn-poster)}}</p>
</div>
<div id="fig:spherical-treadmill-extended">
<figure>
<img src="img/spherical-treadmill-extended/lightson_withsusie.png" alt="{{subfigcaption(lightson_withsusie.png)}}" id="fig:lightson_withsusie.png" /><figcaption>{{subfigcaption(lightson_withsusie.png)}}</figcaption>
</figure>
<p>{{figcaption(spherical-treadmill-extended)}}</p>
</div>
<div id="fig:spherical-treadmill-motion-sensors">
<figure>
<img src="img/spherical-treadmill-motion-sensors/01-motion-sensors-installed.jpg" alt="{{subfigcaption(01-motion-sensors-installed.jpg)}}" id="fig:01-motion-sensors-installed.jpg" /><figcaption>{{subfigcaption(01-motion-sensors-installed.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-motion-sensors/02-motion-sensors.jpg" alt="{{subfigcaption(02-motion-sensors.jpg)}}" id="fig:02-motion-sensors.jpg" /><figcaption>{{subfigcaption(02-motion-sensors.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-motion-sensors/02-teensy.jpg" alt="{{subfigcaption(02-teensy.jpg)}}" id="fig:02-teensy.jpg" /><figcaption>{{subfigcaption(02-teensy.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-motion-sensors/Striatum_Figure2.png" alt="{{subfigcaption(Striatum_Figure2.png)}}" id="fig:Striatum_Figure2.png" /><figcaption>{{subfigcaption(Striatum_Figure2.png)}}</figcaption>
</figure>
<p>{{figcaption(spherical-treadmill-motion-sensors)}}</p>
</div>
<div id="fig:spherical-treadmill-VR">
<figure>
<img src="img/spherical-treadmill-VR/01-treadmill-mouse-running.jpg" alt="{{subfigcaption(01-treadmill-mouse-running.jpg)}}" id="fig:01-treadmill-mouse-running.jpg" /><figcaption>{{subfigcaption(01-treadmill-mouse-running.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-VR/02-treadmill-front.jpg" alt="{{subfigcaption(02-treadmill-front.jpg)}}" id="fig:02-treadmill-front.jpg" /><figcaption>{{subfigcaption(02-treadmill-front.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-VR/03-treadmill-top.jpg" alt="{{subfigcaption(03-treadmill-top.jpg)}}" id="fig:03-treadmill-top.jpg" /><figcaption>{{subfigcaption(03-treadmill-top.jpg)}}</figcaption>
</figure>
<figure>
<embed src="img/spherical-treadmill-VR/04-VR.tif" id="fig:04-VR.tif" /><figcaption>{{subfigcaption(04-VR.tif)}}</figcaption>
</figure>
<p>{{figcaption(spherical-treadmill-VR)}}</p>
</div>
<div id="fig:spherical-treadmill-water-delivery">
<figure>
<img src="img/spherical-treadmill-water-delivery/01-water-port.jpg" alt="{{subfigcaption(01-water-port.jpg)}}" id="fig:01-water-port.jpg" /><figcaption>{{subfigcaption(01-water-port.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-water-delivery/02-water-delivery.jpg" alt="{{subfigcaption(02-water-delivery.jpg)}}" id="fig:02-water-delivery.jpg" /><figcaption>{{subfigcaption(02-water-delivery.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/spherical-treadmill-water-delivery/03-water-delivery-zoom.jpg" alt="{{subfigcaption(03-water-delivery-zoom.jpg)}}" id="fig:03-water-delivery-zoom.jpg" /><figcaption>{{subfigcaption(03-water-delivery-zoom.jpg)}}</figcaption>
</figure>
<p>{{figcaption(spherical-treadmill-water-delivery)}}</p>
</div>
<div id="fig:SW-2015-09-image-processing-refinement">
<figure>
<img src="img/SW-2015-09-image-processing-refinement/After_Layer_EDGE_Refinement_2015Sep02_201AM.jpg" alt="{{subfigcaption(After_Layer_EDGE_Refinement_2015Sep02_201AM.jpg)}}" id="fig:After_Layer_EDGE_Refinement_2015Sep02_201AM.jpg" /><figcaption>{{subfigcaption(After_Layer_EDGE_Refinement_2015Sep02_201AM.jpg)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2015-09-image-processing-refinement/After_Layer_PEAK_Refinement_2015Sep02_201AM.jpg" alt="{{subfigcaption(After_Layer_PEAK_Refinement_2015Sep02_201AM.jpg)}}" id="fig:After_Layer_PEAK_Refinement_2015Sep02_201AM.jpg" /><figcaption>{{subfigcaption(After_Layer_PEAK_Refinement_2015Sep02_201AM.jpg)}}</figcaption>
</figure>
<p><img src="img/SW-2015-09-image-processing-refinement/After_Layer_PEAK_Refinement_(squaring)_2015Sep02_210AM.jpg" alt="{{subfigcaption(After_Layer_PEAK_Refinement_(squaring)_2015Sep02_210AM.jpg)}}" />{#fig:After_Layer_PEAK_Refinement_(squaring)_2015Sep02_210AM.jpg}</p>
<p><img src="img/SW-2015-09-image-processing-refinement/After_Layer_PEAK_Refinement_(using_sign)_2015Sep02_209AM.jpg" alt="{{subfigcaption(After_Layer_PEAK_Refinement_(using_sign)_2015Sep02_209AM.jpg)}}" />{#fig:After_Layer_PEAK_Refinement_(using_sign)_2015Sep02_209AM.jpg}</p>
<figure>
<img src="img/SW-2015-09-image-processing-refinement/Before_Layer_Refinement_2015Sep02_201AM.jpg" alt="{{subfigcaption(Before_Layer_Refinement_2015Sep02_201AM.jpg)}}" id="fig:Before_Layer_Refinement_2015Sep02_201AM.jpg" /><figcaption>{{subfigcaption(Before_Layer_Refinement_2015Sep02_201AM.jpg)}}</figcaption>
</figure>
<p>{{figcaption(SW-2015-09-image-processing-refinement)}}</p>
</div>
<div id="fig:SW-2015-11-batch05-image-processing">
<figure>
<img src="img/SW-2015-11-batch05-image-processing/spatially-adaptive-vs-temporally-adaptive-filter-frame1208.png" alt="{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1208.png)}}" id="fig:spatially-adaptive-vs-temporally-adaptive-filter-frame1208.png" /><figcaption>{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1208.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2015-11-batch05-image-processing/spatially-adaptive-vs-temporally-adaptive-filter-frame1213.png" alt="{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1213.png)}}" id="fig:spatially-adaptive-vs-temporally-adaptive-filter-frame1213.png" /><figcaption>{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1213.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2015-11-batch05-image-processing/spatially-adaptive-vs-temporally-adaptive-filter-frame1218.png" alt="{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1218.png)}}" id="fig:spatially-adaptive-vs-temporally-adaptive-filter-frame1218.png" /><figcaption>{{subfigcaption(spatially-adaptive-vs-temporally-adaptive-filter-frame1218.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-2015-11-batch05-image-processing)}}</p>
</div>
<div id="fig:SW-2016-01-batch12-image-processing">
<figure>
<img src="img/SW-2016-01-batch12-image-processing/distance-from-bounding-box-corners.png" alt="{{subfigcaption(distance-from-bounding-box-corners.png)}}" id="fig:distance-from-bounding-box-corners.png" /><figcaption>{{subfigcaption(distance-from-bounding-box-corners.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/max-area-log1pabs.png" alt="{{subfigcaption(max-area-log1pabs.png)}}" id="fig:max-area-log1pabs.png" /><figcaption>{{subfigcaption(max-area-log1pabs.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/max-area-sqrttabs.png" alt="{{subfigcaption(max-area-sqrttabs.png)}}" id="fig:max-area-sqrttabs.png" /><figcaption>{{subfigcaption(max-area-sqrttabs.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/max-relative-centroid-distance.png" alt="{{subfigcaption(max-relative-centroid-distance.png)}}" id="fig:max-relative-centroid-distance.png" /><figcaption>{{subfigcaption(max-relative-centroid-distance.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/max-relative-centroid.png" alt="{{subfigcaption(max-relative-centroid.png)}}" id="fig:max-relative-centroid.png" /><figcaption>{{subfigcaption(max-relative-centroid.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/mean-area.png" alt="{{subfigcaption(mean-area.png)}}" id="fig:mean-area.png" /><figcaption>{{subfigcaption(mean-area.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/mean-centroid-distance-meanBoundaryDistance-meanArea.png" alt="{{subfigcaption(mean-centroid-distance-meanBoundaryDistance-meanArea.png)}}" id="fig:mean-centroid-distance-meanBoundaryDistance-meanArea.png" /><figcaption>{{subfigcaption(mean-centroid-distance-meanBoundaryDistance-meanArea.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/mean-centroid-distance-real-and-imaginary.png" alt="{{subfigcaption(mean-centroid-distance-real-and-imaginary.png)}}" id="fig:mean-centroid-distance-real-and-imaginary.png" /><figcaption>{{subfigcaption(mean-centroid-distance-real-and-imaginary.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/min-aera.png" alt="{{subfigcaption(min-aera.png)}}" id="fig:min-aera.png" /><figcaption>{{subfigcaption(min-aera.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/min-area-log1pads-inf-removed.png" alt="{{subfigcaption(min-area-log1pads-inf-removed.png)}}" id="fig:min-area-log1pads-inf-removed.png" /><figcaption>{{subfigcaption(min-area-log1pads-inf-removed.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/min-extent-both-distance-corners.png" alt="{{subfigcaption(min-extent-both-distance-corners.png)}}" id="fig:min-extent-both-distance-corners.png" /><figcaption>{{subfigcaption(min-extent-both-distance-corners.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/min-extent-distance-lowerRight.png" alt="{{subfigcaption(min-extent-distance-lowerRight.png)}}" id="fig:min-extent-distance-lowerRight.png" /><figcaption>{{subfigcaption(min-extent-distance-lowerRight.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2016-01-batch12-image-processing/min-extent-distance-upperLeft.png" alt="{{subfigcaption(min-extent-distance-upperLeft.png)}}" id="fig:min-extent-distance-upperLeft.png" /><figcaption>{{subfigcaption(min-extent-distance-upperLeft.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-2016-01-batch12-image-processing)}}</p>
</div>
<div id="fig:SW-2017-08-roi05">
<figure>
<img src="img/SW-2017-08-roi05/Screenshot_20170815001718.png" alt="{{subfigcaption(Screenshot_20170815001718.png)}}" id="fig:Screenshot_20170815001718.png" /><figcaption>{{subfigcaption(Screenshot_20170815001718.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2017-08-roi05/Screenshot_20170816233739.png" alt="{{subfigcaption(Screenshot_20170816233739.png)}}" id="fig:Screenshot_20170816233739.png" /><figcaption>{{subfigcaption(Screenshot_20170816233739.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2017-08-roi05/Screenshot_20170816234331.png" alt="{{subfigcaption(Screenshot_20170816234331.png)}}" id="fig:Screenshot_20170816234331.png" /><figcaption>{{subfigcaption(Screenshot_20170816234331.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-2017-08-roi05)}}</p>
</div>
<div id="fig:SW-2018-08-batch02-image-processing">
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-04_15-25-55.png" alt="{{subfigcaption(Screenshot_from_2018-08-04_15-25-55.png)}}" id="fig:Screenshot_from_2018-08-04_15-25-55.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-04_15-25-55.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-04_15-26-30.png" alt="{{subfigcaption(Screenshot_from_2018-08-04_15-26-30.png)}}" id="fig:Screenshot_from_2018-08-04_15-26-30.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-04_15-26-30.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-04_17-10-24.png" alt="{{subfigcaption(Screenshot_from_2018-08-04_17-10-24.png)}}" id="fig:Screenshot_from_2018-08-04_17-10-24.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-04_17-10-24.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-04_17-48-26.png" alt="{{subfigcaption(Screenshot_from_2018-08-04_17-48-26.png)}}" id="fig:Screenshot_from_2018-08-04_17-48-26.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-04_17-48-26.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-06_04-07-50.png" alt="{{subfigcaption(Screenshot_from_2018-08-06_04-07-50.png)}}" id="fig:Screenshot_from_2018-08-06_04-07-50.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-06_04-07-50.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-2018-08-batch02-image-processing/Screenshot_from_2018-08-06_04-32-01.png" alt="{{subfigcaption(Screenshot_from_2018-08-06_04-32-01.png)}}" id="fig:Screenshot_from_2018-08-06_04-32-01.png" /><figcaption>{{subfigcaption(Screenshot_from_2018-08-06_04-32-01.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-2018-08-batch02-image-processing)}}</p>
</div>
<div id="fig:SW-sequence-2015-07-seq02">
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-10.png" alt="{{subfigcaption(frame-10.png)}}" id="fig:frame-10.png" /><figcaption>{{subfigcaption(frame-10.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-13.png" alt="{{subfigcaption(frame-13.png)}}" id="fig:frame-13.png" /><figcaption>{{subfigcaption(frame-13.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-26.png" alt="{{subfigcaption(frame-26.png)}}" id="fig:frame-26.png" /><figcaption>{{subfigcaption(frame-26.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-42.png" alt="{{subfigcaption(frame-42.png)}}" id="fig:frame-42.png" /><figcaption>{{subfigcaption(frame-42.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-45.png" alt="{{subfigcaption(frame-45.png)}}" id="fig:frame-45.png" /><figcaption>{{subfigcaption(frame-45.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-46.png" alt="{{subfigcaption(frame-46.png)}}" id="fig:frame-46.png" /><figcaption>{{subfigcaption(frame-46.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-47.png" alt="{{subfigcaption(frame-47.png)}}" id="fig:frame-47.png" /><figcaption>{{subfigcaption(frame-47.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-07-seq02/frame-48.png" alt="{{subfigcaption(frame-48.png)}}" id="fig:frame-48.png" /><figcaption>{{subfigcaption(frame-48.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-sequence-2015-07-seq02)}}</p>
</div>
<div id="fig:SW-sequence-2015-12-seq23">
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-101.png" alt="{{subfigcaption(frame-101.png)}}" id="fig:frame-101.png" /><figcaption>{{subfigcaption(frame-101.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-112.png" alt="{{subfigcaption(frame-112.png)}}" id="fig:frame-112.png" /><figcaption>{{subfigcaption(frame-112.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-2.png" alt="{{subfigcaption(frame-2.png)}}" id="fig:frame-2.png" /><figcaption>{{subfigcaption(frame-2.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-34.png" alt="{{subfigcaption(frame-34.png)}}" id="fig:frame-34.png" /><figcaption>{{subfigcaption(frame-34.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-50.png" alt="{{subfigcaption(frame-50.png)}}" id="fig:frame-50.png" /><figcaption>{{subfigcaption(frame-50.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-6.png" alt="{{subfigcaption(frame-6.png)}}" id="fig:frame-6.png" /><figcaption>{{subfigcaption(frame-6.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2015-12-seq23/frame-80.png" alt="{{subfigcaption(frame-80.png)}}" id="fig:frame-80.png" /><figcaption>{{subfigcaption(frame-80.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-sequence-2015-12-seq23)}}</p>
</div>
<div id="fig:SW-sequence-2016-01-seq25">
<figure>
<img src="img/SW-sequence-2016-01-seq25/frame-01.png" alt="{{subfigcaption(frame-01.png)}}" id="fig:frame-01.png" /><figcaption>{{subfigcaption(frame-01.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2016-01-seq25/frame-05.png" alt="{{subfigcaption(frame-05.png)}}" id="fig:frame-05.png" /><figcaption>{{subfigcaption(frame-05.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2016-01-seq25/frame-08.png" alt="{{subfigcaption(frame-08.png)}}" id="fig:frame-08.png" /><figcaption>{{subfigcaption(frame-08.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2016-01-seq25/frame-10.png" alt="{{subfigcaption(frame-10.png)}}" id="fig:frame-10.png" /><figcaption>{{subfigcaption(frame-10.png)}}</figcaption>
</figure>
<figure>
<img src="img/SW-sequence-2016-01-seq25/frame-16.png" alt="{{subfigcaption(frame-16.png)}}" id="fig:frame-16.png" /><figcaption>{{subfigcaption(frame-16.png)}}</figcaption>
</figure>
<p>{{figcaption(SW-sequence-2016-01-seq25)}}</p>
</div>
<div id="fig:SW-sequence-vlcsnap">
<p><img src="img/SW-sequence-vlcsnap/vlcsnap_(1).png" alt="{{subfigcaption(vlcsnap_(1).png)}}" />{#fig:vlcsnap_(1).png}</p>
<p><img src="img/SW-sequence-vlcsnap/vlcsnap_(2).png" alt="{{subfigcaption(vlcsnap_(2).png)}}" />{#fig:vlcsnap_(2).png}</p>
<p><img src="img/SW-sequence-vlcsnap/vlcsnap_(3).png" alt="{{subfigcaption(vlcsnap_(3).png)}}" />{#fig:vlcsnap_(3).png}</p>
<p><img src="img/SW-sequence-vlcsnap/vlcsnap_(4).png" alt="{{subfigcaption(vlcsnap_(4).png)}}" />{#fig:vlcsnap_(4).png}</p>
<p>{{figcaption(SW-sequence-vlcsnap)}}</p>
</div>
